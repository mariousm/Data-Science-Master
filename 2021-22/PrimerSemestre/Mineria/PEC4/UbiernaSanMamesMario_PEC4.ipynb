{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufIapjtVdA5H"
   },
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.855 · Modelos avanzados de minería de datos · PEC4</p>\n",
    "<p style=\"margin: 0; text-align:right;\">2021-1 · Máster universitario en Ciencia de datos (Data science)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios de Informática, Multimedia y Telecomunicación</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>\n",
    "\n",
    "\n",
    "# PEC 4: Series temporales y Combinación de clasificadores\n",
    "Está práctica está dividida en dos partes:\n",
    "\n",
    "En el primer ejercicio veremos como **descomponer y componer series temporales para realizar predicciones a futuro**.\n",
    "\n",
    "En el segundo ejercicio estudiaremos **diferentes métodos de combinación de clasificadores**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Descomposición y composición de series temporales para realizar predicciones\n",
    "       1.1. Componentes de la serie temporal\n",
    "          1.1.a Heterocedasticidad\n",
    "          1.1.b Tendencia\n",
    "          1.1.c Estacionalidad\n",
    "       1.2. Predicciones\n",
    "          1.2.a Predicción SARIMA\n",
    "          1.2.b Tendencia\n",
    "          1.3.c Heterocedasticidad\n",
    "2. Métodos de combinación de clasificadores\n",
    "       2.1. Combinación paralela de clasificadores\n",
    "          2.1.1 Árboles de decisión\n",
    "          2.1.2.a Bagging\n",
    "          2.1.2.b Boosting\n",
    "       2.2. Combinación secuencial de clasificadores de base diferente\n",
    "          2.2.1 KNN\n",
    "          2.2.2 SVM\n",
    "          2.2.3 Stacking\n",
    "          2.2.4 Cascading\n",
    "\n",
    "**Importante: La exportación del notebook a html se puede realizar desde el menú File $\\to$ Download as $\\to$ HTML.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Nombre y apellidos: Mario Ubierna San Mamés</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8_2TTpb_hUh"
   },
   "source": [
    "# Introducción\n",
    "Una serie temporal (time series) es una sucesión de datos ordenados cronológicamente, espaciados a intervalos iguales o desiguales. El proceso de Forecasting consiste en predecir el valor futuro de una serie temporal, bien modelando la serie temporal únicamente en función de su comportamiento pasado (autorregresivo) o empleando otras variables externas a la serie temporal.\n",
    "\n",
    "A lo largo de este documento, se describe cómo utilizar modelos de regresión de Scikit-learn para realizar forecasting sobre series temporales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install session-info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9ESJDjZDfQP"
   },
   "outputs": [],
   "source": [
    "# Tratamiento de datos\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels\n",
    "import math\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import ensemble\n",
    "\n",
    "\n",
    "# Gráficos\n",
    "# ==============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Configuración warnings\n",
    "# ==============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07UnuNsT_zWS"
   },
   "source": [
    "El dataset (corticoides.csv) que se va a utilizar en este primer ejercicio se ha obtenido del libro de forecasting de series temporales: [Principles and Practice by Rob J Hyndman and George Athanasopoulos](https://otexts.com/fpp3/).\n",
    "\n",
    "Se dispone de una serie temporal con el gasto mensual (millones de dólares) en fármacos con corticoides que tuvo el sistema de salud Australiano entre 1991 y 2008. Se pretende crear un modelo autoregresivo capaz de predecir el futuro gasto mensual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "np8XWcim1s2R",
    "outputId": "1f29ea25-568b-4c00-9a45-60af30c1b80b"
   },
   "outputs": [],
   "source": [
    "datos = pd.read_csv('corticoides.csv', sep=',')\n",
    "datos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOykxYIb1pxa"
   },
   "source": [
    "La columna fecha se ha almacenado como string. Para convertirla en datetime, se emplea la función pd.to_datetime(). Una vez en formato datetime, y para hacer uso de las funcionalidades de pandas, se establece como índice. Además, dado que los datos son mensuales, se indica la frecuencia (Monthly Started 'MS')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PHo9IdfG12r-"
   },
   "outputs": [],
   "source": [
    "# Preparación del dato\n",
    "# ==============================================================================\n",
    "datos['fecha'] = pd.to_datetime(datos['fecha'], format='%Y/%m/%d')\n",
    "datos = datos.set_index('fecha')\n",
    "datos = datos.rename(columns={'x': 'y'})\n",
    "datos = datos.asfreq('MS')\n",
    "datos = datos['y']\n",
    "datos = datos.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hY5EisGEDWg"
   },
   "source": [
    "Verificamos que la serie temporal esté completa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PD6p_w0M2Eho",
    "outputId": "6541bc21-2c76-461e-b3d9-849ff899ad4e"
   },
   "outputs": [],
   "source": [
    "# Verificar que un índice temporal está completo\n",
    "# ==============================================================================\n",
    "(datos.index == pd.date_range(start=datos.index.min(),\n",
    "                              end=datos.index.max(),\n",
    "                              freq=datos.index.freq)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k2IoRhHY2K3o"
   },
   "outputs": [],
   "source": [
    "# Completar huecos en un índice temporal\n",
    "# ==============================================================================\n",
    "# datos.asfreq(freq='30min', fill_value=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEad5nv8D9Bl"
   },
   "source": [
    "Utilizaremos los últimos 36 meses como conjunto de test para evaluar la capacidad predictiva del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AsNQ5YdP4Z4o",
    "outputId": "8e418ef4-06c4-40f7-afb9-f1a25007c9cf"
   },
   "outputs": [],
   "source": [
    "TEST_SIZE = 36\n",
    "train, test = datos.iloc[:-TEST_SIZE], datos.iloc[-TEST_SIZE:]\n",
    "x_train, x_test = np.array(range(train.shape[0])), np.array(range(train.shape[0], datos.shape[0]))\n",
    "train.shape, x_train.shape, test.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "o0pq1pUF41i1",
    "outputId": "410788df-4b5d-43a6-e88f-814045e9c733"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(15, 5))\n",
    "ax.plot(x_train, train)\n",
    "ax.plot(x_test, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4R8ForaWEZh6"
   },
   "source": [
    "# Series temporales (5 puntos)\n",
    "##1.1 Componentes de la serie temporal (2.5 puntos)\n",
    "###1.1.a. Heterocedasticidad\n",
    "Se dice que una serie temporal tiene heterocedasticidad cuando la variancia varía con el tiempo (https://es.wikipedia.org/wiki/Heterocedasticidad).\n",
    "\n",
    "El correcto funcionamiento del modelo de regresión lineal está basado en el cumplimiento de una serie de hipótesis entre las que destacan las relacionadas con el error, que son:\n",
    "\n",
    "El término error es completamente aleatorio y sigue una distribución normal, de esperanza 0. La varianza del error es constante a lo largo de las observaciones del modelo (Var[εi]=σ2). El término error no esta correlacionado entre los elementos del modelo y es independiente también de la variable explicativa. La heteroscedasticidad es un problema que surge con el incumplimiento de la segunda de estás hipótesis, la que implica que la varianza del error debe ser constante a lo largo de las observaciones.\n",
    "\n",
    "En nuestro caso, observamos que tenemos heterocedasticidad, ya que la amplitud de onda varía con el tiempo. En este primer apartado debéis eliminar la heterocedasticidad de la serie temporal. Es decir, que la diferencia entre el mínimo y el máximo de la estacionalidad (anual) sea más o menos la misma a lo largo del tiempo.\n",
    "\n",
    "**Ayuda**: Para saber qué transformación elimina mejor la heterocedasticidad, tenemos que ver tanto numéricamente como visualmente cuál de ellas presenta la menor varianza. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9rNKctyEoj7"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong> Transformación de los datos para eliminar la heterocedasticidad. Para hacer eso debeis transformar los datos aplicando la función que consideréis que elimina mejor la heterocedasticidad en el dataset (exponencial, logarítmica, ...). Estas funciones ya están implementadas en numpy. Graficar los datos originales junto con los datos transformados.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nco9By_dFkIr"
   },
   "source": [
    "### 1.1.b. Tendencia\n",
    "\n",
    "La [tendencia](https://miro.medium.com/max/1872/1*rDQL2fAp_X_dgAHNZuwRfw.png) es el comportamento que tiene los datos a largo plazo. \n",
    "\n",
    "En nuestra serie temporal tenemos una tendencia lineal creciente. En este apartado debéis eliminar la tendencia, quedando una serie temporal con tendencia constante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byeq0MvQFkFt"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong> Eliminar la tendencia de los datos. Si observamos la serie podemos apreciar que tenemos una tendencia lineal, podemos ajustar una regresión lineal (usando scikit-learn) y sustraerla a los datos originales (sin heterocedasticidad). Graficar los datos con tendencia junto con los datos sin tendencia.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spAxBrMVFj_m"
   },
   "source": [
    "### 1.1.c. Estacionalidad\n",
    "\n",
    "Definimos [estacionalidad](https://analisisdedatos.net/analisis/ST/estacionariedad.php) como la variación cíclica que se produce en los datos. En este apartado se debe encontrar y eliminar la estacionalidad. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34idxJt_Fj9E"
   },
   "source": [
    "#### 1.1.c.a. Encontrar el periodo de la estacionalidad\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong> En primer lugar debeis encontrar el ciclo, es decir, cada cuando se repiten los datos. Para ello podemos usar la autocorrelación (numpy te permite obtener los coeficientes de correlación). Graficar los coeficientes para los 24 primeros valores de autocorrelación y determinar su valor máximo, está será nuestra estacionalidad. Dar una interpretación semántica del resultado obtenido.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "soGq1B9yFj6p"
   },
   "source": [
    "#### 1.1.c.b. Aplicar un modelo SARIMA\n",
    "\n",
    "Para tratar la estacionalidad aplicaremos un modelo SARIMA. Las siglas corresponden a \"stationality ARIMA\", es decir, un modelo ARIMA con estacionalidad (la que acabamos de encontrar).\n",
    "\n",
    "El modelo [ARIMA](https://otexts.com/fpp2/arima.html) nos va a permitir tratar el ruido que queda al eliminar la heterocedasticidad, tendencia y estacionalidad. Para hacerlo tiene en cuenta las siguientes componentes:\n",
    "\n",
    "- AR: auto-regressive, se denomina *p*. Tiene en cuenta la correlación con sus lags, es decir, mira si las observaciones pasadas afectan para calcular el siguiente punto. \n",
    "- I: integrated, se denomina *d*. Es el orden de diferenciación, en nuestro caso no es necesario, puesto que la serie ya no tiene heterocedasticidad ni tendencia. Por lo tanto nuestra *d* = 0\n",
    "- MA: moving-average, se denomina *q*. Tiene en cuenta la correlación con los lags de los errores, es decir, una vez se ha aplicado el modelo, mira los errores del modelo versus los datos reales.\n",
    "\n",
    "Los modelos SARIMA dependen de varios parámetros (p,d,q)(P,D,Q)s, donde los primeros (minúsculas) corresponden a AR, I, MA de la serie normal, u los segundos (mayúsculas) a AR, I, MA con estacionalidad. \n",
    "\n",
    "Para encontrar estos parámetros tenemos que mirar los gráficos PACF (<i>partial autocorrelation function</i>) y ACF (<i>autocorrelation function</i>).\n",
    "\n",
    "El gráfico PACF nos determina el parámetro *p*, es decir, la AR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gwa3rMgTBhE"
   },
   "source": [
    "Vamos a describir cada una de las partes: \n",
    "\n",
    "#### ¿Qué entendemos por autocorrelación en las series temporales?\n",
    "Es un término estadístico que se utiliza para describir la presencia o ausencia de correlación en los datos de las series temporales, indicando, si las observaciones pasadas influyen en las actuales.\n",
    "\n",
    "Por tanto, se puede decir que la autocorrelación hace referencia cuando los valores que toman una variable en el tiempo no son independientes entre sí, sino que un valor determinado depende de los valores anteriores.\n",
    "\n",
    "El problema de la autocorrelación se denomina también frecuentemente de **“correlación serial”**.\n",
    "\n",
    "Para medir la autocorrelación se suele usar:\n",
    "\n",
    "\n",
    "La **función de autocorrelación (ACF)**:\n",
    "- Mide la correlación entre dos variables separadas por k periodos.\n",
    "- Mide el grado de asociación lineal que existe entre dos variables del mismo proceso estocástico.\n",
    "\n",
    "La **función de autocorrelación parcial (PACF)**\n",
    "- Mide la correlación entre dos variables separadas por k periodos cuando no se considera la dependencia creada por los retardos intermedios existentes entre ambas.\n",
    "- Mide la autocorrelación que existe entre dos variables separadas k períodos descontando los posibles efectos debidos a variables intermedias.\n",
    "\n",
    "Por lo tanto, la función ACF es usada para identificar el proceso de media móvil (MA) en un modelo ARIMA; mientras que la función PACF se usa para identificar los valores de la parte del proceso autoregresivo (AR).\n",
    "\n",
    "Los gráficos generados a partir de estas funciones se denominan **correlogramas**, los cuales contienen bandas con un intervalo de confianza del 95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "UirlLYlTLJM2",
    "outputId": "c68ee507-234b-4cd3-bfe3-2d5f9758151b"
   },
   "outputs": [],
   "source": [
    "# Supongamos que los datos sin heterocedasticidad ni tendencia se llaman train_sqrt_trend\n",
    "plot_pacf(train_sqrt_trend)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HcFHwLWFj4B"
   },
   "source": [
    "Para leer este gráfico, simplemente nos tenemos que fijar en los valores que salen fuera del intervalo de confianza (zona azul).\n",
    "\n",
    "Nada mas empezar hay dos valores fuera del intervalo. De todos modos el primero no se debe tener en cuenta, puesto que mira la correlación de un valor consigo mismo (lag0), y esta siempre será 1. Si no tenemos en cuenta este primer valor, solo hay un valor fuera del intervalo de confianza antes de que éste cambie de signo, con lo cual p = 1.\n",
    "\n",
    "Cuando se repite el ciclo, es decir, a partir del valor 12 (recordemos que éste es el lag11, vuelve a haber un valor fuera del intervalo antes de que éste cambie de signo, con lo cual P = 2.\n",
    "\n",
    "Veamos ahora el gráfico ACF, este determinará el valor de q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "Vjz8fqiEVDKL",
    "outputId": "88a53e96-ba53-4458-91f5-e961a7923157"
   },
   "outputs": [],
   "source": [
    "# Supongamos que los datos sin heterocedasticidad ni tendencia se llaman train_sqrt_trend\n",
    "plot_acf(train_sqrt_trend)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTlrBkXWFj1W"
   },
   "source": [
    "Este gráfico se lee igual que el anterior. Nada más empezar hay dos valores fuera del intervalo (recordemos que el lag0 no lo contamos), con lo cual q = 2. Cuando se repite el ciclo, es decir, a partir del valor 11, hay dos valores fuera del intervalo, con lo cual Q = 2.\n",
    "\n",
    "Como los datos no tienen ni tendencia ni heterocedasticidad (se la hemos quitado en apartados anteriores), d = D = 0.\n",
    "\n",
    "Como hemos visto en el apartado anterior, el ciclo es 12, con lo cual s = 12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caBs8Fs2ZLAY"
   },
   "source": [
    "## 1.2 Predicciones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpx-aN33Fjyz"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong> Aplica un modelo SARIMA a los datos sin heterocedasticidad ni tendencia. Puedes usar SARIMAX (de <i>statsmodels.tsa.statespace.sarimax</i>) con los parámetros que acabamos de ver. \n",
    "    \n",
    "Mostrar gráficamente el resultado del modelo junto con la serie original para comparar si se ajusta bien.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pvNHIXOWcJo"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Análisis:</strong> Comenta los resultados que te han salido del modelo SARIMAX\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U32DJeYWZ_ao"
   },
   "source": [
    "### 1.2.a. Predicción SARIMA (2.5 puntos)\n",
    "\n",
    "Utilizaremos el modelo SARIMA que hemos filtrado antes para predecir los tres próximos años (que son los años que hemos quitado anteriormente)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plLZh6kJZ_Tt"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong> Genera tres años de datos predichos mediante el modelo SARIMA realizado en el apartado anterior.\n",
    "\n",
    "Grafica toda la serie (sin heterocedasticidad ni tendencia), diferenciando con colores diferentes la serie real de los tres años de predicción.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwaNKnqyWcHG"
   },
   "source": [
    "### 1.2.b. Tendencia\n",
    "\n",
    "Anteriormente hemos visto que los datos de la serie temporal tienen una tendencia lineal y la hemos calculado mediante una regresión lineal. Vamos a añadir esta tendencia a nuestra predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXZ-ao1fWcEn"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong> Añadir a la serie anterior, la tendencia encontrada en el apartado 1.1.b.\n",
    "\n",
    "Mostrar gráficamente toda la serie (sin heterocedasticitat), diferenciando con colores diferentes la serie real de los dos años de predicción.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SirQ3jzEWb_u"
   },
   "source": [
    "### 1.2.c. Heterocedasticidad\n",
    "\n",
    "En el primer apartado de esta práctica hemos visto que la serie temporal tiene heterocedasticidad y la hemos eliminado transformando los datos. En este apartado haremos la transfromación inversa para añadir heterocedasticidad a nuestra predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7vFReDs0u1H"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementació:</strong> Añadir a la serie anterior, la heterocedasticidad aplicando la función inversa a la encontrada en el apartado 1.1.a.\n",
    "    \n",
    "Mostrar gráficamente toda la serie, diferenciando con colores diferentes la serie real de los años de predicción. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlMzQks7Wb9M"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong> Añade al gráfico anterior los datos de test para ver si coinciden con la predicción.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MNLluWX2Yhs"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong> Calcula el error cometido en test (por ejemplo calcula el MSE).\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZVJdfCtEnHP"
   },
   "source": [
    "## 2. Combinación de clasificadores (5 puntos)\n",
    "\n",
    "El ensemble learning es una estrategia en la que se utiliza un grupo de modelos para resolver un problema  mediante la combinación estratégica de diversos modelos de aprendizaje automático en un solo modelo predictivo. \n",
    "\n",
    "En general, los métodos de ensemble se utilizan principalmente para mejorar la precisión del rendimiento general de un modelo y combinar varios modelos diferentes, también conocidos como *aprendices básicos*, para predecir los resultados, en lugar de utilizar un solo modelo.\n",
    "\n",
    "¿Por qué entrenamos tantos clasificadores diferentes en lugar de uno solo? Bueno, el uso de varios modelos para predecir el resultado final en realidad reduce la probabilidad de sopesar las decisiones tomadas por modelos deficientes (sobreentrenados, no debidamente ajustados...).\n",
    "\n",
    "Cuanto más diversos sean estos aprendices básicos, más poderoso será el modelo final. \n",
    "\n",
    "Tengamos en cuenta que en cualquier modelo de aprendizaje automático, el error de generalización viene dado por la suma de cuadrados de bias + varianza + error irreductible. \n",
    "\n",
    "¡Los errores irreductibles son algo que está más allá de nosotros! No podemos reducirlos. \n",
    "\n",
    "Sin embargo, utilizando ensembles, podemos reducir el sesgo (bias) y la varianza de un modelo. Esto reduce el error de generalización general.\n",
    "\n",
    "La **compensación de sesgo-varianza** es el punto de referencia más importante que diferencia un modelo robusto de uno inferior (entendamos por inferior un modelo no demasiado generalizable). \n",
    "\n",
    "Aunque no es una regla exacta, en el aprendizaje automático, los modelos que tienen un sesgo alto tienden a tener una varianza más baja y viceversa.\n",
    "\n",
    "Hemos estado hablando de bias y varianza. Pero veamos que entendemos por sesgo de un modelo y por varianza de un modelo. \n",
    "\n",
    "1. **Sesgo**: El sesgo es un error que surge debido a suposiciones falsas realizadas en la fase de aprendizaje de un modelo. Un sesgo alto puede hacer que un algoritmo de aprendizaje omita información importante y correlaciones entre las variables independientes y las etiquetas de clase, por lo que no se ajusta al modelo.\n",
    "\n",
    "2. **Varianza**: la varianza nos dice qué tan sensible es un modelo a los pequeños cambios en los datos de entrenamiento. Es decir, cuánto cambia el modelo. Una gran variación en un modelo lo hará propenso al ruido aleatorio presente en el conjunto de datos, por lo que se ajustará demasiado al modelo.\n",
    "\n",
    "Para comprender con más detalle la compensación de sesgo y varianza en los modelos de aprendizaje automático, podeís consultar este [artículo](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229). \n",
    "\n",
    "\n",
    "Una vez llegados a este punto, podemos dividir los ensembles en cuatro categorías: \n",
    "\n",
    "1. **Bagging**: El bagging se utiliza principalmente para reducir la variación en un modelo. Un ejemplo simple de bagging es el algoritmo Random Forest.\n",
    "\n",
    "2. **Boosting**: El boosting se utiliza principalmente para reducir el sesgo en un modelo. Ejemplos de algoritmos de impulso son Ada-Boost, XGBoost, árboles de decisión mejorados por gradiente, etc.\n",
    "\n",
    "3. **Stacking**: el stacking se utiliza principalmente para aumentar la precisión de predicción de un modelo. Para implementar el stacking usaremos la biblioteca mlextend proporcionada por scikit learn.\n",
    "\n",
    "4. **Cascading**: esta clase de modelos son muy precisos. La conexión en cascada se usa principalmente en escenarios en los que no puede permitirse cometer un error. Por ejemplo, una técnica en cascada se usa principalmente para detectar transacciones fraudulentas con tarjetas de crédito.\n",
    "\n",
    "## Datos\n",
    "\n",
    "Para este ejercicio usaremos el dataset *diabetes.csv*.  Este conjunto de datos es original del Instituto Nacional de Diabetes y Enfermedades Digestivas y Renales. El objetivo de este dataset es predecir basándose en las mediciones de diagnóstico si un paciente tiene diabetes.\n",
    "\n",
    "En particular, todos los pacientes aquí son mujeres de al menos 21 años de ascendencia india Pima.\n",
    "\n",
    "El dataset contiene la siguiente información\n",
    "\n",
    "Embarazos: número de embarazos\n",
    "Glucosa: concentración de glucosa en plasma a 2 horas en una prueba de tolerancia a la glucosa oral\n",
    "Presión arterial: presión arterial diastólica (mm Hg)\n",
    "SkinThickness: Espesor del pliegue cutáneo del tríceps (mm)\n",
    "Insulina: insulina sérica de 2 horas (mu U / ml)\n",
    "IMC: índice de masa corporal (peso en kg / (altura en m) ^ 2)\n",
    "DiabetesPedigreeFunction: función del pedigrí de la diabetes\n",
    "Edad: Edad (años)\n",
    "Resultado (variable objetivo): variable de clase (0 o 1) \n",
    "\n",
    "En la primera parte de este ejercicio veremos la combinación de clasificadores en paralelo mediante las tecnicas de \n",
    "**_Bagging_** y **_Boosting_**.\n",
    "\n",
    "En la segunda parte intentaremos mejorar los resultados aplicando tecnicas de combinación secuencial de clasificadores: **_Stacking_** y **_Cascading_**.\n",
    "\n",
    "Para empezar, veamos como es el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "QVu-lXoK_L_d",
    "outputId": "c37bf1a4-895a-492f-e436-b806a0ed0bc3"
   },
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv('diabetes.csv')\n",
    "\n",
    "nRow, nCol = diabetes.shape\n",
    "print(f'Hay {nRow} filas y {nCol} columnas')\n",
    "diabetes.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTF33Hwv6fe-"
   },
   "source": [
    "Para poder probar varios modelos, primero vamos a dividir el dataset entre train y test.\n",
    "\n",
    "Para que todos obtengáis los mismos resultados y poder comentar dudas por el foro, fijaremos la seed para obtener los mismos datasets de train y test.\n",
    "\n",
    "Como en este ejercicio trataremos *stacking* y *cascading*, y ambos se aplican sobre el conjunto de test, haremos un *split* del 60% para tener un poco más de base al aplicar estas dos técnicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D97ORws3_-_5"
   },
   "outputs": [],
   "source": [
    "myseed= 38\n",
    "X = diabetes.drop(columns = 'Outcome')\n",
    "y = diabetes['Outcome']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6, random_state=myseed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80Jw5L8A6fbu"
   },
   "source": [
    "## 2.1. Combinación paralela de clasificadores (2 puntos)\n",
    "\n",
    "### 2.1.1. Árboles de decisión\n",
    "\n",
    "Para poder comparar el aumento de  *performance* obtenido a medida que vamos aplicando técnicas nuevas, utilizaremos como  *baseline* un simple árbol de decisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3daM2M6K6fYt"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong> Entrena un árbol de decisión sobre el conjunto de datos de train con profundidad máxima de 3 niveles (aplicaremos la misma restricción en las siguientes secciones).\n",
    "    \n",
    "A continuación evalua sobre test y calcula su precisión aplicando validación cruzada con 5 conjuntos.\n",
    "   \n",
    "    \n",
    "<u>Sugerencia</u>: usar el módulo [*cross_val_score*](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) de *sklearn*. Para aprender más sobre [*cross validation](http://scikit-learn.org/stable/modules/cross_validation.html) y sobre como usar estos módulos, os recomendamos los siguientes enlaces: \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdYghdb16fVq"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Análisis:</strong> Analizad los resultados obtenidos \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrUZJBCo6fTA"
   },
   "source": [
    "### 2.1.2.a. *Bagging*\n",
    "\n",
    "La idea central del bagging es usar réplicas del conjunto de datos original y usarlas para entrenar diferentes clasificadores.\n",
    "\n",
    "Crearemos subconjuntos muestreando aleatoriamente un montón de puntos del conjunto de datos de entrenamiento con reemplazo. \n",
    "\n",
    "Ahora entrenaremos clasificadores individuales en cada uno de estos subconjuntos bootstrap. \n",
    "\n",
    "Cada uno de estos clasificadores base predecirá la etiqueta de clase para un problema dado. Aquí es donde combinamos las predicciones de todos los modelos base. Esta parte se llama etapa de agregación. Es por eso que encontraréis los ensembles bagging por el nombre de ensembles de agregación. \n",
    "\n",
    "Por lo general, se usa un voto de mayoría simple en un sistema de clasificación y se toma la media de todas las predicciones para los modelos de regresión para combinar todos los clasificadores base en un solo modelo y proporcionar el resultado final del modelo de conjunto. \n",
    "\n",
    "Un ejemplo simple de tal enfoque es el algoritmo Random Forest. El bagging reduce la alta variación (varianza) de un modelo, reduciendo así el error de generalización. Es un método muy eficaz, especialmente cuando tenemos datos muy limitados con pudiera ser nuestro caso. \n",
    "\n",
    "Mediante el uso de muestras de bootstrap, podemos obtener una estimación agregando las puntuaciones de muchas muestras.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RUXUdU16fP3"
   },
   "source": [
    "**¿Cómo haríamos el bagging?**\n",
    "\n",
    "Supongamos que tenemos un conjunto de entrenamiento que contiene 100.000 puntos de datos. \n",
    "\n",
    "Crearíamos N subconjuntos muestreando al azar 50K puntos de datos para cada subconjunto. \n",
    "\n",
    "Cada uno de estos N subconjuntos se utilizará para entrenar N clasificadores diferentes. \n",
    "\n",
    "En la etapa de agregación, todas estas N predicciones se combinarán en un solo modelo, también llamado metaclasificador. \n",
    "\n",
    "De los 100.000 puntos presentes originalmente en el conjunto de datos, si eliminamos 1000 puntos, el impacto que tendrá en los conjuntos de datos muestreados será muy inferior. \n",
    "\n",
    "Si pensamos intuitivamente, es posible que algunos de estos 1000 puntos no estén presentes en todos los conjuntos de datos muestreados y, por lo tanto, la cantidad de puntos que se eliminarán de cada conjunto de datos muestreados será muy inferior. ¡Incluso cero en algunos casos! En resumen, el impacto de eliminar 1000 puntos de este tipo será en realidad menor en los clasificadores base, lo que reducirá la variación en un modelo y lo hará más sólido. \n",
    "\n",
    "La varianza no es más que sensibilidad al ruido, como hemos comentado anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6rzIA586fMm"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong> Entrena un <i>random forest</i> sobre el conjunto de datos de train con <b>20 árboles</b> de decisión y <b>profundidad máxima de 3</b> niveles.\n",
    "    \n",
    "A continuación evalua sobre test y calcula su precisión aplicando validación cruzada con 5 conjuntos.\n",
    "\n",
    "\n",
    "\n",
    "<u>Sugerencia</u>: usar el módulo [*RandomForestClassifier*](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) de *sklearn*. Per apender a usar este módulo os recomendamos el siguiente enlace:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpMyZMpJZdE6"
   },
   "source": [
    "### 2.1.2.b. *Boosting*\n",
    "\n",
    "El boosting se utiliza para convertir a los clasificadores de base débil en fuertes. Los clasificadores débiles generalmente tienen una correlación muy débil con las etiquetas de clase verdaderas y los clasificadores fuertes tienen una correlación muy alta entre el modelo y las etiquetas de clase verdaderas.\n",
    "\n",
    "El boosting capacita a los clasificadores débiles de manera iterativa, cada uno tratando de corregir el error cometido por el modelo anterior. Esto se logra entrenando un modelo débil en todos los datos de entrenamiento, luego construyendo un segundo modelo que tiene como objetivo corregir los errores cometidos por el primer modelo. Luego construimos un tercer modelo que intenta corregir los errores cometidos por el segundo modelo y así sucesivamente. Los modelos se agregan de forma iterativa hasta que el modelo final ha corregido todos los errores cometidos por todos los modelos anteriores.\n",
    "\n",
    "Cuando se agregan los modelos en cada etapa, se asignan algunos pesos al modelo que está relacionado con la precisión del modelo anterior. Después de agregar un clasificador débil, los pesos se vuelven a ajustar. Los puntos clasificados incorrectamente reciben pesos más altos y los puntos clasificados correctamente reciben pesos más bajos. Este enfoque hará que el siguiente clasificador se centre en los errores cometidos por el modelo anterior.\n",
    "\n",
    "El boosting reduce el error de generalización tomando un modelo de alto bias y baja varianza y reduciendo el bias en un nivel significativo. Recuerde, el bagging reduce la varianza. Al igual que el bagging, el boosting también nos permite trabajar con modelos de clasificación y regresión. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRIgEhOxZtVx"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong> Entrena un gradient boosting sobre el conjunto de datos de train con 20 árboles de decisión y profundidad máxima de 3 niveles. A continuación evalua sobre test y calcula su precisión aplicando validación cruzada con 5 conjuntos. Finalmente, dibuja la frontera de decisión. \n",
    "\n",
    "Sugerencia: usar el módulo [*GradientBoostingClassifier*](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) de sklearn. \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vra3Yb6arRv"
   },
   "source": [
    "## 2.2. Combinación secuencial de clasificadores base diferentes (3 puntos)\n",
    "\n",
    "Para poder hacer combinación secuencial de modelos, necessitamos tener varios modelos diferentes entrenados. \n",
    "\n",
    "En nuestro caso, ya tenemos un árbol de decisión. Vamos a entrenar un par de modelos más."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzZB1fmBa7DE"
   },
   "source": [
    "### 2.2.1 KNN (k vecinos más próximos)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong> Entrena un <i>k-neighbors</i> con 2 vecinos sobre el conjunto de datos de train.  \n",
    "\n",
    "A continuación evalua sobre test y calcula su precisión aplicando validación cruzada con 5 conjuntos.\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FkdLuIvbYXu"
   },
   "source": [
    "### 2.2.1 SVM (Support Vector Machines)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong> Entrena un SVM con gamma = 0.07 sobre el conjunto de datos de train.\n",
    "\n",
    "A continuación evalua sobre test y calcula su precisión aplicando validación cruzada con 5 conjuntos.\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foZmCAGKcMib"
   },
   "source": [
    "### 2.2.1 *Stacking*\n",
    "Todos los modelos individuales se entrenan por separado en el conjunto completo de datos de entrenamiento y se ajustan para lograr una mayor precisión. La compensación de bias y varianza se tiene en cuenta para cada modelo. El modelo final, también conocido como metaclasificador, se alimenta de las etiquetas de clase predichas por los modelos base o de las probabilidades predichas para cada etiqueta de clase. Luego, el metaclasificador se entrena en función de los resultados dados por los modelos base. \n",
    "\n",
    "En el stacking, se entrena un nuevo modelo en función de las predicciones realizadas por los modelos anteriores.\n",
    "Este proceso se lleva a cabo de forma secuencial. Esto significa que varios modelos se entrenan en la etapa 1 y se ajustan con precisión. Las probabilidades pronosticadas de cada modelo de la etapa 1 se alimentan como entrada a todos los modelos en la etapa 2. Los modelos en la etapa 2 luego se ajustan con precisión y las salidas correspondientes se alimentan a los modelos en la etapa 3 y así sucesivamente. Este proceso se produce varias veces en función de la cantidad de capas de apilamiento que desee utilizar.\n",
    "\n",
    "La etapa final consiste en un único modelo que nos da el resultado final al combinar el resultado de todos los modelos presentes en las capas anteriores. \n",
    "\n",
    "A menudo, el uso de clasificadores apilables aumenta la precisión de predicción de un modelo. ¡Pero de ninguna manera puede garantizarse que el uso de apilamiento aumente la precisión de la predicción en todo momento!\n",
    "Echad un vistazo al siguiente [link](http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfureSnUcSWn"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong> Construye un clasificador de <i>stacking</i> usando un <i>Gradient Boosting</i> (con <b>20 árboles</b> de decisión y <b>profundidad máxima de 3</b> niveles) que use como atributos las predicciones hechas en el conjunto de test por los algoritmos: \n",
    "    \n",
    "- árbol de decisión\n",
    "- knn\n",
    "- svm \n",
    " \n",
    "Calcula la precisión del modelo resultante con *cross-validation* en el conjunto de test (en este caso no tenemos conjunto de train, con lo cual se hace directamente cross-validation sobre test).\n",
    "\n",
    "<u>Sugerencia</u>: usar la función [column_stack](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.column_stack.html) de numpy para juntar todas las predicciones. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7Gd8knpdGP2"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <strong>Análisis:</strong> ¿Has conseguido mejorar la precisión gracias al <i>stacking</i>? Comenta los resultados.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fY9VUwhdzBX"
   },
   "source": [
    "### 2.2.2. *Cascading*\n",
    "\n",
    "El caso de cascading es parecido al de stacking pero utilizando no solamente las predicciones parciales de los clasificadores base, sino también los datos originales.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong> Para ello construimos un clasificador de cascading usando un Gradient Boosting (con 20 árboles de decisión y profundidad máxima de 3 niveles) que use como atributos las predicciones obtenidas con los modelos anteriores en el conjunto de test (igual que con el stacking), y también las variables originales. \n",
    "Calcula la precisión del modelo resultante con *cross-validation* en el conjunto de test. Sugerencia: Usa el mismo conjunto de datos que en el ejercicio anterior pero añade `X_test`.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVrUx_-1eZR2"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <strong>Análisis:</strong> Has conseguido mejorar la precisión gracias al <i>cascading</i>? Comenta los resultados.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBw8cxes6fFa"
   },
   "source": [
    "# Bibliografía utilizada\n",
    "\n",
    "- [Hyndman, R.J., & Athanasopoulos, G. (2021) Forecasting: principles and practice, 3rd edition, OTexts: Melbourne, Australia](https://otexts.com/fpp3/) \n",
    "\n",
    "- [Time Series Analysis and Forecasting with ADAM Ivan Svetunkov](https://openforecast.org/adam/)\n",
    "\n",
    "- [Python Data Science Handbook by Jake VanderPlas](https://www.amazon.es/gp/product/1491912057/ref=as_li_qf_asin_il_tl?ie=UTF8&tag=cienciadedato-21&creative=24630&linkCode=as2&creativeASIN=1491912057&linkId=73620d22f9d4a0a76d27592dabf13c83) \n",
    "\n",
    "- [Python for Finance: Mastering Data-Driven Finance](https://www.amazon.es/gp/product/1492024333/ref=as_li_qf_asin_il_tl?ie=UTF8&tag=cienciadedato-21&creative=24630&linkCode=as2&creativeASIN=1492024333&linkId=70c3175ad015970cd1c2328b7a40a055) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import session_info\n",
    "session_info.show(html=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "20211_M2.855_PEC4_Solucion.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de contenidos",
   "title_sidebar": "Tabla de contenidos",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "284.75px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.391px",
    "left": "1478px",
    "right": "20px",
    "top": "126px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
