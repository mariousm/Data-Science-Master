{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkkr1HUBPOcL"
   },
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.883 · Aprendizaje por refuerzo</p>\n",
    "<p style=\"margin: 0; text-align:right;\">2021-1 · Máster universitario en Ciencia de datos (<i>Data science</i>)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios de Informática, Multimedia y Telecomunicación</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PEC 1 - Soluciones tabulares\n",
    "\n",
    "En esta práctica implementaremos los diferentes métodos de aprendizaje por refuerzo vistos en las Partes I y II del curso. En concreto, nos centraremos en la definición de un entorno e implementaremos los diferentes métodos para buscar una solución óptima del problema.\n",
    "\n",
    "**<u>Importante</u>**: \n",
    "\n",
    "1. Esta PEC debe realizarse de **forma estrictamente individual**. Cualquier indicio de copia será penalizado con un suspenso (D) para todas las partes implicadas y la posible evaluación negativa de la asignatura de forma íntegra.\n",
    "\n",
    "2. Es necesario que el estudiante indique **todas las fuentes que ha utilizado para la realización de la PEC**. De no ser así, se considerará que el estudiante ha cometido plagio, siendo penalizado con un suspenso (D) y la posible evaluación negativa de la asignatura de forma íntegra.\n",
    "\n",
    "3. La entrega debe hacerse en **formato notebook y en formato html** donde se vea el código, los resultados y comentarios de cada ejercicio. Es decir, deben entregarse dos ficheros: uno con extensión .ipynb y otro .html. Para exportar el notebook a html puede hacerse desde el menú File $\\to$ Download as $\\to$ HTML.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyFUy1qlDGku"
   },
   "source": [
    "## 0. El entorno __CliffWalking__\n",
    "\n",
    "El entorno __CliffWalking__ consiste en un agente que se mueve en una cuadrícula de dimensiones 12x4 (ancho x alto). En cada paso, el agente tiene 4 opciones de acción o movimiento: ARRIBA, DERECHA, ABAJO, IZQUIERDA. La posición de cada casilla viene dada por una pareja de números naturales [x, y], donde la posición de la esquina de arriba a la izquierda sería el origen de coordenadas [0, 0]. El agente siempre sale de la misma casilla [0, 3] (esquina abajo izquierda) y el juego termina cuando el agente llega a la casilla de llegada [11, 3] (esquina abajo derecha).\n",
    "\n",
    "El entorno se corresponde con el ejemplo 'Cuadrícula con precipicio' explicado en la sección 3.2.1. del módulo \"Métodos de Diferencia Temporal\". El problema radica en que en todas las casillas la recompensa inmediata es R=-1 excepto en las casillas que unen en línea recta la casilla de salida con la de llegada, casillas  [1, 3] a  [10, 3]. En estas casillas, que simulan un precipicio, la recompensa es R=-100 y se vuelve a la casilla inicial.\n",
    "\n",
    "<img src=\"CliffWalking.png\">\n",
    "Fuente: Sutton & Barto, 2018\n",
    "\n",
    "El código para implementar este entorno, que se encuentra disponible en el fichero adjunto `cliff_env.py`, ha sido adaptado del siguiente enlace:\n",
    "https://pypi.org/project/gym-gridworlds/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBkO2FuFJdH-"
   },
   "source": [
    "Vamos a empezar cargando el entorno y ver qué características tiene, ejecutando un episodio de prueba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBrIWvZmDB6H"
   },
   "source": [
    "### 0.1. Carga de datos\n",
    "El siguiente código carga los paquetes necesarios para ejecutar el ejemplo, crea el entorno mediante la instanciación de un objeto de la clase `CliffEnv` (importada del archivo adjunto `cliff_env.py`) e imprime por pantalla la dimensión del espacio de acciones (0=arriba, 1=derecha, 2=abajo y 3=izquierda), el espacio de observaciones (una tupla que indica la posición del agente en la cuadrícula) y el rango de la variable de recompensa (cuyo valor es -1 o -100 dependiendo de la casilla visitada y que por tanto va de menos infinito a más infinito)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "id": "h5y5sIqBPPxr",
    "outputId": "853c4fb1-0806-4393-f51c-a9098d9eddc5"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import cliff_env as cenv\n",
    "\n",
    "env=cenv.CliffEnv()\n",
    "print(\"Action space is {} \".format(env.action_space))\n",
    "print(\"Observation space is {} \".format(env.observation_space))\n",
    "print(\"Reward range is {} \".format(env.reward_range))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXpl0ErhE_I1"
   },
   "source": [
    "### 0.2. Ejecución de un episodio\n",
    "\n",
    "A continuación, realizaremos la ejecución de un episodio del entorno CliffWalking utilizando un agente que selecciona las acciones de forma aleatoria.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84mPbgBGE1zz"
   },
   "outputs": [],
   "source": [
    "# Inicializamos el entorno\n",
    "obs = env.reset()\n",
    "t, total_reward, done = 0, 0, False\n",
    "\n",
    "print(\"Obs inicial: {} \".format(obs))\n",
    "\n",
    "switch_action = {\n",
    "        0: \"U\",\n",
    "        1: \"R\",\n",
    "        2: \"D\",\n",
    "        3: \"L\",\n",
    "    }\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    # Elegir una acción aleatoria (ésta es la implementación del agente)\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Ejecutar la acción y esperar la respuesta del entorno\n",
    "    new_obs, reward, done, info = env.step(action)\n",
    "    \n",
    "    # Imprimir time-step\n",
    "    print(\"Action: {} -> Obs: {} and reward: {}\".format(switch_action[action], new_obs, reward))\n",
    "    \n",
    "    # Actualizar variables\n",
    "    obs = new_obs\n",
    "    total_reward += reward\n",
    "    t += 1\n",
    "    \n",
    "print(\"Episode finished after {} timesteps and reward was {} \".format(t, total_reward))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mc_f31JOkh2d"
   },
   "source": [
    "## 1. Modificación del entorno (1,5 puntos)\n",
    "El entorno CliffWalking tiene varios parámetros que pueden ser modificados:\n",
    "\n",
    "\n",
    "*   La dimensión de la cuadrícula.\n",
    "*   La posición del precipicio.\n",
    "*   La posición de las casillas de salida y de llegada.\n",
    "*   La recompensa inmediata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 1.1</strong> (0,75 puntos)</div>\n",
    "\n",
    "Modificar el codigo de CliffWalking (fichero adjunto `cliff_env.py`) para que represente las propiedades de la cuadrícula descritas a continuación.\n",
    "\n",
    "- Cuadrícula 5x11\n",
    "- Posición del precipicio = Casillas [1,0], [2,0], [3,0], [4,0], [5,0], [6,0], [7,0], [8,0] y [9,0]\n",
    "- Casilla de inicio [0,0]\n",
    "- Casilla final [10,0]\n",
    "- Recompensa en casillas normales = -1\n",
    "- Recompensa en casillas del precipicio = -500 (y volvemos a la casilla de salida)\n",
    "\n",
    "Guardar el entorno modificado en el archivo `cliff_env_v2.py`, en la misma carpeta que el original. Este archivo debe entregarse junto a los archivos de este notebook (.ipynb y .html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZIKcFYi-IX-C"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import cliff_env_v2 as ce2\n",
    "\n",
    "env=ce2.CliffEnv()\n",
    "print(\"Action space is {} \".format(env.action_space))\n",
    "print(\"Observation space is {} \".format(env.observation_space))\n",
    "print(\"Reward range is {} \".format(env.reward_range))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AVvZVSqIggI"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 1.2</strong> (0,25 puntos)</div>\n",
    "\n",
    "A continuación, implementar un agente que lleve a cabo una política aleatoria. Comprobar que las casillas visitadas y las recompensas recibidas se corresponden con las acciones y el entorno programado.\n",
    "\n",
    "Mostrar la trayectoria seguida por el agente. No es necesario graficarla, tan sólo mostrar las coordenadas de las casillas visitadas en orden y las recompensas recibidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ksk8N63OHpEO"
   },
   "outputs": [],
   "source": [
    "# Environment reset\n",
    "obs = env.reset()\n",
    "t, total_reward, done = 0, 0, False\n",
    "\n",
    "print(\"Obs inicial: {} \".format(obs))\n",
    "\n",
    "switch_action = {\n",
    "        0: \"U\",\n",
    "        1: \"R\",\n",
    "        2: \"D\",\n",
    "        3: \"L\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1B0qogIJB2z"
   },
   "outputs": [],
   "source": [
    "# Escribir el código aquí\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AVvZVSqIggI"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 1.3</strong> (0,5 puntos)</div>\n",
    "\n",
    "A continuación, implementar un agente que lleve a cabo la política óptima determinista, es decir, que partiendo de la casilla inicial [0,0] llegue a la casilla final [10,0] en el menor número de pasos posible. ¿Cuál es el valor del número de pasos mínimos?\n",
    "\n",
    "Mostrar la trayectoria seguida por el agente y el retorno obtenido. No es necesario graficarla, tan sólo mostrar las coordenadas de las casillas visitadas en orden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ksk8N63OHpEO"
   },
   "outputs": [],
   "source": [
    "# Environment reset\n",
    "obs = env.reset()\n",
    "t, total_reward, done = 0, 0, False\n",
    "\n",
    "print(\"Obs inicial: {} \".format(obs))\n",
    "\n",
    "switch_action = {\n",
    "        0: \"U\",\n",
    "        1: \"R\",\n",
    "        2: \"D\",\n",
    "        3: \"L\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1B0qogIJB2z"
   },
   "outputs": [],
   "source": [
    "# Escribir el código aquí\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1B0qogIJB2z"
   },
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentarios:</strong>\n",
    "<br><br>\n",
    "Escribir aquí los comentarios oportunos\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Programación Dinámica (2.5 puntos)\n",
    "\n",
    "Dado que el entorno es determinista, es factible encontrar una política óptima (que en este caso es única) que consiga el mayor retorno (y por tanto la trayectoria más corta).\n",
    "\n",
    "El objetivo de este apartado es realizar una estimación de la política óptima mediante los métodos de Programación Dinámica, en concreto estudiaremos el algoritmo *Iteración del valor*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 2.1</strong> (1 puntos)</div>\n",
    "\n",
    "Implementar el Algoritmo 3 explicado en el módulo \"Programación Dinámica\" utilizando los siguientes parámetros:\n",
    "    \n",
    "- Theta = 0.001\n",
    "- Factor de descuento (*discount factor*) = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Environment reset\n",
    "obs = env.reset()\n",
    "t, total_reward, done = 0, 0, False\n",
    "\n",
    "print(\"Obs inicial: {} \".format(obs))\n",
    "\n",
    "# Escribir el código aquí\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 2.2</strong> (0.5 puntos)</div>\n",
    "\n",
    "Implementar una función que imprima por pantalla los valores finales de la función de valor para cada celda. \n",
    "\n",
    "Se muestra a continuación una imagen de ejemplo para la cuadrícula del entorno modificado de 11x5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ValueFunction.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# función que muestra los valores de la función de estado V(s) en la cuadricula\n",
    "\n",
    "def print_values(V, width, height):\n",
    "    # Escribir el código aquí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"values:\")\n",
    "print_values(V, 11, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 2.3</strong> (0.5 puntos)</div>\n",
    "\n",
    "Implementar una función que imprima por pantalla la política óptima encontrada para cada celda. \n",
    "\n",
    "Se muestra a continuación una imagen de ejemplo para la cuadrícula del entorno modificado de 11x5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Policy.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy_DP(policy, width, height):\n",
    "    # Escribir el código aquí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"policy:\")\n",
    "print_policy_DP(policy,11,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 2.4</strong> (0.5 puntos)</div>\n",
    "\n",
    "Ejecutar un episodio con la política óptima encontrada y mostrar la trayectoria del agente y el retorno obtenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_episode_DP(policy,env):\n",
    "    # Escribir el código aquí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_episode_DP(policy,env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Métodos de Montecarlo (2 puntos)\n",
    "\n",
    "El objetivo de este apartado es realizar una estimación de la política óptima mediante los métodos de Montecarlo, en concreto estudiaremos el algoritmo *On-policy every-visit MC control (para políticas $\\epsilon$-soft)*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 3.1</strong> (1 punto)</div>\n",
    "\n",
    "A partir del Algoritmo 3 explicado en el módulo \"Métodos de Montecarlo\", modificarlo para implementar el algoritmo *On-policy every-visit MC control (para políticas $\\epsilon$-soft)* utilizando los siguientes parámetros:\n",
    "    \n",
    "- Número de episodios = 100000\n",
    "- Epsilon inicial = 1\n",
    "- Factor de decaimiento de epsilon (*epsilon decay*) = 0.999\n",
    "- Actualizar epsilon según: $$\\textrm{max}(\\epsilon · \\epsilon_{\\textrm{decay}}, 0.01)$$\n",
    "- Factor de descuento = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Escribir el código aquí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Escribir el código aquí"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 3.2</strong> (0.5 puntos)</div>\n",
    "\n",
    "Implementar una función que imprima por pantalla la política óptima encontrada para cada celda. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy_MC(policy, width, height):\n",
    "    # Escribir el código aquí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_policy_MC(policy,11,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 3.3</strong> (0.5 puntos)</div>\n",
    "\n",
    "Ejecutar un episodio con la política óptima encontrada y mostrar la trayectoria del agente y el retorno obtenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_episode_MC(policy, env):\n",
    "    # Escribir el código aquí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_episode_MC(policy,env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Métodos de Diferencia Temporal (2 puntos)\n",
    "\n",
    "El objetivo de este apartado es realizar una estimación de la política óptima mediante los métodos de Diferencia Temporal en el entorno CliffWalking modificado anteriormente. Concretamente nos centraremos en el método SARSA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 4.1</strong> (1 punto)</div>\n",
    "\n",
    "Implementar el algoritmo *SARSA* explicado en el modulo \"Aprendizaje por Diferencia Temporal\" utilizando los siguientes parámetros:\n",
    "    \n",
    "- Número de episodios = 1000\n",
    "- *learning rate* = 0.5\n",
    "- *discount factor* = 1\n",
    "- *epsilon* = 0.05  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escribir el código aquí"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 4.2</strong> (0.25 puntos)</div>\n",
    "\n",
    "Mostrar por pantalla los valores Q estimados para cada par estado-acción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Escribir el código aquí"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 4.3</strong> (0.25 puntos)</div>\n",
    "\n",
    "Mostrar por pantalla los valores de la función de valor $v_\\pi(s)$ estimada para cada estado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# función que muestra los valores de la función de estado V(s) en la cuadricula\n",
    "\n",
    "def print_values_SARSA(Q, width, height):\n",
    " #Escribir el código aquí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_values_SARSA(q, 11, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 4.4</strong> (0.25 puntos)</div>\n",
    "\n",
    "Implementar una función que imprima por pantalla la política óptima encontrada para cada celda. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy_SARSA(Q, width, height):\n",
    "    # Escribir el código aquí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_policy_SARSA(q,11,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 4.5</strong> (0.25 puntos)</div>\n",
    "\n",
    "Ejecutar un episodio siguiendo la política óptima encontrada, donde se pueda reconocer la trayectoria seguida por el agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejecución de un episodio siguiendo la politica optima\n",
    "def execute_episode_SARSA(q, env):\n",
    " #Escribir el código aquí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_episode_SARSA(q, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparativa de los algoritmos (2 puntos)\n",
    "\n",
    "En este apartado haremos diferentes comparativas de las implementaciones de los metodos programados en los apartados anteriores.\n",
    "\n",
    "Compararemos el comportamiento de los algoritmos al modificar los valores del factor de descuento, el *learning rate* y el numero de episodios.\n",
    "\n",
    "Para cada ejercicio se debe mostrar y justificar el resultado.\n",
    "\n",
    "<u>NOTA</u>: se recomienda realizar varias veces las simulaciones en cada ejercicio, ya que éstas son aleatorias, y comentar el resultado más frecuente, o una media de estos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 5.1 - Análisis de las políticas obtenidas</strong> (0,5 puntos)</div> \n",
    "\n",
    "Realizar un estudio de las políticas obtenidas respondiendo a las siguientes preguntas:\n",
    "- ¿Todos los algoritmos consiguen llegar a la política óptima?\n",
    "- ¿A qué pueden ser debidas las diferencias entre políticas? (Si las hay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>CONCLUSIONES:</strong>\n",
    "<br><br>\n",
    "Conclusiones aquí\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 5.2 - Influencia del número de episodios</strong> (0,5 puntos)</div> \n",
    "\n",
    "Realizar un estudio cambiando el número de episodios en cada uno de los 3 algoritmos indicando:\n",
    "- Cómo influye el número de episodios en cada algoritmo (a partir de qué número de episodios converge cada algoritmo)\n",
    "- Comparativa de velocidad entre los 3 algoritmos. ¿Qué método es el más rápido en converger? ¿Cuál es el más lento? ¿A qué es debido?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Escribir el código de las simulaciones aquí\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>CONCLUSIONES:</strong>\n",
    "<br><br>\n",
    "Conclusiones aquí\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 5.3 - Influencia del factor de descuento (discount factor)</strong> (0,5 puntos)</div> \n",
    "\n",
    "Ejecutar los 3 algoritmos con *discount factor*=0.1 y el resto de parámetros iguales que en los ejercicios 2, 3 y 4. Describir los cambios en la política óptima, comparando el resultado obtenido con el resultado de los ejercicios 2, 3 y 4 (*discount factor*=1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escribir el código de las simulaciones aquí\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>CONCLUSIONES:</strong>\n",
    "<br><br>\n",
    "Conclusiones aquí\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 5.4 - Influencia del learning rate</strong> (0,5 puntos)</div> \n",
    "\n",
    "Ejecutar el algoritmo de *SARSA* con los siguientes valores de *learning rate*: 0.1 y 0.9. Analizar las diferencias con los resultados obtenidos en la pregunta 4 en términos de número de *time-steps* para llegar a la posición objetivo. Para ello, es necesario usar el mismo número de episodios. Finalmente, comparar el número de pasos por episodio (representar gráficamente el número de *time-steps* para cada episodio de la simulación en ambos casos) y verificar con que valor de *learning rate* el algoritmo converge más rápidamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escribir el código de las simulaciones aquí\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>CONCLUSIONES:</strong>\n",
    "<br><br>\n",
    "Conclusiones aquí\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "M2.883_PEC1_.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
