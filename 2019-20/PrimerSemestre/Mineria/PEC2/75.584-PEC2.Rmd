---
title: 'Minería de datos: PEC2'
author: "Autor: Mario Ubierna San Mamés"
date: "Abril 2021"
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
    includes:
      in_header: 75.584-PEC-header.html
  pdf_document:
    highlight: zenburn
    toc: yes
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

******
# Introducción
******
## Presentación
Esta prueba de evaluación continuada cubre los módulo 5 y 6 del programa de la asignatura.  

## Competencias
Las competencias que se trabajan en esta prueba son:

* Uso y aplicación de las TIC en el ámbito académico y profesional
* Capacidad para innovar y generar nuevas ideas.
* Capacidad para evaluar soluciones tecnológicas y elaborar propuestas de proyectos teniendo en cuenta los recursos, las alternativas disponibles y las condiciones de mercado.
* Conocer las tecnologías de comunicaciones actuales y emergentes, así como saberlas aplicar convenientemente para diseñar y desarrollar soluciones basadas en sistemas y tecnologías de la información.
* Aplicación de las técnicas específicas de ingeniería del software en las diferentes etapas del ciclo de vida de un proyecto.
* Capacidad para aplicar las técnicas específicas de tratamiento, almacenamiento y administración de datos.
* Capacidad para proponer y evaluar diferentes alternativas tecnológicas para resolver un problema concreto.
* Capacidad de utilizar un lenguaje de programación.  
* Capacidad para desarrollar en una herramienta IDE.  
* Capacidad de plantear un proyecto de minería de datos.  

## Objetivos
En esta PEC trabajaremos la generación, interpretación y evaluación de un modelo de agregación y de un modelo donde generaremos reglas de asociación con el software de practicas. No perderemos de vista las fases de preparación de los datos, calidad del modelo y extracción inicial del conocimiento.

## Descripción de la PEC a realizar
La prueba está estructurada en 2 ejercicios teóricos y 3 ejercicios prácticos.

## Recursos

Para realizar esta práctica recomendamos como punto de partida la lectura de los siguientes documentos:

* Módulos 5 y 6 del material didáctico.
* El aula laboratorio de R para resolver dudas o problemas.
* RStudio Cheat Sheet: Disponible en el aula Laboratorio de Minería de datos.
* R Base Cheat Sheet: Disponible en el aula Laboratorio de Minería de datos.
  

## Formato y fecha de entrega
El formato de entrega es: usernameestudiant-PAC1.html (pdf o word) y rmd.
Fecha de Entrega: 21/04/2021.
Se tiene que depositar la PEC en el buzón de entregas del aula.


## Nota: Propiedad intelectual 

A menudo es inevitable, al producir una obra multimedia, hacer uso de recursos creados por terceras personas. Es por lo tanto comprensible hacerlo en el marco de una práctica de los estudios de Informática, Multimedia y Telecomunicación de la UOC, siempre y cuando esto se documente claramente y no suponga plagio en la práctica. 

Por lo tanto, al presentar una práctica que haga uso de recursos ajenos, se debe presentar junto con ella un documento en qué se detallen todos ellos, especificando el nombre de cada recurso, su autor, el lugar dónde se obtuvo y su estatus legal: si la obra está protegida por el copyright o se acoge a alguna otra licencia de uso (Creative Commons, licencia GNU, GPL ...). 
El estudiante deberá asegurarse de que la licencia no impide específicamente su uso en el marco de la práctica. En caso de no encontrar la información correspondiente tendrá que asumir que la obra está protegida por copyright. 

Deberéis, además, adjuntar los ficheros originales cuando las obras utilizadas sean digitales, y su código fuente si corresponde.  

******
# Ejercicio 1
******

## Enunciado
1. Explica de forma resumida cual es el objetivo de los métodos de agregación. Relaciona la respuesta con un ejemplo. 
2. Razona si tiene sentido aplicar un método de agregación al ejemplo que pusiste en la PEC1. Si lo tiene, comenta como se podría aplicar y que tipo de resultados se podrían obtener. Si no lo tiene, reformula el problema para que si lo tenga.

## Respuesta ejercicio 1

Respecto al primer apartado, cuando hablamos de los métodos de agregación o clustering nos estamos refiriendo a un tipo de modelos de míneria de datos que nos permiten obtener una primera aproximación de la estructura de un dominio, es decir, nos permite discernir qué elementos son parecidos, pero sin poner de primeras ningún criterio.

Por lo tanto, el objetivo de este tipo de modelos es obtener/discernir grupos de elementos/objetos dentro de un dominio, en el cual a priori desconocemos el cómo se pueden agrupar dichos elementos ya que no tenemos una variable que nos clasifique los elementos, es decir, este tipo de modelos son parecidos a los modelos de clasificación, ambos tratan de identificar qué elemento pertenece a qué grupo gracias a características similares, sin embargo, los modelos de agregación son modelos no supervisados, mientras que los de clasificación sí que lo son. Es por ello que cuando aplicamos un modelo de clustering es porque tenemos un desconocmiento de lo que sabemos.

Un buen ejemplo en el uso de modelos de clustering podría ser el siguiente, imaginémonos que tenemos una tienda online de ropa, y queremos identificar cuáles son los diferentes tipos de clientes que nos compran para así poder aplicar mejores descuentos, o ver qué tipo de ropa se compra dentro de cada grupo, o mejorar las promociones en nuestra tienda...

Ante el ejemplo anterior tenemos el objetivo claro, es decir, necesitamos saber qué tipo de clientes hay en nuestra tienda, cómo se agrupan los clientes con características similares, pero tenemos un problema, hay un desconocimiento del dominio, es decir, no sabemos cómo se agrupan los clientes con características similares, ¿se agrupan por edad?, ¿por sexo?, ¿por estatura?, ¿por peso?, ¿por clase social?... Como podemos apreciar tenemos un gran desconocimiento del problema, no tenemos información suficiente.

Es en este tipo de problemas en los que se hace uso de un modelo de clustering, porque nos va a permitir discernir grupos sin tener el conocimiento suficiente del problema.

Respecto al segundo apartado, lo primero que tenemos que saber es el objetivo de la PEC1. En dicha prática los datos que usamos fueron los del censo en los Estados Unidos de América en el año 1994, es decir, se recogía información sobre la edad, la clase de trabajo, la educación recibida, la raza, el estado civil, los ingresos, las pérdidas...

Como podemos apreciar, la información era variada y nos permitía hacernos una idea de cómo era la sociedad en aquella época. El objetivo de ese conjunto de datos era el de clasificarlos, es decir, aplicar un modelo de clasificación para poder discenir dos grupos, aquellos en los que el salario anual era mayor a 50K dólares y los que era menor o igual a 50K.

Era un problema de clasificación ya que sabíamos el valor en todo momento de la variable objetivo, es decir, sabíamos para cada registro si tenía un salario mayor o menor a 50K.

Por lo tanto, tal cual era el ejemplo de la PEC1 no tendría sentido modelarlo como si fuera un problema de clustering, ya que como bien hemos mencionado, se sabía en todo momento el valor de la variable objetivo. Sin embargo, si no tuviéramos ese conocimiento sí que tendría sentido hacer uso de un método de agregación, ya que buscaría discernir grupos sin saber cuántos hay, cómo se relacionan...

Suponiendo que no tenemos el conocimiento suficiente respecto al ejemplo de la PEC1, podríamos aplicar por ejemplo un método de agregación como el k-means, en el cual tenemos que predefinir el número de grupos. Si ponemos que el número de grupos es dos, podríamos comprobar el cómo clasifca cada uno de los elementos y si es parecido al resolver el problema usando un modelo de clasificación. Podríamos, poner más grupos o no, dependiendo de la granularidad que queremos en los grupos, a mayor número de grupos más específico va a ser nuestro modelo, y a menor número de grupos más general va a ser.

Resumiendo, sí que podríamos hacer uso de un método de agregación en la PEC1, siempre y cuando quisiéramos analizar el cómo se agrupan los diferentes elementos y no tenemos claro un objetivo para agrupar, ya sea por raza, por sexo, por edad, por ingresos, por pérdidas... Al hacer uso de un modelo de clustering podríamos analizar el cómo se agrupan, cuáles son las características similares de cada grupo...

******
# Ejercicio 2
******
## Enunciado
1. Explica de forma resumida cual es el objetivo de los métodos de generación de reglas de asociación. Relaciona la respuesta con un ejemplo. 
2. Razona si tiene sentido aplicar un método de generación de reglas de asociación al ejemplo que pusiste en la PEC1. Si lo tiene, comenta como se podría aplicar y que tipo de resultados se podrían obtener. Si no lo tiene, reformula el problema para que si lo tenga.

## Respuesta ejercicio 2

Respecto al primer apartado. hay que hacer una distinción entre los método de clustering y los método de generación de reglas de asociación, ambos están enfocados al aprendizaje no supervisado, es decir, desconocemos la variable objetivo. Sin embargo, hay una clara diferencia entre ambos modelos.

Los modelos de clustering, tal y como se explicó en el anterior ejercicio, buscan agrupar elementos de los cuales no tenemos el conocimiento suficiente, a partir de la distancia entre elementos y su similitud. Por otro lado, los métodos de generación de reglas de asociación tratan de encontrar relaciones entre atributos que tienden a ocurrir de forma conjunta, es decir, una relación lógica si A entonces B (A => B).

Como podemos apreciar el enfoque a la hora de resolver el problema es diferente, lo que buscamos con la reglas de asociación es poder describir el conjunto de datos que tenemos, para así poder saber si hay una relación entre atributos o no.

En estadística usaríamos un contraste de hipótesis para saber si una hipótesis/condición se cumple o no. Sin embargo, con la reglas de asociación se busca el ver qué condiciones se pueden cumplir con los datos que tenemos, para ello se define tanto un nivel de confianza como de soporte.

Siguiendo con el ejemplo establecido en el ejercicio anterior, si tenemos una tienda online que se encarga de vender ropa, podríamos hacer uso de un método de generación de reglas de asociación para así saber qué atributos (sexo, edad, localización, clase social...) están relacionados a la ahora de comprar un producto, en nuestro caso, una determinada camiseta.

Para ello, definiríamos un nivel de confianza y de soporte mínimo, e iríamos comprobando si al definir una regla de asociación el nivel de confianza y de soporte que obtenemos es igual o superior a los niveles mínimos que hemos definido, de no ser así buscaríamos otra regla de asociación.

Respecto al segundo apartado, tal y como se comentó en el anterior ejercicio, el objetivo de los datos de la PEC1 era el de clasificar ya que conocíamos la variable objetivo, es decir, el salario.

Sin embargo, sí que podríamos hacer uso de métodos de generación de reglas de asociación al proporcionanos otro enfoque, ya que podríamos buscar cuáles son las condiciones que hacen que un determinado registro cumpla que tenga unos ingresos/pérdidas mayores o menores, decir, podríamos comprobar si el sexo, la edad, y la raza por ejemplo condicionan los ingresos/pérdidas de una persona en los Estados Unidos de América en 1994. Este enfoque es completamente direfente al de la clasificación y al del clustering, en el primero se busca predecir la etiqueta ya que conocemos los grupos, en el segundo se busca agrupar por similitudes ya que desconocemos las etiquetas, y el enfoque de las reglas de asociación es el descubrir el qué condiciona los datos.

Un aspecto a destacar de este tipo de métodos, es que necesita la información de forma binarizada, es decir, el dato solo puede ser un 1 o un 0, por ejemplo: o es hombre o mujer, o es negro o no lo es, o es joven o no lo es... Por lo tanto, tendríamos que transformar el dataset original de la PEC1 para poder hacer uso de estos tipos de métodos.

Una vez que ya tenemos la información binariza, podríamos hacer uso de diferentes métodos como el apriori, eclat... Uno de los más usados es el apriori, el cual se podría dividir en dos fases, en la primera identificar todos los conjuntos de elementos que ocurren con un frecuencia superior a un límite definido, y una segunda en la cual convertimos esos conjuntos de elementos en reglas de asociación.

Cabe destacar que podríamos obtener más de una regla de asociación que cumplan el límite mínimo que hemos definido. Una vez que tenemos las reglas de asociación, podemos saber las condiciones que se cumplen para poder predecir.

Resumiendo, el dataset de la PEC1 está enfocado al aprendizaje supervisado ya que conocemos en todo momento la variable objetivo, pero si no fuera así, podríamos hacer uso método de generación de reglas de asocición para ver cómo se relacionan los atributos, es decir, una consecuencia como mayores ingresos o pérdidas de qué condiciones depende.

******
# Ejercicio 3
******
## Enunciado
En este ejercicio seguiréis los pasos del ciclo de vida de un proyecto de minería de datos para el caso de un algoritmo de agregación. Lo haréis con el fichero *clientes.csv* que encontraréis adjunto. No hay que olvidarse de la fase de preparación y análisis de datos. Es muy importante explicar muy bien los resultados obtenidos.

Los ejemplos 1 y 2 se pueden tomar como un punto de partida para la realización de este ejercicio, pero se espera que la respuesta proporcionda tenga un análisis más amplio al mostrado en estos ejemplos, prestando especial atención a explicar el conocimiento que se ha adquirido tras el proceso de minería de datos.


## Ejemplo 1: Métodos de agregación con datos autogenerados 
En este ejemplo vamos a generar un conjunto de muestras aleatorias para posteriormente usar el algoritmo kmeans para agruparlas. Se crearán las muestras alrededor de dos puntos concretos. Por lo tanto, lo lógico será agrupar en dos clústers. Puesto que inicialmente, en un problema real, no se conoce cual es el número más idóneo de clústers k, vamos a probar primero con dos (el valor óptimo) y posteriormente con 4 y 8 clústers. Para evaluar la calidad de cada proceso de agregación vamos a usar la silueta media. La silueta de cada muestra evalúa como de bien o mal está clasificada la muestra en el clúster al que ha sido asignada. Para ello se usa una fórmula que tiene en cuenta la distancia a las muestras de su clúster y la distancia a las muestras del clúster vecino más cercano. 

A la hora de probar el código que se muestra, es importante tener en cuenta que las muestras se generan de forma aleatoria y también que el algoritmo kmeans tiene una inicialización aleatoria. Por lo tanto, en cada ejecución se obtendrá unos resultados ligeramente diferentes.

Lo primero que hacemos es cargar la librería cluster que contiene las funciones que se necesitan

```{r message= FALSE, warning=FALSE}
library(cluster)
```
Generamos las muestras de forma aleatoria tomando como centro los puntos [0,0] y [5,5].
```{r message= FALSE, warning=FALSE}
n <- 150 # número de muestras
p <- 2   # dimensión

sigma <- 1 # varianza de la distribución
mean1 <- 0 # centro del primer grupo
mean2 <- 5 # centro del segundo grupo

n1 <- round(n/2) # número de muestras del primer grupo
n2 <- round(n/2) # número de muestras del segundo grupo

x1 <- matrix(rnorm(n1*p,mean=mean1,sd=sigma),n1,p)
x2 <- matrix(rnorm(n2*p,mean=mean2,sd=sigma),n2,p)
```

Juntamos todas las muestras generadas y las mostramos en una gráfica
```{r message= FALSE, warning=FALSE}
x  <- rbind(x1,x2)
plot (x)
```

Como se puede comprobar las muestras están claramente separadas en dos grupos. Si se quiere complicar el problema se puede modificar los puntos centrales (mean1 y mean2) haciendo que estén más próximos y/o ampliar la varianza (sigma) para que las muestras estén más dispersas.

A continuación vamos a aplicar el algoritmo kmeans con 2, 4 y 8 clústers
```{r message= FALSE, warning=FALSE}
fit2       <- kmeans(x, 2)
y_cluster2 <- fit2$cluster

fit4       <- kmeans(x, 4)
y_cluster4 <- fit4$cluster

fit8       <- kmeans(x, 8)
y_cluster8 <- fit8$cluster
```
Las variables y_cluster2, y_cluster4 e y_cluster8 contienen para cada muestra el identificador del clúster a las que han sido asignadas. Por ejemplo, en el caso de los k=2 las muestras se han asignado al clúster 1 o al 2

```{r message= FALSE, warning=FALSE}
y_cluster2
```

Para visualizar los clústers podemos usar la función clusplot. Vemos la agrupación con 2 clústers
```{r message= FALSE, warning=FALSE}
clusplot(x, fit2$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)
```

con 4
```{r message= FALSE, warning=FALSE}
clusplot(x, fit4$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)
```

y con 8
```{r message= FALSE, warning=FALSE}
clusplot(x, fit8$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)
```

También podemos visualizar el resultado del proceso de agregación con el siguiente código para el caso de 2 clústers
```{r message= FALSE, warning=FALSE}
plot(x[y_cluster2==1,],col='blue', xlim=c(min(x[,1]), max(x[,1])), ylim=c(min(x[,2]), max(x[,2])))
points(x[y_cluster2==2,],col='red')
```

para 4
```{r message= FALSE, warning=FALSE}

plot(x[y_cluster4==1,],col='blue', xlim=c(min(x[,1]), max(x[,1])), ylim=c(min(x[,2]), max(x[,2])))
points(x[y_cluster4==2,],col='red')
points(x[y_cluster4==3,],col='green')
points(x[y_cluster4==4,],col='black')
```

y para 8
```{r message= FALSE, warning=FALSE}
plot(x[y_cluster8==1,],col='blue', xlim=c(min(x[,1]), max(x[,1])), ylim=c(min(x[,2]), max(x[,2])))
points(x[y_cluster8==2,],col='red')
points(x[y_cluster8==3,],col='green')
points(x[y_cluster8==4,],col='black')
points(x[y_cluster8==5,],col='yellow')
points(x[y_cluster8==6,],col='purple')
points(x[y_cluster8==7,],col='cyan')
points(x[y_cluster8==8,],col='orange')
```

Ahora vamos a evaluar la calidad del proceso de agregación. Para ello usaremos la función silhouette que calcula la silueta de cada muestra

```{r message= FALSE, warning=FALSE}
d  <- daisy(x) 
sk2 <- silhouette(y_cluster2, d)
sk4 <- silhouette(y_cluster4, d)
sk8 <- silhouette(y_cluster8, d)
```

La función silhouette devuelve para cada muestra, el clúster dónde ha sido asignado, el clúster vecino y el valor de la silueta. Por lo tanto, calculando la media de la tercera columna podemos obtener una estimación de la calidad del agrupamiento

```{r message= FALSE, warning=FALSE}
mean(sk2[,3])
mean(sk4[,3])
mean(sk8[,3])
```

Como se puede comprobar, agrupar con dos clúster es mejor que en 4 o en 8, lo cual es lógico teniendo en cuenta como se han generado los datos.

En este ejercicio no podemos sacar ninguna conclusión a partir de los clusters obtenidos puesto que los datos de partida no se corresponden con ningún problema real. En un ejemplo real tendríamos que analizar las agrupaciones obtenidas para obtener conocimiento sobre el problema a resolver.

## Ejemplo 2: Métodos de agregación con datos reales 
A continuación vamos a ver otro ejemplo de cómo se usan los modelos de agregación. Para ello usaremos el fichero *flores.csv* que encontraréis adjunto.  

Cargamos el fichero y visualizamos la estructura de los datos
```{r message= FALSE, warning=FALSE}
library(cluster)
flores_data<-read.csv("./data/flores.csv", header=T, sep=",")
colnames(flores_data) <- c("sepalLength", "sepalWidth", "petalLength", "petalWidth")
summary(flores_data)
```

Como podemos comprobar tenemos cuatro características, la longitud y anchura del sépalo y la longitud y anchura del pétalo.
 
Como inicialmente no conocemos el número óptimo de clústers, probamos con varios valores
```{r message= FALSE, warning=FALSE}
d <- daisy(flores_data) 
resultados <- rep(0, 10)
for (i in c(2,3,4,5,6,7,8,9,10))
{
  fit           <- kmeans(flores_data, i)
  y_cluster     <- fit$cluster
  sk            <- silhouette(y_cluster, d)
  resultados[i] <- mean(sk[,3])
}
```


Mostramos en un gráfica los valores de las siluetas media de cada prueba para comprobar que número de clústers es el mejor
```{r message= FALSE, warning=FALSE}
plot(2:10,resultados[2:10],type="o",col="blue",pch=0,xlab="Número de clusters",ylab="Silueta")
```

El mejor valor que se obtiene es k=2.

Otro forma de evaluar cual es el mejor número de clústers es considerar el mejor modelo, aquel que ofrece la menor suma de los cuadrados de las distancias de los puntos de cada grupo con respecto a su centro (withinss), con la mayor separación entre centros de grupos (betweenss). Como se puede comprobar es una idea conceptualmente similar a la silueta. Una manera común de hacer la selección del número de clústers consiste en aplicar el método elbow (codo), que no es más que la selección del número de clústers en base a la inspección de la gráfica que se obtiene al iterar con el mismo conjunto de datos para distintos valores del número de clústers. Se seleccionará el valor que se encuentra en el "codo" de la curva

```{r message= FALSE, warning=FALSE}
resultados <- rep(0, 10)
for (i in c(2,3,4,5,6,7,8,9,10))
{
  fit           <- kmeans(flores_data, i)
  resultados[i] <- fit$tot.withinss
}
plot(2:10,resultados[2:10],type="o",col="blue",pch=0,xlab="Número de clusters",ylab="tot.tot.withinss")
```

En este caso el número óptimo de clústers es 3 o 4 que es cuando la curva comienza a estabilizarse.

También se puede usar la función *kmeansruns* del paquete fpc que ejecuta el algoritmo kmeans con un conjunto de valores, para después seleccionar el valor del número de clústers que mejor funcione de acuerdo a dos criterios: la silueta media ("asw") y Calinski-Harabasz ("ch").

```{r message= FALSE, warning=FALSE}
library(fpc)
fit_ch  <- kmeansruns(flores_data, krange = 1:10, criterion = "ch") 
fit_asw <- kmeansruns(flores_data, krange = 1:10, criterion = "asw") 
```

Podemos comprobar el valor con el que se ha obtenido el mejor resultado y también mostrar el resultado obtenido para todos los valores de k usando ambos criterios

```{r message= FALSE, warning=FALSE}
fit_ch$bestk
fit_asw$bestk

plot(1:10,fit_ch$crit,type="o",col="blue",pch=0,xlab="Número de clústers",ylab="Criterio Calinski-Harabasz")
plot(1:10,fit_asw$crit,type="o",col="blue",pch=0,xlab="Número de clústers",ylab="Criterio silueta media")

```

Los resultados son muy parecidos a los que hemos obtenido anteriormente. Con el criterio de la silueta media se obtienen 2 clústers y con el Calinski-Harabasz se obtienen 3.

Como se ha comprobado, conocer el número óptimo de clústers no es un problema fácil. Tampoco lo es la evaluación de los modelos de agregación.

Tras las pruebas que hemos realizado para obtener el número óptimo de clusters, hemos encontrado que el número de clusters varía según el método entre 2, 3 o 4. Vamos a estudiar los resultados encontrados con 2 y 3 clusters.

A continuación mostramos visualmente los clusters encontrados suponiendo que hay 3 clusters. Hay que tener en cuenta que nos es posible mostrar los clusters en un espacio de 4 dimensiones. Por lo tanto, mostramos los clusters entre pares de caracteríticas:

```{r message= FALSE, warning=FALSE}
cl3 <- kmeans(flores_data, 3)
with(flores_data, pairs(flores_data, col=c(1:4)[cl3$cluster])) 
```

La gráfica (que puede cambiar de una ejecución a otra por el factor aleatorio del kmeans) muestra claramente que hay un cluster que está más diferenciado de los otros dos. 

Una buena técnica que ayuda a entender los grupos que se han formado, es analizar las características de cada grupo:

* Grupo 1: Valores del pétalo alto. Longitud del sépalo alto.
* Grupo 2: Valores del pétalo bajos. Longitud del sépalo bajo.
* Grupo 3: Valores intermedios salvo en el ancho del sépalo que es intermedio.

Vamos a ahora a estudiar los conjuntos que se han obtenido con 2 clusters:

```{r message= FALSE, warning=FALSE}
cl2 <- kmeans(flores_data, 2)
with(flores_data, pairs(flores_data, col=c(1:4)[cl2$cluster])) 
```

En este caso se comprueba que existen dos grupos con un frontera bastante clara y que con los valores del pétalo tenemos suficiente para diferenciar los dos grupos. Podemos describir los dos grupos como:

* Grupo 1: Valores del pétalo bajos.
* Grupo 2: Valores del pétalo alto.

Esto último lo podemos comprobar usando únicamente las dos características relacionadas con el pétalo:

```{r message= FALSE, warning=FALSE}
cl2b <- kmeans(flores_data[c(3,4)], 2)
plot(flores_data[c(3,4)], col=cl2b$cluster)
```

## Respuesta ejercicio 3

## Carga, resumen, explicación, preparación y análisis de los datos

### Carga de los datos
En este primer punto lo que debemos de hacer es la carga de los datos, un resumen de lo que hemos leído, una explicación de los mismos, preparar los datos para el modelo y realizar un análisis de los mismos.

Por lo tanto, lo primero que debemos hacer es la carga del archivo clientes.csv:
```{r}
df_clientes = read.csv("./data/clientes.csv", stringsAsFactors = TRUE)
colnames(df_clientes) = c("CustomerID", "Genr", "Age", "Income", "Spending_Score")
head(df_clientes)
```

### Resumen de los datos

Resumen de la estructura del dataset:
```{r}
str(df_clientes)
```

### Explicación de los datos

Revisamos la descripción de las variables y si los tipos de variable se corresponden al que hemos cargado:

**CustomerID**
    integer - id del cliente.
    
**Genr**
    factor - género del cliente.
    
**Age**
    integer - edad del cliente.
    
**Income**
    integer - ingresos anuales del cliente, representado en k dólares (15 = 15000$)
    
**Spending_Score**
    integer - puntuación del gasto del cliente.
    
Vemos que tenemos 200 registros y 5 variables que los caracterizan. 

Este dataset está formado por 4 variables numéricas y una variable categórica (el género).

### Preparación de los datos

Al hacer un uso posterior del algoritmo kmeans solo nos admite valores numéricos, es por ello que vamos a eliminar la columna género del dataset original. Además, vamos a analizar el cómo se agrupan los clientes según la edad, los ingresos y su puntación en el gasto, por lo que la columna CustomerID la podemos quitar del dataset, si se necesita en un futuro para identificar un elemento en particular podemos añadir dicha columna a posteriori:
```{r}
df_clientes_copy = df_clientes[c(3:5)]
str(df_clientes_copy)
```

Comprobamos que no haya datos sin valores:
```{r}
colSums(is.na(df_clientes_copy))
```

De la anterior ejecución vemos que todos los campos tienen valores.
Realizamos un summary del dataset resultante para hacernos una idea del conjunto de datos que vamos a manejar, es decir, cuáles son los valores mínimos/máximos, ver si a priori hay outliers...
```{r}
summary(df_clientes_copy)
```

Como podemos observar, las variables Age, Income y Spending_Score siempre toman valores positivos, lo cual tiene todo el sentido del mundo, no sería posible ver a alguien con -8 años/ -15000$ de ingresos/ -50 puntos de gasto. Además, todas las variables tienen un rango de valores bastente comunes, sin embargo, en la variable Income hay mucha diferencia entre el tercer cuartil y el valor máximo, comprobamos si hay outliers en dicha variable:
```{r}
library(ggplot2)

boxplot(df_clientes_copy$Income)
ggplot(mapping = aes(x=df_clientes_copy$Income)) + geom_density()
```

Observamos según el boxplot, que sí que tenemos un punto que es outlier, cuyo valor es el máximo 137. Sin embargo, a la hora de ver la distribución de la variable vemos que para nada es un outlier, es decir, no hay una anomalía, por lo tanto, podemos asegurar que dicho valor es correcto, ya que puede ser que alguien ingrese 137000$.

El siguiente paso es preparar los datos para que puedan ser utilizados por el algoritmo kmeans, un aspecto importante de dicho algoritmo es que tiene en cuenta las distancias entre puntos, por lo que, no es lo mismo medir la distancia como la edad que va de 18 a 70 años, a como lo hace los ingresos anuales que van de 15000 a 137000 dólares.

Es por ello, que primero debemos normalizar el dataset:
```{r}
df_clientes_copy = scale(df_clientes_copy)
head(df_clientes_copy)
```

### Análisis de los datos

Finalmente, para terminar con esta primera fase, vamos a hacer un análisis de dichas variables, es decir, los ingresos respecto a la edad:
```{r}
ggplot(df_clientes, aes(x = Age, y = Income)) +
  geom_point() +
  ggtitle("Ingresos respecto a la edad")
```

Como podemos ver no hay una distinción clara de grupos, sin embargo, aquella población de mediana edad entre 30 y 50 años tienen unos ingresos de media más elevados que la población joven y que la población de mayores, pero a priori no podemos hacer una distinción clara entre grupos.

Por otro lado, podemos ver cómo varía la puntuación de gatos respecto a la edad:
```{r}
ggplot(df_clientes, aes(x = Age, y = Spending_Score)) +
  geom_point() +
  ggtitle("Puntuación de gastos respecto a la edad")
```

Analizando la anterior gráfica podemos ver que podríamos tener a priori 4/5 grupos: población joven con una baja puntuación de gastos, población joven con una puntuación de gastos media, población joven con una puntuación de gastos elevada, población de mediana/mayor edad con una baja puntuación de gastos y población de mediana/mayor edad con una puntuación de gastos media.

Por último podemos analizar la relación entre ingresos y gastos:
```{r}
ggplot(df_clientes, aes(x = Income, y = Spending_Score)) +
  geom_point() +
  ggtitle("Puntuación de gastos respecto a los ingresos")
```

La anterior gráfica es bastante curiosa porque podemos apreciar 5 grupos claramente: población que tiene unos ingresos bajos y su puntación de gasto es baja, ingresos bajos y gasto elevado, ingresos medio y gasto medio, ingresos altos y gastos bajos, ingresos altos y gastos altos.

## Aplicación del algoritmo k-means con diferentes k.

Del anterior análisis de los datos podemos apreciar que a priori va a haber 5 grupos según los ingresos y los gastos, pero una cosa es lo que a priori podemos tener y luego lo que realmente es, por lo tanto, vamos a aplicar el algoritmo de k-means con diferentes k para ver cómo se agrupan los elementos:
```{r}
library(cluster)
```

Pero antes, vamos a calcular el número de clústers ideales para poder realizar nuestro modelo. Esto lo vamos a hacer de dos formas, la primera calculando la silhouette. Cabe destacar que calculamos el valor de la silueta x veces para cada modelo, esto es así porque dicho valor cambia todo el rato, al calcularlo x veces podemos hacer una media y que los "mejores clusters" se mantengan independientemente de las veces que se ejecute:
```{r}
# Método que nos devuelve el valor medio de la silueta para un rango de clústers y de iteraciones sobre el mismo clúster
kmeans_silhouette = function(df, minCluster = 2, maxCluster = 10, maxIterations = 10) {
  
  if (minCluster <= maxCluster & minCluster > 1) {
    resultados = rep(0, maxCluster)
    
    for (i in c(minCluster:maxCluster)) {
      resultado = rep(0, maxIterations)
      
      for (j in c(1:maxIterations)) {
        fit = kmeans(df, i)
        y_cluster = fit$cluster
        sk = silhouette(y_cluster, daisy(df))
      
        # Como el valor de la silueta puede cambiar de una iteración a otra, calculamos su valor medio
        resultado[j] = mean(sk[,3])
      }
      resultados[i] = mean(resultado)
    }
    return(resultados)
    
  } else {
    print("ERROR: minCluster debe ser menor que maxCluster y mayor que 1")
  }
}
```

Una vez definido el método que nos va a permitir calcular el valor de la silueta medio para cada modelo, obtenemos la siguiente gráfica:
```{r}
minCluster = 2
maxCluster = 10
maxIterations = 2000

mean_silhouette = kmeans_silhouette(df_clientes_copy, minCluster, maxCluster, maxIterations)

plot(minCluster:maxCluster, mean_silhouette[minCluster:maxCluster],type="o",col="blue",pch=0,xlab="Número de clusters",ylab="Silueta")
```

Tal y como podemos observar, los modelos que mejor silueta proporcionan son cuando tienen 5, 6 y 7 clústers. Si nos ponemos un poco más precisos, podríamos decir que los dos mejores modelos son cuando tienen 5 y 6 clústers ya que cuando tiene 7 el modelo tiene una silueta menor que en los dos casos anteriores.

## Análisis de calidad del agrupamiento

Una vez que ya hemos aplicado el kmeans con diferentes k, para así saber cuántos clusters proporciona una mayor silueta, lo siguiente que debemos hacer es saber cómo de bueno son los agrupamientos, lo cual nos va a definir también cuántos clústers serían lo ideal.

Esto los conseguimos con la menor suma de los cuadrados de las distancias de los puntos de cada grupo con respecto a su centro (withinss), con la mayor separación entre centros de grupos (betweenss):
```{r}
# Método que nos devuelve el valor medio del tot.withinss para un rango de clústers y de iteraciones sobre el mismo clúster
kmeans_totWithinss = function(df, minCluster = 2, maxCluster = 10, maxIterations = 10) {
  
  if (minCluster <= maxCluster & minCluster > 1) {
    resultados = rep(0, maxCluster)
    
    for (i in c(minCluster:maxCluster))
    {
      resultado = rep(0, maxIterations)
      
      for (j in c(1:maxIterations)) {
        fit = kmeans(df, i)
        resultado[j] = fit$tot.withinss
      }
      
      resultados[i] = mean(resultado)
    }
    return(resultados)
    
  } else {
     print("ERROR: minCluster debe ser menor que maxCluster y mayor que 1")
  }
}
```

Una vez definido el método que nos devuelve el valor medio de withinss para cada modelo, obtenemos la siguiente gráfica:
```{r}
mean_totWithinss = kmeans_totWithinss(df_clientes_copy, minCluster, maxCluster, maxIterations)

plot(minCluster:maxCluster,mean_totWithinss[minCluster:maxCluster],type="o",col="blue",pch=0,xlab="Número de clusters",ylab="tot.tot.withinss")
```

Tal y como podemos observar en la anterior  ilustración, los mejores agrupamientos son aquellos en los que el error es mínimo (se ha explicado antes), lógicamente a mayor número de clusters, menor error vamos a tener ya que el modelo va a ser muy específico, de hecho, si cada punto fuera un clúster independiente a priori sería lo mejor, porque reduciríamos el error a 0, pero esto no es lo ideal de un problema de minería, no buscamos crear modelos muy específicos, sino que más bien buscamos generalizar.

Para seguir determinando la calidad de los modelos podemos tener en cuenta la distancia entre centro de grupos, lo que se llama betweenss, a mayor distancia más separación hay entre los grupos, por lo que mejor es el modelo:
```{r}
# Método que nos devuelve el valor medio de la betweenss para un rango de clústers y de iteraciones sobre el mismo clúster
kmeans_betweenss = function(df, minCluster = 2, maxCluster = 10, maxIterations = 10) {
  
  if (minCluster <= maxCluster & minCluster > 1) {
    resultados = rep(0, maxCluster)
    
    for (i in c(minCluster:maxCluster))
    {
      resultado = rep(0, maxIterations)
      
      for (j in c(1:maxIterations)) {
        fit = kmeans(df, i)
        resultado[j] = fit$betweenss
      }
      
      resultados[i] = mean(resultado)
    }
    return(resultados)
    
  } else {
     print("ERROR: minCluster debe ser menor que maxCluster y mayor que 1")
  }
}
```

Una vez definido el método que nos devuelve el valor medio de betweenss para cada modelo, obtenemos la siguiente gráfica:
```{r}
mean_betweenss = kmeans_betweenss(df_clientes_copy, minCluster, maxCluster, maxIterations)

plot(minCluster:maxCluster,mean_betweenss[minCluster:maxCluster],type="o",col="blue",pch=0,xlab="Número de clusters",ylab="betweenss")
```

Por lo tanto, viendo la anterior gráfica podríamos concluir al igual que antes que un modelo con 5,6 o 7 clústers sería lo mejor, aunque es verdad que si elegimos un modelo con 5 y 6 clústers va a ser un pelín mejor.

Una vez que ya hemos visto qué modelos presentan una mejor calidad de agrupamiento dependiendo del número de clusters, vamos a realizar el análisis tanto para un modelo de kmeans con un k igual a 5 e igual a 6.

Lo primero que debemos de hacer es crear ambos modelos:
```{r}
set.seed(0)
clientes_kmeans_k5 = kmeans(df_clientes_copy, 5)
clientes_kmeans_k6 = kmeans(df_clientes_copy, 6)
```

Lo segundo que vamos a hacer es calcular la silueta de cada modelo:
```{r}
set.seed(0)
clientes_kmeans_k5_silhouette = kmeans_silhouette(df_clientes_copy, 5, 5, 1)[5]
clientes_kmeans_k6_silhouette = kmeans_silhouette(df_clientes_copy, 6, 6, 1)[6]
cat("La silueta cuando el número de clusters es igual a 5 es: ", clientes_kmeans_k5_silhouette)
cat("La silueta cuando el número de clusters es igual a 6 es: ", clientes_kmeans_k6_silhouette)
```

Lo tercero que vamos a realizar es calcular el calidad de cada modelo respecto a su withinss:
```{r}
set.seed(0)
clientes_kmeans_k5_totWithinss = kmeans_totWithinss(df_clientes_copy, 5, 5, 1)[5]
clientes_kmeans_k6_totWithinss = kmeans_totWithinss(df_clientes_copy, 6, 6, 1)[6]
cat("El withinss total cuando el número de clusters es igual a 5 es: ", clientes_kmeans_k5_totWithinss)
cat("El withinss total cuando el número de clusters es igual a 6 es: ", clientes_kmeans_k6_totWithinss)
```

Lo cuarto que vamos a hacer es calcular el calidad de cada modelo respecto a su betweenss:
```{r}
set.seed(0)
clientes_kmeans_k5_betweenss = kmeans_betweenss(df_clientes_copy, 5, 5, 1)[5]
clientes_kmeans_k6_betweenss = kmeans_betweenss(df_clientes_copy, 6, 6, 1)[6]
cat("La betweenss total cuando el número de clusters es igual a 5 es: ", clientes_kmeans_k5_betweenss)
cat("La betweenss total cuando el número de clusters es igual a 6 es: ", clientes_kmeans_k6_betweenss)
```

Una vez que tenemos todas las medidas, debemos saber que a mayor distancia a los centros de cada cluster es mejor y a menor distancia entre los puntos de un clúster a su centro también es mejor, por lo que a priori presenta una mayor calidad del modelo cuando tenemos 6 clústers. Sin embargo, el valor de la silueta es algo mayor cuando hay 5 clústers en vez de 6 (esto puede variar dependiendo de la ejecución).

Es por ello, que vamos a analizar los resultado obtenidos en primer lugar para un modelo con 5 clústers y luego para un modelo con 6 clústers.

## Interpretación de los clústers

Lo primero que debemos observar es el cómo se han dividido los grupos por pares de variables, al tener tres variables podríamos hacer un gráfico tridimensional, pero se ve más claro en un gráfico en dos dimensiones.

El primer gráfico representa los grupos según el ingreso que tienen respecto a la edad:
```{r}
plot(df_clientes_copy[,c(1,2)], col=clientes_kmeans_k5$cluster, main="Ingresos respecto a la edad")
```

Cabe destacar que si en un gráfico hay grupos mezclados que apriori no tendría mucho sentido, es porque falta analizar el resto de gráficos (diferentes puntos de vista), por lo que ahora vamos a sacar unas primeras conclusiones y al final de todo el análisis tendremos ya las conclusiones definitivas.

Del anterior gráfico, podemos ver como es lógico que tenemos 5 grupos ya que estamos analizando primero el modelo kmedias con una k igual a 5.

Podríamos concluir a primera vista que tenemos 3 grupos claros:

- Aquellos que son jóvenes y tienen unos ingresos bajos. (grupo negro)
- Mediana/tercera edad con ingresos muy bajos. (grupo azul oscuro)
- Mediana/tercera edad con ingresos bajos/medios. (grupo rojo)

Sin embargo dos grupos se mezclan (verde y azul claro), los primeros son gente de mediana/tercera edad con unos ingresos medios/elevados y los segundo son gente jóven con unos ingresos medios/elevados.

El siguiente gráfico hace referencia a los gastos según la edad:
```{r}
plot(df_clientes_copy[,c(1,3)], col=clientes_kmeans_k5$cluster, main="Gastos respecto a la edad")
```

Al igual que sucedía con los ingresos sí que podemos distinguir tres grupos claramente:

- Aquellos jóvenes que tienen unos gastos medios/elevados. (grupo negro)
- Aquello jóvenes de una edad superior a los anteriores con unos gastos elevados. (grupo azul claro)
- Gente de mediana/tercera edad con unos gastos medios. (grupo rojo)

Sin embargo, no podemos distinguir desde este punto de vista dos grupos:
- Gente de mediana/tercera edad con unos gastos medios/bajos (grupo azul oscuro)
- Gente de mayoritariamente de mediana edad anque también de tercera edad con unos gastos medios/bajos. (grupo verde)

Por último, tenemos los gastos respecto a los ingresos:
```{r}
plot(df_clientes_copy[,c(2,3)], col=clientes_kmeans_k5$cluster, main="Gastos respecto a los ingresos")
```

En este gráfico podemos distinguir mayoritariamente muy bien los grupos:

- Gente con unos gastos altos/medios e ingresos bajos/medios. (grupo negro)
- Gente con gastos bajos e ingresos bajos. (grupo azul oscuro)
- Gente con unos gastos medios e ingresos medios. (grupo rojo)
- Gente con gastos altos e ingresos altos. (grupo azul claro)
- Gente con gastos bajos e ingresos altos. (grupo verde)

### Conclusiones respecto al algortimo de kmeans con una k = 5

Una vez que hemos analizado los clúster cuando tenemos 5 grupos, obtenemos las siguiente conclusiones:

- Respecto al grupo negro, son jóvenes que tienen unos ingresos bajos y sus gastos son medios y elevados.
- Respecto al grupo azul claro, son jóvenes que tienen unos ingresos medios/elevados y unos gastos elevados.
- Respecto al grupo rojo, son gente de mediana/tercera edad con unos ingresos medios/bajos y unos gastos medios.
- Respecto al grupo azul oscuro, son gente de mediana/tercera edad con unos ingresos muy bajos y unos gastos bajos.
- Respecto al grupo verde, son gente de mediana/tercera edad con ingresos medios/elevados y unos gastos medios/bajos.

Una vez obtenidas las conclusiones para el algoritmo kmeans con una k = 5, vamos a hacer lo mismo para una k = 6 

El primer gráfico representa los grupos según el ingreso que tienen respecto a la edad:
```{r}
plot(df_clientes_copy[,c(1,2)], col=clientes_kmeans_k6$cluster, main="Ingresos respecto a la edad")
```

Lógicamente al tener una k = 6 obtenemos 6 clúster, los cuales son:

- Jóvenes con unos ingresos medios. (grupo rosa)
- Jóvenes con unos ingresos bajos. (grupo rojo)
- Gente jóven/mediana edad con ingresos elevados. (grupo azul claro)
- Gente de mediana/tercera edad con ingresos elevados. (grupo verde)
- Gente de mediana/tercera edad con ingresos medios. (grupo azul oscuro)
- Gente de mediana/tercera edad con ingresos bajos. (grupo negro)

El siguiente gráfico hace referencia a los gastos según la edad:
```{r}
plot(df_clientes_copy[,c(1,3)], col=clientes_kmeans_k6$cluster, main="Gastos respecto a la edad")
```

Del gráfico anterior obtenemos las siguiente información:

- Jóvenes con gastos elevados. (grupo rojo)
- Jóvenes con gastos medios. (grupo rosa)
- Gente jóven/mediana edad con gastos elevados. (grupo azul claro)
- Gente de mediana/tercera edad con gastos medios. (grupo azul oscuro)
- Gente de mediana/tercera edad con gastos bajos. (grupo negro)
- Gente de mediana/tercera edad con gastos bajos. (grupo verde)

Cabe destacar que los dos últimos grupos no se ve una buena separación entre ellos, es más están mezclados, pero como veremos acontinuación es solamente porque coinciden en el rango de edad y el nivel de gasos.

Por último, tenemos los gastos respecto a los ingresos:
```{r}
plot(df_clientes_copy[,c(2,3)], col=clientes_kmeans_k6$cluster, main="Gastos respecto a los ingresos")
```

Respecto al anterior gráfico obtenemos la siguiente información:

- Gente que tiene unos ingresos bajos y unos gastos elevados. (grupo rojo)
- Gente que tiene unos ingresos bajos y unos gastos bajos. (grupo negro)
- Gente que tiene unos ingresos medios y unos gastos medios. (grupo azul oscuro)
- Gente que tiene unos ingresos medios y unos gastos medios. (grupo rosa)
- Gente que tiene unos ingresos elevados y unos gastos elevados. (grupo azul claro)
- Gente que tiene unos ingresos elevados y unos gatos bajos. (grupo verde)

### Conclusiones respecto al algortimo de kmeans con una k = 6

Una vez que hemos analizado los clúster cuando tenemos 6 grupos, obtenemos las siguiente conclusiones:

- Respecto al grupo rojo, son jóvenes que tienen unos ingresos bajos y unos gastos elevados.
- Respecto al grupo rosa, son jóvenes que tienen unos ingresos medios y gastos medios.
- Respecto al grupo azul claro, son gente jóven/mediana edad con unos ingresos elevados y gastos elevados.
- Respecto al grupo negro, son gente de mediana/tercera edad con unos ingresos bajos y gastos bajos.
- Respecto al grupo azul oscuro, son gente de mediana/tercera edad con unos ingresos medios y gastos medios.
- Respecto al grupo verde, son gente de mediana/tercera edad con unos ingresos elevados y gastos bajos.

******
# Ejercicio 4
******
## Enunciado
En este ejercicio seguiréis los pasos del ciclo de vida de un proyecto de minería de datos para el caso de un algoritmo de generación de reglas de asociación. Lo haréis con el fichero *Lastfm.csv* que encontraréis adjunto. Este fichero contiene un conjunto de registros del histórico de las canciones que ha escuchado un usuario en un portal Web de música. "artist" es el nombre del grupo que ha escuchado, "sex" y "country" corresponden a variables que describen al usuario.

El ejemplo 3 se pueden tomar como un punto de partida para la realización de este ejercicio, pero se espera que la respuesta proporcionda tenga un análisis más amplio al mostrado en este ejemplo, prestando especial atención a explicar el conocimiento que se ha adquirido tras el proceso de minería de datos.


## Ejemplo 3: Métodos de generación de reglas de asociación
En este ejemplo vamos trabajar el algoritmo "apriori" para obtener reglas de asociación a partir de un data set. Dichas reglas nos ayudarán a comprender cómo la información del data set se relaciona entre si.

Para dicho objetivo vamos a trabajar el dataset de Groceries, que ya viene incluido con las librerías de arules.
```{r message= FALSE, warning=FALSE}
# install.packages("arules")
library(arules)
data("Groceries")
```

Para saber más sobre este dataset ejecutar el comando "?Groceries".

Inspeccionamos el dataset y vemos que tiene un listado de elementos que fueron comprados juntos. Vamos a analizarlo un poco visualmente.
```{r message= FALSE, warning=FALSE}
inspect(head(Groceries, 5))
```

En el siguiente plot podemos ver que los tres elementos más vendidos son la leche entera, otras verduras y bollería. Dada la simplicidad del Dataset no se pueden hacer mucho más análisis. Pero para datasets más complejos miraríamos la frecuencia y distribución de todos los campos, en busca de posibles errores.
```{r message= FALSE, warning=FALSE}
itemFrequencyPlot(Groceries,topN=20,type="absolute")
```

Si lanzamos el algoritmo "apriori", generaremos directamente un set de reglas con diferente soporte, confianza y lift. El soporte indica cuantas veces se han encontrado las reglas {lsh => rhs} en el dataset, cuanto más alto mejor. La confianza habla de la probabilidad de que {rhs} se de en función de {lhs}. Y el lift es un parámetro que nos indica cuánto de aleatoriedad hay en las reglas. Un lift de 1 o menos es que las reglas son completamente fruto del azar.
```{r message= FALSE, warning=FALSE}
grocery_rules <- apriori(Groceries, parameter = list(support = 0.01, confidence = 0.5))

inspect(head(sort(grocery_rules, by = "confidence"), 3))
```

Podemos probar a ordenar las reglas por los diferentes parámetros, para ver que información podemos obtener.
```{r message= FALSE, warning=FALSE}
inspect(head(sort(grocery_rules, by = "support"), 3))
```

Ordenando por support vemos que, con un lift de 2 y una confianza del 51%, podemos decir que la gente que en la misma compra hacía verduras y yogurt, compraban también leche entera. Hay que tener en cuenta que la leche entera es por otro lado el elemento más vendido de la tienda.
```{r message= FALSE, warning=FALSE}
inspect(head(sort(grocery_rules, by = "lift"), 3))
```

Por otro lado, si ordenamos por lift, vemos que con un soporte del 1% y una confianza del 58%, la gente que compra cítricos y tubérculos compra también verduras.

Esta información nos puede ayudar a dar consejos a la dirección de la disposición de los elementos en la tienda o de que productos poner en oferta según lo que se ha comprado. Y si tuviéramos más información podríamos hacer análisis más profundos y ver que clientes compran exactamente qué.


## Respuesta ejercicio 4

## Carga, resumen y explicación de los datos

### Carga de los datos
En este primer punto lo que debemos de hacer es la carga de los datos, un resumen de lo que hemos leído, una explicación de los mismos, preparar los datos para el modelo y realizar un análisis de los mismos.

Por lo tanto, lo primero que debemos hacer es la carga del archivo clientes.csv:
```{r}
df_lastfm = read.csv("./data/lastfm.csv", stringsAsFactors = TRUE)
head(df_lastfm)
```

### Resumen de los datos

Resumen de la estructura del dataset:
```{r}
str(df_lastfm)
```

### Explicación de los datos

Revisamos la descripción de las variables y si los tipos de variable se corresponden al que hemos cargado:

**user**
    integer - id del usuario.
    
**artist**
    factor - artista escucahdo por el usuario.
    
**sex**
    factor - género del usuario (male o female).
    
**country**
    factor - país del usuario.
    
Vemos que tenemos 289955 registros y 4 variables que los caracterizan. 

Este dataset está formado por 1 variable numérica (user) y 3 variable categóricas.

### Preparación de los datos

Comprobamos que no haya datos sin valores:
```{r}
df_lastfm_copy = df_lastfm
colSums(is.na(df_lastfm_copy))
```

De la anterior ejecución vemos que todos los campos tienen valores.

Realizamos un summary del dataset resultante para hacernos una idea del conjunto de datos que vamos a manejar, es decir, cuáles son los valores más frecuentes, las clases que tenemos...
```{r}
summary(df_lastfm_copy)
```

Como podemos observar, las variables tienen valores comunes, es característico ver que hay más usuarios hombres que mujeres, además, los artistas escuchados están más nivelados, sin embargo, vemos que una gran parte del dataset está basado en las escuchas de usuarios de los Estados Unidos de América, este pasís junto con Reino Unido y Alemania representan ellos solos a más de un tercio de los países registrados en el dataset.

En la siguiente gráfica podemos observar la diferencia entre el histórico de usuarios hombres y mujeres:
```{r}
ggplot(data=df_lastfm_copy,aes(x=sex)) +
  geom_bar(fill = "#FF6666") + 
  ylab("Frecuencia") +
  xlab("Sexo") +
  ggtitle("Frecuencia según el sexo")
```

Podríamos hacer una representación gráfica de los artistas y de los países para ver el cómo se distrubuyen los datos, pero hay muchísimos niveles y si tratamos de acotar los niveles a un top 5 estaríamos viendo la misma información que nos proporciona la función summary, ejecutada anteriormente.

Por lo tanto, hemos comprobado que no hay registros sin datos y que los datos que contienen son coherentes, por lo que podemos proceder a aplicar el algoritmo de generación de reglas de asociación.

## Preparación de los datos y aplicación del algoritmo apriori

Aunque nuestro dataset tiene 4 campos (el identificador del usuario, el artista escuchado, el género del usuario y el país del usuario) realmente vamos a comprobar el cómo se relacionan los usuarios con los artistas escuchados, es decir, hacernos una idea según lo que están escuchando lo que podrían escuchar.

Para trabajar de forma óptima con el algoritmo apriori debemos trabajar con trasacciones, es por ello que volvemos a hacer la lecutra de los datos y nos quedamos con el usuario y el artista, tarda mucho menos si volvemos a hacer la lectura del fichero que convertir el dataframe a un objeto de trasacciones.

Un aspecto a destacar de la creación de las transacciones es que debemos definir el formato, puede ser single (si cada línea representa un ítem) o basket (si cada línea representa una transacción completa), en  nuestro caso el formato debe ser single ya que la transcción completa es el histórico de cada usuario y los items los artistas escuchados:
```{r}
library(arules)

transactions_lastfm = read.transactions(file = "./data/lastfm.csv", sep = ",", format = "single", header = TRUE, cols = c("user", "artist"), rm.duplicates = TRUE, encoding = "UTF-8")

transactions_lastfm
```

Como podemos apreciar, tenemos 15000 transacciones y 1004 items.


Las columnas de las transacciones se corresponden con los artistas de nuestro dataset:
```{r}
colnames(transactions_lastfm)[1:20]
```

Las filas se corresponden con las transacciones que tenemos en el dataset:
```{r}
rownames(transactions_lastfm)[1:20]
```

Podemos análizar los items que tenemos en nuestra transacciones, como vemos a continuación, tenemos los artistas correspondientes a la primera trasacción, es decir, al primer usuario (si abrimos el fichero comprobamos que tiene el mismo número de artistas y que se corresponde con estos):
```{r}
inspect(head(transactions_lastfm, 1))
```

Podemos ver cómo varía el tamaño de las transacciones:
```{r}
  ggplot(data.frame(size(transactions_lastfm)), aes(x = size(transactions_lastfm))) +
  geom_histogram() +
  labs(title = "Distribución del tamaño de las transacciones",
       x = "Tamaño") +
  theme_bw()
```

De la anterior gráfica podemos concluir que la gran mayoría de gente escucha entre 8 y 30 artistas, pero también podemos comprobar cuáles son los artistas más escuchados/más frecuentes, es decir, los que tienen un mayor soporte:
```{r}
itemFrequencyPlot(transactions_lastfm, topN = 10, type = "absolute")

frecuencia_items = itemFrequency(x = transactions_lastfm, type = "absolute")
head(sort(frecuencia_items, decreasing = TRUE), 10)
```

Como podemos apreciar, el top 10 de artistas más escuchados son de habla inglesa, lo cual es lógico ya que un tercio de los datos que tenemos son de Estados Unidos y Reino Unido.

Una vez que hemos hecho un pequeño análisis de las transacciones que tenemos, podemos aplicar el algoritmo apriori para obtener las asociaciones más frecuentes, de primeras vamos a crear un modelo con los parámetros por defecto del algoritmo y vamos a analizar los resultados obtenidos:
```{r}
lastfm_rules = apriori(transactions_lastfm, parameter = list(support = 0.01, confidence = 0.5))
```

Una vez generado el modelo, observamos el número de reglas encontradas:
```{r}
cat("El número de reglas encontradas son: ", length(lastfm_rules))
```

Finalmente, realizamos un análisis tanto del soporte, como de la confianza y del lift.

El soporte indica cuantas veces se han encontrado las reglas {lsh => rhs} en el dataset, cuanto más alto mejor ya que indicaría que hay una alta relación entre los artistas de si escucha uno escuchar el otro: 
```{r}
inspect(head(sort(lastfm_rules, by = "support"), 10))[, c(1:4)]
```

Tal y como podemos observar la gran mayoría de {rhs} se corresponden con los grupos más escuchados (Radiohead, Coldplay, the beatles), pero cabe destacar que se ha "colado" el grupo metallica como uno de los grupos que más relación tiene con su antecedente {lhs}, lo cual es normal, porque su grupo antecente megadeth su género músical es igual a metallica.

Además, podemos ver que los artistas más comunes tienen antecedentes similares al género músical que hacen, por ejemplo, hay gente que para escuchar a coldplay viene de snow patrol (hace rock alternativo) y hay gente que viene de keane o de muse, los cuales también hacen rock.

Por lo tanto, podemos ver que los usuarios que escuchan un tipo de género, también escucha otros grupos del mismo tipo de género muscial. Hay casos "especiales", como por ejemplo hay gente que escucha a david bowie o radiohead (rock) y están relacionados con the beatles, los cuales hacen un rock más popular, más comumente conocido como pop.

La confianza habla de la probabilidad de que {rhs} se de en función de {lhs}, a mayor confianza mayor va a ser la relación entre {rhs} y {lhs}:
```{r}
inspect(head(sort(lastfm_rules, by = "confidence"), 10))[, c(1:3,5)]
```

Tal y como podemos apreciar hay una fuerte relación entre determinados grupos que hacen música sobre un mismo género, como oasis y the killers con coldplay.

También podemos ver que hay relaciones entre grupos que sinceramente se hacen por lo famosos que son, por ejemplo que bob dylan y the rolling stones está fuertemente relacionado con the beatles, sin embargo, hacen géneros diferentes y son tipos de personalidad completamente diferentes.

Por lo tanto, nos podemos hacer una idea de que la gran mayoría de relaciones se hacen porque los artistas hacen o el mismo género o géneros similares, pero hay casos especiales, en lo que hay artistas famosos los cuales la gente los escucha por eso mismo, esto no implica que un usuario solo pueda escuchar un género o un determinado artista, pero llama la atención.

Finalmente, el lift es un parámetro que nos indica cuánto de aleatoriedad hay en las reglas. Un lift de 1 o menos es que las reglas son completamente fruto del azar:
```{r}
inspect(head(sort(lastfm_rules, by = "lift"), 10))[, c(1:3,7)]
```

La ejecución anterior nos da una información muy valiosa, hasta ahora las reglas se centraban más en los artistas "más famosos", sin embargo, el lift no está indicando que hay relaciones que no son tan aleaotrio como imaginábamos.

Por ejemplo, el grupo the pussycat dolls hacen música más orientado al cabaret y rihanna en ese caso su estilo musical es muy parecido, por lo que es lógico que no haya una aletoriadad en esa regla.

Otro caso son los que escuchan a pink floyd y the doors hacen un estilo de música muy especial caracterizada en los finales de los 60 y primeros de los 70, es por ello que lógicamente van a escuchar a led zeppelin ya que los géneros son similares y son de la misma época.

Otro caso similar es el de T.I. con kenye west ambos hacen rap más comercial y son de la misma época, por lo que tiene todo el sentido del mundo que escuchen a ambos artistas, ya que si te gusta ese género y tienes dos artistas similares los vas a escucahr sí o sí.

## Variando los parámetros de confianza y soporte

Una vez que ya hemos hecho un modelo de base que nos da una idea de cómo se relacionan los artistas, podemos tratar de mejorar ese modelo aumentando la confianza y soporte, de tal forma que vayamos obteniendo un mejor número de reglas y que estas reglas sean lo más claras posible, para tener la certeza que realmente sí que hay una relación.

En este apartado se podrían hacer muchos modelo, pero nos vamos a centrar en dos: el primero aumentado solamente el soporte y manteniendo la confianza, y un segundo modelo en el que aumentaremos solo la confianza.

Por lo tanto, en el primer modelo vamos a aumentar el soporte de 0.01 a 0.015, esto lo que significa es que aumentamos el nivel que el algormitmo considera como frecuente:
```{r}
lastfm_rules = apriori(transactions_lastfm, parameter = list(support = 0.015, confidence = 0.5))
```

Una vez generado el modelo, observamos el número de reglas encontradas:
```{r}
cat("El número de reglas encontradas son: ", length(lastfm_rules))
```

Tal y como podemos recordar, en el anterior modelo teníamos 50 reglas encontradas, en este caso ya son solo 9.

Finalmente, realizamos un análisis tanto del soporte, como de la confianza y del lift.

El soporte indica cuantas veces se han encontrado las reglas {lsh => rhs} en el dataset, cuanto más alto mejor ya que indicaría que hay una alta relación entre los artistas de si escucha uno escuchar el otro: 
```{r}
inspect(sort(lastfm_rules, by = "support"))[, c(1:4)]
```

Tal y como podemos observar la gran mayoría de {rhs} se corresponden con los grupos más escuchados (Radiohead, Coldplay, the beatles), pero cabe destacar que se ha "colado" el grupo metallica como uno de los grupos que más relación tiene con su antecedente {lhs}, lo cual es normal, porque su grupo antecente megadeth su género músical es igual a metallica.

Además, podemos ver que los artistas más comunes tienen antecedentes similares al género músical que hacen, por ejemplo, hay gente que para escuchar a coldplay viene de snow patrol (hace rock alternativo) y hay gente que viene de keane o de muse, los cuales también hacen rock.

Estas conclusiones obtenidas lógicamente son iguales que en el modelo anterior, ya que son los mismos datos lo único que ponemos más restricciones para no tener un conjunto de reglas tan alto, solo tener verdaderamente las que son llamativas.

La confianza habla de la probabilidad de que {rhs} se de en función de {lhs}, a mayor confianza mayor va a ser la relación entre {rhs} y {lhs}:
```{r}
inspect(sort(lastfm_rules, by = "confidence"))[, c(1:3,5)]
```

De la ejecución anterior podemos ver realmente la relación que hay, es decir, la probablidad de que se escuche al artista {rhs} dependiendo de {lhs}, esto puede ser por diversos factores.

Por ejemplo, en el primer caso tenemos que si escuchas keane problablemnte escuches coldplay, es lógico, son mismo género musical, ambos son de Reino Unido y como hemos visto anteriormente de los Estados Unidos y Reino Unido son la gran mayoría de los usuarios.

Otro caso similar al anterior, es el de megadeth con metallica, ambos son grupos que hacen la misma  Y lo mismo sucede con simon & garfunke y the beatles, ya que hacen la música parecida (los primeros fols rock y los segundo pop rock), son de la misma época de los 60, comparten el mismo público porque ambos son de habla inglesa...

Por lo tanto, nos podemos hacer una idea de que la gran mayoría de relaciones se hacen porque los artistas hacen o el mismo género o géneros similares, son de una misma zona geográfica, son de la misma época, cantan en el mismo idioma...

Finalmente, el lift es un parámetro que nos indica cuánto de aleatoriedad hay en las reglas. Un lift de 1 o menos es que las reglas son completamente fruto del azar:
```{r}
inspect(sort(lastfm_rules, by = "lift"))[, c(1:3,7)]
```

Algo curioso es que al aumentar  el soporte hemos obtenido un menor lift, esto significa que tenemos más aleatoriedad, lógicamente al aumentar el soporte estamos aumenta el nivel de frecuencia de los artistas, por lo que ahora solo aparecen los artistas más escuchados, Rihanna ya no sale ya que su frecuencia era menor.

Aunque haya disminuido el lift, la relaciones que se siguen produciendo son lógicas y para nada son producidas por el azar, como ya hemos comentado megadeth y metallica son grupso simalres, keane y coldplay también, por lo que no podemos hablar del azar en cuanto a estas reglas.

Finalmente, vamos a ajustar un poco más el modelo, y para ello vamos a manter el nivel de soporte pero aumetaremos el nivel de confianza de 0.5 a 0.525:
```{r}
lastfm_rules = apriori(transactions_lastfm, parameter = list(support = 0.015, confidence = 0.525))
```

Una vez generado el modelo, observamos el número de reglas encontradas:
```{r}
cat("El número de reglas encontradas son: ", length(lastfm_rules))
```

Tal y como podemos recordar, en el anterior modelo teníamos 9 reglas encontradas, en este caso ya son solo 4.

Finalmente, realizamos un análisis tanto del soporte, como de la confianza y del lift.

Respecto al soporte: 
```{r}
inspect(sort(lastfm_rules, by = "support"))[, c(1:4)]
```

En la ejecución anterior se mide la frecuencia, por lo que al aumentar la confianza buscamos que la probabilidad de que una regla suceda sea alta, por lo tanto ahora obtenemos solamente los grupos con mayor frecuencia y cuya relación es elevada.

Respecto a la confianza:
```{r}
inspect(sort(lastfm_rules, by = "confidence"))[, c(1:3,5)]
```

Al igual que sucedía en el anterior apartado, nos estamos fijando en los grupos más frecuentes cuya relación es muy probable y vamoe que la relación más probable es si escuchas a keane y coldplay, lo cual tal y como hemos venido comentando a lo largo de todo el ejercicio es normal, ya que son grupos similares, de la misma zona geográfica, que cantan en el mismo idioma y cuya música es parecida.

Finalmente, el lift es un parámetro que nos indica cuánto de aleatoriedad hay en las reglas:
```{r}
inspect(sort(lastfm_rules, by = "lift"))[, c(1:3,7)]
```

Al igual que sucedía con el modelo anterior, al ponernos más restrictivos con el modelo diminuye el lift, pero tal y como apreciamos sigue sin ser aletorias estas reglas, ya que cada una de ellas los artistas están relacionados.

## Conclusiones
Aunque las conclusiones se han ido comentando a lo largo del ejercicio, en este apartado vamos a tratar de hacer un resumen.

Para realizar este ejercicio hemos creado tres modelo apriori:

- El primero tenía un nivel de soporte de 0.01 y de confianza de 0.5: con esta configuración de parámetros hemos obtenido 50 reglas, las cuales buscaban de reflejar de una forma general el cómo se relacionaban los artistas. Hemos visto que la gran mayoría de las relaciones se producían porque hacían géneros de música similares, eran de la misma zona geográfica, hablaban el mismo idioma o porque escuchaban a grupos famosos.

- El segundo tenía un nivel de soporte de 0.015 y de confianza de 0.5: con esta configuración de parámetros hemos obtenido 9 reglas, las cuales eran más específicas que en el modelo anterior, hemos obtenido las mismas conclusiones que antes con la excepción de que la relaciones entre los artistas ya no estaban influenciadas por los famosos que fueran, es decir, su frecuencia, ya que el parámetro que hemos modifica (soporte) busca eso, considerar que el nivel de frecuencia sea más alto. Un aspecto a tener en cuenta era que a medida que nos poníamos más restrictivos se reducía el valor del lift, lógicamente hay más registros con esos grupos por lo que la aletoriedad va a aumentar ya que hay más casos.

- El tercero tenía un nivel de soporte de 0.015 y de confianza de 0.525: con esta configuración de parámetos hemos obtenido 4 reglas, las cuales eran más restrictivas que en los dos modelos anteriores, las conclusiones encuanto a cómo se relacionaban los artistas eran las mismas que en el modelo anterior. Al igual que sucedía en antes, al aumentar la confianza seguía aumentado la aletoriedad.

******
# Ejercicio 5
******
## Enunciado
Busca información sobre otros métodos de agregación diferentes al *k-means*. Partiendo del Ejercicio 3, probar el funcionamiento de al menos 2 métodos diferentes y comparar los resultados obtenidos.

## Respuesta ejercicio 5
Escribe aquí la respuesta







******
# Criterios de evaluación
******

## Ejercicio 1 (20%)
* 60%. Explicar de forma resumida cual es el objetivo de los métodos de agregación. Relacionar la respuesta con un ejemplo.
* 40%. Razonar si tiene sentido aplicar un método agregación al ejemplo que pusiste en la PEC1. Si lo tiene, comentar como se podría aplicar y que tipo de resultados se podrían obtener. Si no lo tiene, reformular el problema para que si lo tenga.


## Ejercicio 2 (20%)
* 60%. Explicar de forma resumida cual es el objetivo de los métodos de generación de reglas de asociación. Relacionar la respuesta con un ejemplo.
* 40%. Razonar si tiene sentido aplicar un método de generación de reglas de asociación al ejemplo que pusiste en la PEC1. Si lo tiene, comentar como se podría aplicar y que tipo de resultados se podrían obtener. Si no lo tiene, reformular el problema para que si lo tenga.


## Ejercicio 3 (25%)
* 20%. Se explican los campos de la base de datos, preparación y análisis de datos
* 20%. Se aplica el algoritmo de agrupamiento de forma correcta y se prueban con diferentes valores de k.
* 10%. Se obtiene una medida de lo bueno que es el agrupamiento.
* 40%. Se ponen nombres a las asociaciones y se describen e interpretan los diferentes clústers obtenidos.
* 10%. Se presenta el código y es fácilmente reproducible.

## Ejercicio 4 (25%)
* 10%. Se realiza un resumen de los datos incluidos en la base de datos.
* 15%. Se preparan los datos de forma correcta.
* 10%. Se aplica el algoritmo de reglas de asociación.
* 20%. Se realizan diferentes pruebas variando algunos parámetros.
* 35%. Se explican las conclusiones que se obtienen.
* 10%. Se presenta el código y es fácilmente reproducible.

## Ejercicio 5 (10%)
* 25%. Se prueba un algoritmo diferente al kmeans.
* 25%. Se prueba otro algoritmo diferente al kmeans.
* 40%. Se comparan los resultados del kmeans y los otros dos métodos probados en este ejercicio.
* 10%. Se presenta el código y es fácilmente reproducible.
