---
title: 'Minería de datos: PEC2'
author: "Autor: Mario Ubierna San Mamés"
date: "Abril 2021"
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
    includes:
      in_header: 75.584-PEC-header.html
  pdf_document:
    highlight: zenburn
    toc: yes
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

******
# Introducción
******
## Presentación
Esta prueba de evaluación continuada cubre los módulo 5 y 6 del programa de la asignatura.  

## Competencias
Las competencias que se trabajan en esta prueba son:

* Uso y aplicación de las TIC en el ámbito académico y profesional
* Capacidad para innovar y generar nuevas ideas.
* Capacidad para evaluar soluciones tecnológicas y elaborar propuestas de proyectos teniendo en cuenta los recursos, las alternativas disponibles y las condiciones de mercado.
* Conocer las tecnologías de comunicaciones actuales y emergentes, así como saberlas aplicar convenientemente para diseñar y desarrollar soluciones basadas en sistemas y tecnologías de la información.
* Aplicación de las técnicas específicas de ingeniería del software en las diferentes etapas del ciclo de vida de un proyecto.
* Capacidad para aplicar las técnicas específicas de tratamiento, almacenamiento y administración de datos.
* Capacidad para proponer y evaluar diferentes alternativas tecnológicas para resolver un problema concreto.
* Capacidad de utilizar un lenguaje de programación.  
* Capacidad para desarrollar en una herramienta IDE.  
* Capacidad de plantear un proyecto de minería de datos.  

## Objetivos
En esta PEC trabajaremos la generación, interpretación y evaluación de un modelo de agregación y de un modelo donde generaremos reglas de asociación con el software de practicas. No perderemos de vista las fases de preparación de los datos, calidad del modelo y extracción inicial del conocimiento.

## Descripción de la PEC a realizar
La prueba está estructurada en 2 ejercicios teóricos y 3 ejercicios prácticos.

## Recursos

Para realizar esta práctica recomendamos como punto de partida la lectura de los siguientes documentos:

* Módulos 5 y 6 del material didáctico.
* El aula laboratorio de R para resolver dudas o problemas.
* RStudio Cheat Sheet: Disponible en el aula Laboratorio de Minería de datos.
* R Base Cheat Sheet: Disponible en el aula Laboratorio de Minería de datos.
  

## Formato y fecha de entrega
El formato de entrega es: usernameestudiant-PAC1.html (pdf o word) y rmd.
Fecha de Entrega: 21/04/2021.
Se tiene que depositar la PEC en el buzón de entregas del aula.


## Nota: Propiedad intelectual 

A menudo es inevitable, al producir una obra multimedia, hacer uso de recursos creados por terceras personas. Es por lo tanto comprensible hacerlo en el marco de una práctica de los estudios de Informática, Multimedia y Telecomunicación de la UOC, siempre y cuando esto se documente claramente y no suponga plagio en la práctica. 

Por lo tanto, al presentar una práctica que haga uso de recursos ajenos, se debe presentar junto con ella un documento en qué se detallen todos ellos, especificando el nombre de cada recurso, su autor, el lugar dónde se obtuvo y su estatus legal: si la obra está protegida por el copyright o se acoge a alguna otra licencia de uso (Creative Commons, licencia GNU, GPL ...). 
El estudiante deberá asegurarse de que la licencia no impide específicamente su uso en el marco de la práctica. En caso de no encontrar la información correspondiente tendrá que asumir que la obra está protegida por copyright. 

Deberéis, además, adjuntar los ficheros originales cuando las obras utilizadas sean digitales, y su código fuente si corresponde.  

******
# Ejercicio 1
******

## Enunciado
1. Explica de forma resumida cual es el objetivo de los métodos de agregación. Relaciona la respuesta con un ejemplo. 
2. Razona si tiene sentido aplicar un método de agregación al ejemplo que pusiste en la PEC1. Si lo tiene, comenta como se podría aplicar y que tipo de resultados se podrían obtener. Si no lo tiene, reformula el problema para que si lo tenga.

## Respuesta ejercicio 1

Respecto al primer apartado, cuando hablamos de los métodos de agregación o clustering nos estamos refiriendo a un tipo de modelos de míneria de datos que nos permiten obtener una primera aproximación de la estructura de un dominio, es decir, nos permite discernir qué elementos son parecidos, pero sin poner de primeras ningún criterio.

Por lo tanto, el objetivo de este tipo de modelos es obtener/discernir grupos de elementos/objetos dentro de un dominio, en el cual a priori desconocemos el cómo se pueden agrupar dichos elementos ya que no tenemos una variable que nos clasifique los elementos, es decir, este tipo de modelos son parecidos a los modelos de clasificación, ambos tratan de identificar qué elemento pertenece a qué grupo gracias a características similares, sin embargo, los modelos de agregación son modelos no supervisados, mientras que los de clasificación sí que lo son. Es por ello que cuando aplicamos un modelo de clustering es porque tenemos un desconocmiento de lo que sabemos.

Un buen ejemplo en el uso de modelos de clustering podría ser el siguiente, imaginémonos que tenemos una tienda online de ropa, y queremos identificar cuáles son los diferentes tipos de clientes que nos compran para así poder aplicar mejores descuentos, o ver qué tipo de ropa se compra dentro de cada grupo, o mejorar las promociones en nuestra tienda...

Ante el ejemplo anterior tenemos el objetivo claro, es decir, necesitamos saber qué tipo de clientes hay en nuestra tienda, cómo se agrupan los clientes con características similares, pero tenemos un problema, hay un desconocimiento del dominio, es decir, no sabemos cómo se agrupan los clientes con características similares, ¿se agrupan por edad?, ¿por sexo?, ¿por estatura?, ¿por peso?, ¿por clase social?... Como podemos apreciar tenemos un gran desconocimiento del problema, no tenemos información suficiente.

Es en este tipo de problemas en los que se hace uso de un modelo de clustering, porque nos va a permitir discernir grupos sin tener el conocimiento suficiente del problema.

Respecto al segundo apartado, lo primero que tenemos que saber es el objetivo de la PEC1. En dicha prática los datos que usamos fueron los del censo en los Estados Unidos de América en el año 1994, es decir, se recogía información sobre la edad, la clase de trabajo, la educación recibida, la raza, el estado civil, los ingresos, las pérdidas...

Como podemos apreciar, la información era variada y nos permitía hacernos una idea de cómo era la sociedad en aquella época. El objetivo de ese conjunto de datos era el de clasificarlos, es decir, aplicar un modelo de clasificación para poder discenir dos grupos, aquellos en los que el salario anual era mayor a 50K dólares y los que era menor o igual a 50K.

Era un problema de clasificación ya que sabíamos el valor en todo momento de la variable objetivo, es decir, sabíamos para cada registro si tenía un salario mayor o menor a 50K.

Por lo tanto, tal cual era el ejemplo de la PEC1 no tendría sentido modelarlo como si fuera un problema de clustering, ya que como bien hemos mencionado, se sabía en todo momento el valor de la variable objetivo. Sin embargo, si no tuviéramos ese conocimiento sí que tendría sentido hacer uso de un método de agregación, ya que buscaría discernir grupos sin saber cuántos hay, cómo se relacionan...

Suponiendo que no tenemos el conocimiento suficiente respecto al ejemplo de la PEC1, podríamos aplicar por ejemplo un método de agregación como el k-means, en el cual tenemos que predefinir el número de grupos. Si ponemos que el número de grupos es dos, podríamos comprobar el cómo clasifca cada uno de los elementos y si es parecido al resolver el problema usando un modelo de clasificación. Podríamos, poner más grupos o no, dependiendo de la granularidad que queremos en los grupos, a mayor número de grupos más específico va a ser nuestro modelo, y a menor número de grupos más general va a ser.

Resumiendo, sí que podríamos hacer uso de un método de agregación en la PEC1, siempre y cuando quisiéramos analizar el cómo se agrupan los diferentes elementos y no tenemos claro un objetivo para agrupar, ya sea por raza, por sexo, por edad, por ingresos, por pérdidas... Al hacer uso de un modelo de clustering podríamos analizar el cómo se agrupan, cuáles son las características similares de cada grupo...

******
# Ejercicio 2
******
## Enunciado
1. Explica de forma resumida cual es el objetivo de los métodos de generación de reglas de asociación. Relaciona la respuesta con un ejemplo. 
2. Razona si tiene sentido aplicar un método de generación de reglas de asociación al ejemplo que pusiste en la PEC1. Si lo tiene, comenta como se podría aplicar y que tipo de resultados se podrían obtener. Si no lo tiene, reformula el problema para que si lo tenga.

## Respuesta ejercicio 2

Respecto al primer apartado. hay que hacer una distinción entre los método de clustering y los método de generación de reglas de asociación, ambos están enfocados al aprendizaje no supervisado, es decir, desconocemos la variable objetivo. Sin embargo, hay una clara diferencia entre ambos modelos.

Los modelos de clustering, tal y como se explicó en el anterior ejercicio, buscan agrupar elementos de los cuales no tenemos el conocimiento suficiente, a partir de la distancia entre elementos y su similitud. Por otro lado, los métodos de generación de reglas de asociación tratan de encontrar relaciones entre atributos que tienden a ocurrir de forma conjunta, es decir, una relación lógica si A entonces B (A => B).

Como podemos apreciar el enfoque a la hora de resolver el problema es diferente, lo que buscamos con la reglas de asociación es poder describir el conjunto de datos que tenemos, para así poder saber si hay una relación entre atributos o no.

En estadística usaríamos un contraste de hipótesis para saber si una hipótesis/condición se cumple o no. Sin embargo, con la reglas de asociación se busca el ver qué condiciones se pueden cumplir con los datos que tenemos, para ello se define tanto un nivel de confianza como de soporte.

Siguiendo con el ejemplo establecido en el ejercicio anterior, si tenemos una tienda online que se encarga de vender ropa, podríamos hacer uso de un método de generación de reglas de asociación para así saber qué atributos (sexo, edad, localización, clase social...) están relacionados a la ahora de comprar un producto, en nuestro caso, una determinada camiseta.

Para ello, definiríamos un nivel de confianza y de soporte mínimo, e iríamos comprobando si al definir una regla de asociación el nivel de confianza y de soporte que obtenemos es igual o superior a los niveles mínimos que hemos definido, de no ser así buscaríamos otra regla de asociación.

Respecto al segundo apartado, tal y como se comentó en el anterior ejercicio, el objetivo de los datos de la PEC1 era el de clasificar ya que conocíamos la variable objetivo, es decir, el salario.

Sin embargo, sí que podríamos hacer uso de métodos de generación de reglas de asociación al proporcionanos otro enfoque, ya que podríamos buscar cuáles son las condiciones que hacen que un determinado registro cumpla que tenga unos ingresos/pérdidas mayores o menores, decir, podríamos comprobar si el sexo, la edad, y la raza por ejemplo condicionan los ingresos/pérdidas de una persona en los Estados Unidos de América en 1994. Este enfoque es completamente direfente al de la clasificación y al del clustering, en el primero se busca predecir la etiqueta ya que conocemos los grupos, en el segundo se busca agrupar por similitudes ya que desconocemos las etiquetas, y el enfoque de las reglas de asociación es el descubrir el qué condiciona los datos.

Un aspecto a destacar de este tipo de métodos, es que necesita la información de forma binarizada, es decir, el dato solo puede ser un 1 o un 0, por ejemplo: o es hombre o mujer, o es negro o no lo es, o es joven o no lo es... Por lo tanto, tendríamos que transformar el dataset original de la PEC1 para poder hacer uso de estos tipos de métodos.

Una vez que ya tenemos la información binariza, podríamos hacer uso de diferentes métodos como el apriori, eclat... Uno de los más usados es el apriori, el cual se podría dividir en dos fases, en la primera identificar todos los conjuntos de elementos que ocurren con un frecuencia superior a un límite definido, y una segunda en la cual convertimos esos conjuntos de elementos en reglas de asociación.

Cabe destacar que podríamos obtener más de una regla de asociación que cumplan el límite mínimo que hemos definido. Una vez que tenemos las reglas de asociación, podemos saber las condiciones que se cumplen para poder predecir.

Resumiendo, el dataset de la PEC1 está enfocado al aprendizaje supervisado ya que conocemos en todo momento la variable objetivo, pero si no fuera así, podríamos hacer uso método de generación de reglas de asocición para ver cómo se relacionan los atributos, es decir, una consecuencia como mayores ingresos o pérdidas de qué condiciones depende.

******
# Ejercicio 3
******
## Enunciado
En este ejercicio seguiréis los pasos del ciclo de vida de un proyecto de minería de datos para el caso de un algoritmo de agregación. Lo haréis con el fichero *clientes.csv* que encontraréis adjunto. No hay que olvidarse de la fase de preparación y análisis de datos. Es muy importante explicar muy bien los resultados obtenidos.

Los ejemplos 1 y 2 se pueden tomar como un punto de partida para la realización de este ejercicio, pero se espera que la respuesta proporcionda tenga un análisis más amplio al mostrado en estos ejemplos, prestando especial atención a explicar el conocimiento que se ha adquirido tras el proceso de minería de datos.


## Ejemplo 1: Métodos de agregación con datos autogenerados 
En este ejemplo vamos a generar un conjunto de muestras aleatorias para posteriormente usar el algoritmo kmeans para agruparlas. Se crearán las muestras alrededor de dos puntos concretos. Por lo tanto, lo lógico será agrupar en dos clústers. Puesto que inicialmente, en un problema real, no se conoce cual es el número más idóneo de clústers k, vamos a probar primero con dos (el valor óptimo) y posteriormente con 4 y 8 clústers. Para evaluar la calidad de cada proceso de agregación vamos a usar la silueta media. La silueta de cada muestra evalúa como de bien o mal está clasificada la muestra en el clúster al que ha sido asignada. Para ello se usa una fórmula que tiene en cuenta la distancia a las muestras de su clúster y la distancia a las muestras del clúster vecino más cercano. 

A la hora de probar el código que se muestra, es importante tener en cuenta que las muestras se generan de forma aleatoria y también que el algoritmo kmeans tiene una inicialización aleatoria. Por lo tanto, en cada ejecución se obtendrá unos resultados ligeramente diferentes.

Lo primero que hacemos es cargar la librería cluster que contiene las funciones que se necesitan

```{r message= FALSE, warning=FALSE}
library(cluster)
```
Generamos las muestras de forma aleatoria tomando como centro los puntos [0,0] y [5,5].
```{r message= FALSE, warning=FALSE}
n <- 150 # número de muestras
p <- 2   # dimensión

sigma <- 1 # varianza de la distribución
mean1 <- 0 # centro del primer grupo
mean2 <- 5 # centro del segundo grupo

n1 <- round(n/2) # número de muestras del primer grupo
n2 <- round(n/2) # número de muestras del segundo grupo

x1 <- matrix(rnorm(n1*p,mean=mean1,sd=sigma),n1,p)
x2 <- matrix(rnorm(n2*p,mean=mean2,sd=sigma),n2,p)
```

Juntamos todas las muestras generadas y las mostramos en una gráfica
```{r message= FALSE, warning=FALSE}
x  <- rbind(x1,x2)
plot (x)
```

Como se puede comprobar las muestras están claramente separadas en dos grupos. Si se quiere complicar el problema se puede modificar los puntos centrales (mean1 y mean2) haciendo que estén más próximos y/o ampliar la varianza (sigma) para que las muestras estén más dispersas.

A continuación vamos a aplicar el algoritmo kmeans con 2, 4 y 8 clústers
```{r message= FALSE, warning=FALSE}
fit2       <- kmeans(x, 2)
y_cluster2 <- fit2$cluster

fit4       <- kmeans(x, 4)
y_cluster4 <- fit4$cluster

fit8       <- kmeans(x, 8)
y_cluster8 <- fit8$cluster
```
Las variables y_cluster2, y_cluster4 e y_cluster8 contienen para cada muestra el identificador del clúster a las que han sido asignadas. Por ejemplo, en el caso de los k=2 las muestras se han asignado al clúster 1 o al 2

```{r message= FALSE, warning=FALSE}
y_cluster2
```

Para visualizar los clústers podemos usar la función clusplot. Vemos la agrupación con 2 clústers
```{r message= FALSE, warning=FALSE}
clusplot(x, fit2$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)
```

con 4
```{r message= FALSE, warning=FALSE}
clusplot(x, fit4$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)
```

y con 8
```{r message= FALSE, warning=FALSE}
clusplot(x, fit8$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)
```

También podemos visualizar el resultado del proceso de agregación con el siguiente código para el caso de 2 clústers
```{r message= FALSE, warning=FALSE}
plot(x[y_cluster2==1,],col='blue', xlim=c(min(x[,1]), max(x[,1])), ylim=c(min(x[,2]), max(x[,2])))
points(x[y_cluster2==2,],col='red')
```

para 4
```{r message= FALSE, warning=FALSE}

plot(x[y_cluster4==1,],col='blue', xlim=c(min(x[,1]), max(x[,1])), ylim=c(min(x[,2]), max(x[,2])))
points(x[y_cluster4==2,],col='red')
points(x[y_cluster4==3,],col='green')
points(x[y_cluster4==4,],col='black')
```

y para 8
```{r message= FALSE, warning=FALSE}
plot(x[y_cluster8==1,],col='blue', xlim=c(min(x[,1]), max(x[,1])), ylim=c(min(x[,2]), max(x[,2])))
points(x[y_cluster8==2,],col='red')
points(x[y_cluster8==3,],col='green')
points(x[y_cluster8==4,],col='black')
points(x[y_cluster8==5,],col='yellow')
points(x[y_cluster8==6,],col='purple')
points(x[y_cluster8==7,],col='cyan')
points(x[y_cluster8==8,],col='orange')
```

Ahora vamos a evaluar la calidad del proceso de agregación. Para ello usaremos la función silhouette que calcula la silueta de cada muestra

```{r message= FALSE, warning=FALSE}
d  <- daisy(x) 
sk2 <- silhouette(y_cluster2, d)
sk4 <- silhouette(y_cluster4, d)
sk8 <- silhouette(y_cluster8, d)
```

La función silhouette devuelve para cada muestra, el clúster dónde ha sido asignado, el clúster vecino y el valor de la silueta. Por lo tanto, calculando la media de la tercera columna podemos obtener una estimación de la calidad del agrupamiento

```{r message= FALSE, warning=FALSE}
mean(sk2[,3])
mean(sk4[,3])
mean(sk8[,3])
```

Como se puede comprobar, agrupar con dos clúster es mejor que en 4 o en 8, lo cual es lógico teniendo en cuenta como se han generado los datos.

En este ejercicio no podemos sacar ninguna conclusión a partir de los clusters obtenidos puesto que los datos de partida no se corresponden con ningún problema real. En un ejemplo real tendríamos que analizar las agrupaciones obtenidas para obtener conocimiento sobre el problema a resolver.

## Ejemplo 2: Métodos de agregación con datos reales 
A continuación vamos a ver otro ejemplo de cómo se usan los modelos de agregación. Para ello usaremos el fichero *flores.csv* que encontraréis adjunto.  

Cargamos el fichero y visualizamos la estructura de los datos
```{r message= FALSE, warning=FALSE}
library(cluster)
flores_data<-read.csv("flores.csv", header=T, sep=",")
colnames(flores_data) <- c("sepalLength", "sepalWidth", "petalLength", "petalWidth")
summary(flores_data)
```

Como podemos comprobar tenemos cuatro características, la longitud y anchura del sépalo y la longitud y anchura del pétalo.
 
Como inicialmente no conocemos el número óptimo de clústers, probamos con varios valores
```{r message= FALSE, warning=FALSE}
d <- daisy(flores_data) 
resultados <- rep(0, 10)
for (i in c(2,3,4,5,6,7,8,9,10))
{
  fit           <- kmeans(flores_data, i)
  y_cluster     <- fit$cluster
  sk            <- silhouette(y_cluster, d)
  resultados[i] <- mean(sk[,3])
}
```


Mostramos en un gráfica los valores de las siluetas media de cada prueba para comprobar que número de clústers es el mejor
```{r message= FALSE, warning=FALSE}
plot(2:10,resultados[2:10],type="o",col="blue",pch=0,xlab="Número de clusters",ylab="Silueta")
```

El mejor valor que se obtiene es k=2.

Otro forma de evaluar cual es el mejor número de clústers es considerar el mejor modelo, aquel que ofrece la menor suma de los cuadrados de las distancias de los puntos de cada grupo con respecto a su centro (withinss), con la mayor separación entre centros de grupos (betweenss). Como se puede comprobar es una idea conceptualmente similar a la silueta. Una manera común de hacer la selección del número de clústers consiste en aplicar el método elbow (codo), que no es más que la selección del número de clústers en base a la inspección de la gráfica que se obtiene al iterar con el mismo conjunto de datos para distintos valores del número de clústers. Se seleccionará el valor que se encuentra en el "codo" de la curva

```{r message= FALSE, warning=FALSE}
resultados <- rep(0, 10)
for (i in c(2,3,4,5,6,7,8,9,10))
{
  fit           <- kmeans(flores_data, i)
  resultados[i] <- fit$tot.withinss
}
plot(2:10,resultados[2:10],type="o",col="blue",pch=0,xlab="Número de clusters",ylab="tot.tot.withinss")
```

En este caso el número óptimo de clústers es 3 o 4 que es cuando la curva comienza a estabilizarse.

También se puede usar la función *kmeansruns* del paquete fpc que ejecuta el algoritmo kmeans con un conjunto de valores, para después seleccionar el valor del número de clústers que mejor funcione de acuerdo a dos criterios: la silueta media ("asw") y Calinski-Harabasz ("ch").

```{r message= FALSE, warning=FALSE}
library(fpc)
fit_ch  <- kmeansruns(flores_data, krange = 1:10, criterion = "ch") 
fit_asw <- kmeansruns(flores_data, krange = 1:10, criterion = "asw") 
```

Podemos comprobar el valor con el que se ha obtenido el mejor resultado y también mostrar el resultado obtenido para todos los valores de k usando ambos criterios

```{r message= FALSE, warning=FALSE}
fit_ch$bestk
fit_asw$bestk

plot(1:10,fit_ch$crit,type="o",col="blue",pch=0,xlab="Número de clústers",ylab="Criterio Calinski-Harabasz")
plot(1:10,fit_asw$crit,type="o",col="blue",pch=0,xlab="Número de clústers",ylab="Criterio silueta media")

```

Los resultados son muy parecidos a los que hemos obtenido anteriormente. Con el criterio de la silueta media se obtienen 2 clústers y con el Calinski-Harabasz se obtienen 3.

Como se ha comprobado, conocer el número óptimo de clústers no es un problema fácil. Tampoco lo es la evaluación de los modelos de agregación.

Tras las pruebas que hemos realizado para obtener el número óptimo de clusters, hemos encontrado que el número de clusters varía según el método entre 2, 3 o 4. Vamos a estudiar los resultados encontrados con 2 y 3 clusters.

A continuación mostramos visualmente los clusters encontrados suponiendo que hay 3 clusters. Hay que tener en cuenta que nos es posible mostrar los clusters en un espacio de 4 dimensiones. Por lo tanto, mostramos los clusters entre pares de caracteríticas:

```{r message= FALSE, warning=FALSE}
cl3 <- kmeans(flores_data, 3)
with(flores_data, pairs(flores_data, col=c(1:4)[cl3$cluster])) 
```

La gráfica (que puede cambiar de una ejecución a otra por el factor aleatorio del kmeans) muestra claramente que hay un cluster que está más diferenciado de los otros dos. 

Una buena técnica que ayuda a entender los grupos que se han formado, es analizar las características de cada grupo:

* Grupo 1: Valores del pétalo alto. Longitud del sépalo alto.
* Grupo 2: Valores del pétalo bajos. Longitud del sépalo bajo.
* Grupo 3: Valores intermedios salvo en el ancho del sépalo que es intermedio.

Vamos a ahora a estudiar los conjuntos que se han obtenido con 2 clusters:

```{r message= FALSE, warning=FALSE}
cl2 <- kmeans(flores_data, 2)
with(flores_data, pairs(flores_data, col=c(1:4)[cl2$cluster])) 
```

En este caso se comprueba que existen dos grupos con un frontera bastante clara y que con los valores del pétalo tenemos suficiente para diferenciar los dos grupos. Podemos describir los dos grupos como:

* Grupo 1: Valores del pétalo bajos.
* Grupo 2: Valores del pétalo alto.

Esto último lo podemos comprobar usando únicamente las dos características relacionadas con el pétalo:

```{r message= FALSE, warning=FALSE}
cl2b <- kmeans(flores_data[c(3,4)], 2)
plot(flores_data[c(3,4)], col=cl2b$cluster)

```

## Respuesta ejercicio 3
Escribe aquí la respuesta

******
# Ejercicio 4
******
## Enunciado
En este ejercicio seguiréis los pasos del ciclo de vida de un proyecto de minería de datos para el caso de un algoritmo de generación de reglas de asociación. Lo haréis con el fichero *Lastfm.csv* que encontraréis adjunto. Este fichero contiene un conjunto de registros del histórico de las canciones que ha escuchado un usuario en un portal Web de música. "artist" es el nombre del grupo que ha escuchado, "sex" y "country" corresponden a variables que describen al usuario.

El ejemplo 3 se pueden tomar como un punto de partida para la realización de este ejercicio, pero se espera que la respuesta proporcionda tenga un análisis más amplio al mostrado en este ejemplo, prestando especial atención a explicar el conocimiento que se ha adquirido tras el proceso de minería de datos.


## Ejemplo 3: Métodos de generación de reglas de asociación
En este ejemplo vamos trabajar el algoritmo "apriori" para obtener reglas de asociación a partir de un data set. Dichas reglas nos ayudarán a comprender cómo la información del data set se relaciona entre si.

Para dicho objetivo vamos a trabajar el dataset de Groceries, que ya viene incluido con las librerías de arules.
```{r message= FALSE, warning=FALSE}
# install.packages("arules")
library(arules)
data("Groceries")
```

Para saber más sobre este dataset ejecutar el comando "?Groceries".

Inspeccionamos el dataset y vemos que tiene un listado de elementos que fueron comprados juntos. Vamos a analizarlo un poco visualmente.
```{r message= FALSE, warning=FALSE}
inspect(head(Groceries, 5))
```

En el siguiente plot podemos ver que los tres elementos más vendidos son la leche entera, otras verduras y bollería. Dada la simplicidad del Dataset no se pueden hacer mucho más análisis. Pero para datasets más complejos miraríamos la frecuencia y distribución de todos los campos, en busca de posibles errores.
```{r message= FALSE, warning=FALSE}
itemFrequencyPlot(Groceries,topN=20,type="absolute")
```

Si lanzamos el algoritmo "apriori", generaremos directamente un set de reglas con diferente soporte, confianza y lift. El soporte indica cuantas veces se han encontrado las reglas {lsh => rhs} en el dataset, cuanto más alto mejor. La confianza habla de la probabilidad de que {rhs} se de en función de {lhs}. Y el lift es un parámetro que nos indica cuánto de aleatoriedad hay en las reglas. Un lift de 1 o menos es que las reglas son completamente fruto del azar.
```{r message= FALSE, warning=FALSE}
grocery_rules <- apriori(Groceries, parameter = list(support = 0.01, confidence = 0.5))

inspect(head(sort(grocery_rules, by = "confidence"), 3))
```

Podemos probar a ordenar las reglas por los diferentes parámetros, para ver que información podemos obtener.
```{r message= FALSE, warning=FALSE}
inspect(head(sort(grocery_rules, by = "support"), 3))
```

Ordenando por support vemos que, con un lift de 2 y una confianza del 51%, podemos decir que la gente que en la misma compra hacía verduras y yogurt, compraban también leche entera. Hay que tener en cuenta que la leche entera es por otro lado el elemento más vendido de la tienda.
```{r message= FALSE, warning=FALSE}
inspect(head(sort(grocery_rules, by = "lift"), 3))
```

Por otro lado, si ordenamos por lift, vemos que con un soporte del 1% y una confianza del 58%, la gente que compra cítricos y tubérculos compra también verduras.

Esta información nos puede ayudar a dar consejos a la dirección de la disposición de los elementos en la tienda o de que productos poner en oferta según lo que se ha comprado. Y si tuviéramos más información podríamos hacer análisis más profundos y ver que clientes compran exactamente qué.


## Respuesta ejercicio 4
Escribe aquí la respuesta


******
# Ejercicio 5
******
## Enunciado
Busca información sobre otros métodos de agregación diferentes al *k-means*. Partiendo del Ejercicio 3, probar el funcionamiento de al menos 2 métodos diferentes y comparar los resultados obtenidos.

## Respuesta ejercicio 5
Escribe aquí la respuesta







******
# Criterios de evaluación
******

## Ejercicio 1 (20%)
* 60%. Explicar de forma resumida cual es el objetivo de los métodos de agregación. Relacionar la respuesta con un ejemplo.
* 40%. Razonar si tiene sentido aplicar un método agregación al ejemplo que pusiste en la PEC1. Si lo tiene, comentar como se podría aplicar y que tipo de resultados se podrían obtener. Si no lo tiene, reformular el problema para que si lo tenga.


## Ejercicio 2 (20%)
* 60%. Explicar de forma resumida cual es el objetivo de los métodos de generación de reglas de asociación. Relacionar la respuesta con un ejemplo.
* 40%. Razonar si tiene sentido aplicar un método de generación de reglas de asociación al ejemplo que pusiste en la PEC1. Si lo tiene, comentar como se podría aplicar y que tipo de resultados se podrían obtener. Si no lo tiene, reformular el problema para que si lo tenga.


## Ejercicio 3 (25%)
* 20%. Se explican los campos de la base de datos, preparación y análisis de datos
* 20%. Se aplica el algoritmo de agrupamiento de forma correcta y se prueban con diferentes valores de k.
* 10%. Se obtiene una medida de lo bueno que es el agrupamiento.
* 40%. Se ponen nombres a las asociaciones y se describen e interpretan los diferentes clústers obtenidos.
* 10%. Se presenta el código y es fácilmente reproducible.

## Ejercicio 4 (25%)
* 10%. Se realiza un resumen de los datos incluidos en la base de datos.
* 15%. Se preparan los datos de forma correcta.
* 10%. Se aplica el algoritmo de reglas de asociación.
* 20%. Se realizan diferentes pruebas variando algunos parámetros.
* 35%. Se explican las conclusiones que se obtienen.
* 10%. Se presenta el código y es fácilmente reproducible.

## Ejercicio 5 (10%)
* 25%. Se prueba un algoritmo diferente al kmeans.
* 25%. Se prueba otro algoritmo diferente al kmeans.
* 40%. Se comparan los resultados del kmeans y los otros dos métodos probados en este ejercicio.
* 10%. Se presenta el código y es fácilmente reproducible.
