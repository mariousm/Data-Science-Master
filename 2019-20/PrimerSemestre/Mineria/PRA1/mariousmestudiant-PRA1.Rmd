---
title: 'Minería de datos: PRA1 - Selección y preparación de un juego de datos'
author: "Autor: Mario Ubierna San Mamés"
date: "Abril 2021"
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 3
    includes:
      in_header: 75.584-PEC-header.html
  word_document: default
  pdf_document:
    highlight: zenburn
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
******
# Introducción
******
## Presentación
Esta práctica cubre de forma transversal la asignatura.

Las Prácticas 1 y 2 de la asignatura se plantean de una forma conjunta de modo que la Práctica 2 será continuación de la 1.

El objetivo global de las dos prácticas consiste en seleccionar uno o varios juegos de datos, realizar las tareas de **preparación y análisis exploratorio** con el objetivo de disponer de datos listos para **aplicar algoritmos** de clustering, asociación y clasificación.

## Competencias
Las competencias que se trabajan en esta prueba son:  

* Uso y aplicación de las TIC en el ámbito académico y profesional.
* Capacidad para innovar y generar nuevas ideas.
* Capacidad para evaluar soluciones tecnológicas y elaborar propuestas de proyectos teniendo en cuenta los recursos, las alternativas disponibles y las condiciones de mercado.
* Conocer las tecnologías de comunicaciones actuales y emergentes así como saberlas aplicar convenientemente para diseñar y desarrollar soluciones basadas en sistemas y tecnologías de la información.
* Aplicación de las técnicas específicas de ingeniería del software en las diferentes etapas del ciclo de vida de un proyecto.
* Capacidad para aplicar las técnicas específicas de tratamiento, almacenamiento y administración de datos.
* Capacidad para proponer y evaluar diferentes alternativas tecnológicas para resolver un problema concreto.

## Objetivos
La correcta asimilación de todos los aspectos trabajados durante el semestre.  
En esta práctica abordamos un caso real de minería de datos donde tenemos que poner en juego todos los conceptos trabajados.
Hay que trabajar todo el ciclo de vida del proyecto. Desde el objetivo del proyecto hasta la implementación del conocimiento encontrado pasando por la preparación, limpieza de los datos, conocimiento de los datos, generación del modelo, interpretación y evaluación.

## Descripción de la PRA a realizar

## Recursos Básicos
Material docente proporcionado por la UOC. 

## Criterios de valoración

**Ejercicios prácticos** 

Para todas las PRA es **necesario documentar** en cada apartado del ejercicio práctico que se ha hecho y como se ha hecho.

## Formato y fecha de entrega PRA_1
El formato de entrega es: usernameestudiant-PRAn.html/doc/docx/odt/pdf  
Fecha de entrega: 05/05/2021  
Se debe entregar la PRA_1 en el buzón de entregas del aula  

## Nota: Propiedad intelectual 

> A menudo es inevitable, al producir una obra multimedia, hacer uso de recursos creados por terceras personas. Es por lo tanto comprensible hacerlo en el marco de una práctica de los estudios de Informática, Multimedia y Telecomunicación de la UOC, siempre y cuando esto se documente claramente y no suponga plagio en la práctica. 

> Por lo tanto, al presentar una práctica que haga uso de recursos ajenos, se debe presentar junto con ella un documento en que se detallen todos ellos, especificando el nombre de cada recurso, su autor, el lugar donde se obtuvo y su estatus legal: si la obra esta protegida por el copyright o se acoge a alguna otra licencia de uso (Creative Commons, licencia GNU, GPL ...). 
El estudiante deberá asegurarse de que la licencia no impide específicamente su uso en el marco de la práctica. En caso de no encontrar la información correspondiente tendrá que asumir que la obra esta protegida por copyright. 

> Deberéis, además, adjuntar los ficheros originales cuando las obras utilizadas sean digitales, y su código fuente si corresponde.  

******
# Enunciado
******
Todo estudio analítico debe nacer de una necesidad por parte del **negocio** o de una voluntad de dotarle de un conocimiento contenido en los datos y que solo podremos obtener a través de una colección de buenas prácticas basadas en la Minería de Datos.  

El mundo de la analítica de datos se sustenta en 3 ejes:  

1. Uno de ellos es el profundo **conocimiento** que deberíamos tener **del negocio** al que tratamos de dar respuestas mediante los estudios analíticos.  

2. El otro gran eje es sin duda las **capacidades analíticas** que seamos capaces de desplegar y en este sentido, las dos prácticas de esta asignatura pretenden que el estudiante realice un recorrido sólido por este segundo eje.  

3. El tercer eje son los **Datos**. Las necesidades del Negocio deben concretarse con preguntas analíticas que a su vez sean viables responder a partir de los datos de que disponemos. La tarea de analizar los datos es sin duda importante, pero la tarea de identificarlos y obtenerlos va a ser para un analista un reto permanente.  

Como **primera parte** del estudio analítico que nos disponemos a realizar, se pide al estudiante que complete los siguientes pasos:   


1. Plantear un problema de analítica de datos detallando los objetivos analíticos y explica una metodología para resolverlos de acuerdo con lo que se ha practicado en las PEC y lo que se ha aprendido en el material didáctico.

2. Seleccionar un juego de datos y justificar su elección. El juego de datos deberá tener capacidades para que se le puedan aplicar algoritmos supervisados, algoritmos no supervisados y reglas de asociación y deberá estar alineado con el problema analítico planteado en el paso anterior.   
El juego de datos deberá tener como mínimo 100 observaciones y debe ser distinto del usado en las PEC anteriores.

3. Realizar un análisis exploratorio del juego de datos seleccionado.   

4. Realizar tareas de limpieza y acondicionado para poder ser usado en procesos de modelado.

5. Realizar métodos de discretización

6. Aplicar un estudio PCA sobre el juego de datos. A pesar de no estar explicado en el material didáctico, se valorará si en lugar de PCA investigáis por vuestra cuenta y aplicáis SVD (Single Value Decomposition).


******
# Solución
******

## Planteamiento del problema de analítica de datos

En esta primera fase lo que debemos de realizar es el planteamiento del problema de analítica que vamos a resolver, detallando cuáles son los objetivos analíticos y explicar qué metodologías podríamos seguir para resolver dicho problema.

El cáncer es una de las mayores enfermades que sufrimos diariamente, todo el mundo conoce a gente que ha padecido o padece de dicha enfermedad, cuando escuchamos la palabra cáncer nos ponemos en alerta debido al gran desconocimiento que tenemos sobre por qué surge esa enferdad, cómo evoluciona, cómo evitarla...

Al no conocer bien las diferentes causas de esta enfermedad no sabemos cuándo empieza hasta que no tenemos síntomas a nivel celular de que nuestras células están cambiando/mutando, de ser células beningnas a malignas.

Por experiencia personal este es un tema delicado ya que ha afectado a mi familia directamente, para ser más concisos, algunas mujeres de mi familia padecieron cáncer de mama. Este tipo de cáncer sobre todo en las mujeres es el segundo tipo de cáncer que más ocurre entre ellas, solo en España según la AECC (la Asociación Española Contra el Cáncer), en 2018 se estimó que 130000 mujeres fueron diagnósticadas con dicha enfermedad.

A día de hoy seguimos teniendo miedo del cáncer debido a que no tenemos un cura como tal, sino que la única forma de superarlo es diagnosticarlo pronto, y es aquí donde entra en juego la minería de datos, ya que gracias a diferentes algoritmos podemos identificar si un persona padece cáncer o no antes de que sea demasiado tarde.

Por lo tanto, una vez que tenemos el contexto del problema vamos a definir cada una de las fases del proyecto de minería de datos:

### Definición del proyecto

Lo primero que debemos de hacer es definir los objetivos de nuestro proyecto, es decir, que es lo que buscamos dar respuesta. Esta fase es fundamental ya que de no definir los objetivos de forma correcta podría darse el caso de no llegar a las correctas conclusiones.

Por lo tanto, en nuestro caso el cáncer de mama va a ser el caso de estudio, con el fin de saber si una persona padece cáncer o no a partir de las características de las células, gracias a esto podremos diagnósticar temprenamente la fase en la que se encuentra el cáncer y si es cáncer o no.

Anque a priori es un problema de clasificación, es decir, buscamos saber si una persona padece cáncer de mama o no, y vamos a tener constancia para cada persona si padece cáncer o no, podríamos aplicar otros algoritmos que no fueran de aprendizaje supervisado, como el clustering, sería interesante el ver cómo un algoritmo agrupa los elementos más parecidos sin el conocimiento necesario para clasificar si es beningno o maligno el cáncer de mama, es decir, buscar que un algoritmo nos determine qué es lo que caracteriza al cáncer sin entrar a saber si una persona tiene cáncer o no.

En resumen, el problema al que hacemos frente es el saber cómo es el cáncer de mama, es decir, que le caracteriza y por otro lado, diagnosticar si una persona padece cáncer de mama o no los más pronto posible para aumentar las probabilidades de vida.

Como vemos los objetivos son claros, lo que necesitamos ahora es obtener los datos.

### Origen de los datos

Tal y como vimos en la etapa anterior lo primero que debemos hacer es definir los objetivos, y una vez que están claros hay que buscar los datos.

Los datos son necesarios para poder aplicar cualquier algoritmo de minería de datos, por lo que al no tener datos sobre el cáncer de mama a mano, hemos obtado por buscar en internet datasets que contuvieran la información necesaria para poder alcanzar los objetivos.

Por lo que, Machine Learning Respositry nos ofrece el dataset que estamos buscando sobre el cáncer de mama [https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Original%29]

### Preparación de los datos

Una vez que ya tenemos los datos necesitmos construir el modelo, pero para que éste funcione correctamente debemos pasarle los mismos con la mayor calidad posible, esto se debe a que nos podemos encotrar problemas como: hay observaciones sin algún valor, observaciones duplicadas...

Por lo tanto, el objetivo de esta fase es hacer la limpieza de todo el dataset para que los resultados obtenidos sean los de mayor calidad posible.

Dentro de esta fase, siempre y cuando tengamos determinados problemas, debemos hacer: la limpieza de datos innesarios o redundantes, transformar algunos datos, reducidir la dimensionalidad del dataset.

### Construcción del modelo

Cuando ya tenemos todos los datos limpios debemos construir el modelo, como bien hemos mencionado, este problema junto con su dataset se identifica mejor como un problema de clasificación, por lo que hacer uso de los árboles de decisión nos va a permitir ver según que parámetros y bajo qué valores una persona puede padecer cáncer o no.

Aunque inicialmente sea un problema más acorde con la clasificación, esto no significa que no se puedan usar otros algoritmos, ya que por ejemplo podemos hacer uso del clustering, para saber cómo se agrupan las células, aunque sabemos que hay solo dos tipos (benignas y malignas), al hacer uso del clustering dejamos a manos de un algoritmo que agrupe sin nostros indicarle nada, gracias a esto podemos saber qué caracteriza a un paciente con cáncer de mama respecto a un paciente que no padece de dicha enfermedad.

Además, podríamos hacer uso de algoritmos de reglas de asociación para ver cómo están relacioandas cada una de las variables, es decir, si el el tamaño de una célula implica que aumenta la forma y el grosor...

En resumen, podemos hacer uso de árboles de decisión y algoritmos de clustering para alcanzar los objetivos de nuestro proyecto que son dos: saber si una persona padece cáncer de mama o no, y saber qué le caracteriza al cáncer, es decir, cómo es el cáncer de mama.

### Evaluación e interpretación del modelo

Una vez que ya hemos construido los diferentes modelos, tenemos que evaluar cómo de buenos son, es decir, si realmente nos sirven para alcanzar los objetivos y responder a las preguntas que nos hemos hecho, en caso de que no, tendremos que buscar otros modelos que nos den mejores resultados.

Para evaluar el modelo de clasificación, podemos dividir el dataset original en dos, uno para el entrenamiento y otro de testing para validar los resultados obtenidos y calcular el error, de tal forma que a menor error mejor es el modelo. Respecto al uso de árboles de decisión podemos asignar un peso/porcentaje a cada posible resultado de tal forma que máximice la identificación de pacientes con cáncer de mama y a los que no tienen cáncer, es decir, será mejor el modelo cuanto menos falsos positivos y falsos negativos haya.

Para evaluar el modelo de clustering, podemos calcular la distancia que hay entre los puntos de un clúster al centro de su propio clúster, por lo que a menor distancia más compacto/mejor definido está el clúster, pero también podemos calcular la distancia entre los centros de todos los clústers, por lo que a mayor distancia más separación entre clústers y características más diferenciadas hay.

### Integración de los resultados

Una vez que hemos evaluado la calidad del modelo y si éste da respuestas a las preguntas que nos hacemos, tenemos que hacer uso de esta información para mejorar la situación actual sobre el cáncer de mama, ya sea informando a los médicos, a los pacientes directamente...

Cabe destacar que esto es un proceso iterativo, es decir, que podemos llegar a esta fase y volver a empazar de cero porque nos hacemos nuevas preguntas, o porque hay que seguir mejorando el modelo.

## Selección del dataset

Tal y como hemos mencionado en el apartado anterior, el dataset elegido para dar respuestas a las preguntas que nos hemos hecho es el de Breast Cancer Wisconsin (Original) Data Set de Machine Learning Repository [https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Original%29]

Hemos elegido dicho dataset porque nos proporciona toda la información necesaria para dar respuesta a las preguntas que nos hacemos, a partir de datos como el tamaño de la célula, la forma de la célula...

Es verdad que hay datasets similares a este pero con más atributos, es decir, aquí solo tenemos un campo para el tamaño de la célula, mientras que en otros datasets ese campo podía venir representado por tres atributos, pero este dataset consigue mantener la misma información reduciendo ya la dimensionalidad de los atributos. Por otro lado, los valores numéricos en este conjunto de datos van del 1 al 10, es decir, [1-10], estos valores no son aleatorios sino que un grupo de expertos ha "normalizado" dichos valores para representar la misma información pero de una forma más sencilla.

Finalmente, a continuación se indica los atributos que tiene este dataset:

**Id**
    integer - identificador de la observación.
    
**Thickness**
    integer [1-10] - espesor/grosor del tumor.
    
**Cell_Size**
    integer [1-10] - uniformidad del tamaño celular.
    
**Cell_Shape**
    integer [1-10] - uniformidad de la forma celular.
    
**Adhesion**
    integer [1-10] - adherencia celular.
    
**Epithelial_Cell_Size**
    integer [1-10] - tamaño de una sola célula epitelial.
    
**Bare_Nuclei**
    integer [1-10] - núcleo desnudo.
    
**Bland_Chromatin**
    integer [1-10] - textura del núcleo.
    
**Normal_Nucleoli**
    integer [1-10] - nucléolos normales.
    
**Mitoses**
    integer [1-10] - mitosis.
    
**Class**
    integer - clase si es beningno 2, si es maligno 4.
    
Aunque algun concepto es fácil de entender los demás no, es por ello que vamos a definir cada concepto:

- Thickness: hace referencia al espesor/grosor de la masa mamaria que se estudia, es decir, del tumor.

- Cell_Size: tamaño de las células que se estudian, las células cancerígenas suelen cambiar de tamaño.

- Cell_Shape: forma de las células que se estudian, las células cancerígenas suelen cambiar de forma.

- Adhesion: las células que son cancerígenas tienden a separarse unas de las otras, mientras que en las células normales sucede todo lo contrario.

- Epithelial_Cell_Size: las células que son significativamente grandes suelen ser cancerígenas.

- Bare_Nuclei: nos indica si el núcleo de las células está rodeado por el citoplasma (células normales), en caso contrario es probable que sean células cancerígenas.

- Bland_Chromatin: textura de los núcleos de las células, cuanto más tosco sea más probabiliades de que la célula sea cancerígena.

- Normal_Nucleoli: los nucleólos son pequeñas estructuras que se encuentran en los núcleos, en células benignas suelen ser estructuras pequeñas mientras que en las malignas son más grandes.

- Mitoses: proceso en el cual las células eucariotas se dividen.

Toda esta información se ha obtenido de la siguiente página [https://core.ac.uk/download/pdf/77274625.pdf], cabe destacar que a mayor valor de cada atributo más probable es que la célula sea anormal y dentro de esta anormalidad sea cancerígena.

Por último, tenemos 699 observaciones y 11 atributos.


## Análisis exploratorio

Antes de analizar cómo son las distribuciones, correlaciones, vamos a obtener unas estadísticas básicas para saber qué datos estamos manejando, para ello lo primero que debemos hacer es cargar el dataset:
```{r}
# Cargamos el juego de datos
df = read.table(file="./data/breast-cancer-wisconsin.data", fileEncoding="UTF-8", sep=",", na.strings = "NA")

# Cambiamos el nombre de las columnas tal y como se llaman según la página web
colnames(df) = c("Id", "Thickness", "Cell_Size", "Cell_Shape", "Adhesion", "Epithelial_Cell_Size", "Bare_Nuclei", "Bland_Chromatin", "Normal_Nucleoli", "Mitoses", "Class")

# Creamos una copia del dataframe, ya que va a ser la copia sobre la que vamos a realizar las modificaciones
df_copy = df

# Visualizamos los datos
str(df_copy)
```

### Estadísticas iniciales

Como podemos observar de la anterior ejecución, todos los campos los atribuye bien menos Bare_Nuclei, esto se debe a que en dicho campo tenemos tanto valores númericos como un interrogante "?" para los valores desconocidos, este problema se solventará en el siguiente punto cuando hagamos la limpieza del dataset, mientras tanto vamos a establecer que donde haya un interrogante lo añada como NA, para así poder hacer los cálculos y analizar el dataset de forma completa:
```{r}
df_copy[df_copy$Bare_Nuclei == "?", "Bare_Nuclei"] = NA
df_copy$Bare_Nuclei = as.integer(df_copy$Bare_Nuclei)

str(df_copy)
```
Una vez solventado ese problema, podríamos considerar que la variable Class es categórica, ya que solo representa un 2 si es benigno el cáncer o un 4 en caso contrario, pero como es indiferente hacer ese cambio mantenemos que siga siendo de tipo integer. 

Finalmente, sacamos las estadísticas iniciales:
```{r}
summary(df_copy)
```

De la anterior ejecución podemos ver que a excepción de la variable Id y Class, el resto el valor mínimo es 1 y el máximo es 10.

Respecto a Thickness (grosor) vemos que están bastante equilibrados los datos, es decir, la media y la mediana casi coinciden, y ambas están cerca del valor 4.5 que es la mitad a nivel numérico del rango [1,10], esto nos da a entender que está bien repartidos los datos, ya que además el primer cuartil tiene un valor de 2 que es practicamente la mitad de la media y el tercer cuartíl un valor de 6 que es prácticamente la mitad posterior a la media.

Respecto a Cell_Size, Cell_Shape y Epithelial_Cell_size, vemos que lo datos ya no están tan equilibrados, hay bastante diferencia entre la media y la mediana, lo cual a la vista podemos ver que es más común tener tamaños y formas pequeñas que grandes.

Con el atributo Adhesion pasa algo parecido, aunque no sabemos cómo se mide, a priori según los datos a menor valor de Adhesion menos probable es que tengas cáncer, es decir, más juntas están las células, por lo que vemos que la media y la mediana son bajas, esto nos da a entender que la gran mayoría de datos que tenemos las células esán muy juntas, es decir, la mayoría de los casos son beningnos.

Por otro lado, respecto a los atributos Bare_Nuclei, Bland_Chromain, Normal_Nucleoli y Mitoses no sabemos cómo están definidos los datos, es decir, en la documentación del dataset no se explica si a menor número menos probabilidades de cáncer, pero analizando los datos de forma manual vemos que sigue esta tendencia, es decir, cuanto más pequeño sean dichos valores mejor. Por lo tanto, observamos que la media y la mediana son pequeñas, algo muy significativo ya que de tener valores altos indicaría un mayor riesgo de cáncer.

En resumen, de esta primer análisis podemos ver que la mayoría de las observaciones que tenemos representan casos benignos, aunque es verdad que también hay casos malignos.

### Distribuciones

Teniendo claro esto, vamos a ver cómo son las distribuciones de las variables representativas, es decir, todas menos los atributos Id y Class.

La distribución de Thickness:
```{r}
hist(df_copy$Thickness, 
     main = "Histograma del atributo Thickness",
     ylab = "Frecuencia",
     xlab = "Thickness",
     col = "steelblue")
```

Tal y como podemos ver no sigue una distribución normal ya que la gran mayoría de los casos se encuentran entre una thickness de 1 a 5.

A menor grosor o menor Thickness más probable es que el cáncer sea benigno, por lo que si la gran mayoría tiene un grosor pequeño significa que la mayoría de los casos no suponen un cáncer.

Cabe destacar que el último grupo, es decir, aquel con una Thickness mayor que 9 tenemos alrededor de 75 casos, lo cuál es más probable que o tengas un grosor pequeño o que tu grosor sea demasiado grande.

La distribución de Cell_Size:
```{r}
hist(df_copy$Cell_Size, 
     main = "Histograma del atributo Cell_Size",
     ylab = "Frecuencia",
     xlab = "Cell_Size",
     col = "steelblue")
```

Al igual que sucedía antes, no sigue una distribución normal, ya que la gran mayoría de los casos tenemos un tamaño de célula pequeño. Esto es positivo ya que a menor tamaño de célula menos probable es que tengamos cáncer de mama, pero este tamaño de célula o es pequeño o es demasiado grande no hay un termino medio.

La distribución de Cell_Shape:
```{r}
hist(df_copy$Cell_Shape, 
     main = "Histograma del atributo Cell_Shape",
     ylab = "Frecuencia",
     xlab = "Cell_Shape",
     col = "steelblue")
```

Vemos que la tónica de los atributos es la misma, no siguen una distribución normal y la gran mayoría de los casos se encuentran con una forma de célula pequeña, sin embargo, hay bastantes casos en los que la forma de la célula es excesivamente grande.

Vemos que tanto el tamñao de la célula como la forma de la misma, están altamente correlacionados positivamente, ya que al aumentar uno aumenta el otro y viceversa, aún así este punto se verá de forma más clara en el siguietne apartado de correlaciones.

La distribución de Adhesion:
```{r}
hist(df_copy$Adhesion, 
     main = "Histograma del atributo Adhesion",
     ylab = "Frecuencia",
     xlab = "Adhesion",
     col = "steelblue")
```

Siguiendo con la tónica de las distribuciones, no es una distribución normal, y por norma general lo más común es que las células estén muy cercanas las unas de las otras, aún así seguimos viendo que hay una correlación muy fuerte respecto a si el cancer es benigno o maligno.

La distribución de Epithelial_Cell_Size:
```{r}
hist(df_copy$Epithelial_Cell_Size, 
     main = "Histograma del atributo Epithelial_Cell_Size",
     ylab = "Frecuencia",
     xlab = "Epithelial_Cell_Size",
     col = "steelblue")
```

Al igual que sucedía en los casos anteriores, no sigue una distribución normal y es llamativo que en respecto al tamaño de las células epiteliales suelen ser mayoritariamente más pequeñas, es decir, no hay tantos casos en los que la célula epitelial sea grande.

La distribución de Bare_Nuclei:
```{r}
hist(df_copy$Bare_Nuclei, 
     main = "Histograma del atributo Bare_Nuclei",
     ylab = "Frecuencia",
     xlab = "Bare_Nuclei",
     col = "steelblue")
```

Este atributo tampoco sigue una distribución normal y vemos que estos datos están muy polarizados, es decir, o tienes un valor muy pequeño o tienes un valor muy grande, esto puede hacernos a la idea que respecto al Bare_Nuclei sí que determina si un cáncer es benigno o maligno.

La distribución de Bland_Chromatin:
```{r}
hist(df_copy$Bland_Chromatin, 
     main = "Histograma del atributo Bland_Chromatin",
     ylab = "Frecuencia",
     xlab = "Bland_Chromatin",
     col = "steelblue")
```

Al igual que sucedía antes, continua sin seguir una distribución normal, pero en este caso es más probable que estos valores sean pequeños, ya que apenas hay casos en los que sea excesivamente grande, esto nos hace una idea que no es tan fácil determinar si el cáncer es benigno o maligno a partir de este atributo.

La distribución de Normal_Nucleoli:
```{r}
hist(df_copy$Normal_Nucleoli, 
     main = "Histograma del atributo Normal_Nucleoli",
     ylab = "Frecuencia",
     xlab = "Normal_Nucleoli",
     col = "steelblue")
```

Seguimos sin ver una distribución normal y vemos que los datos están polarizados, es decir, aunque mayoritariamente los casos son que el tamaño de los nucleólos son pequeños (esto significa que el cáncer es benigno), hay casos significativos en los que los nucleólos son excesivamente grandes (cáncer de mama maligno).

La distribución de Mitoses:
```{r}
hist(df_copy$Mitoses, 
     main = "Histograma del atributo Mitoses",
     ylab = "Frecuencia",
     xlab = "Mitoses",
     col = "steelblue")
```

Continuamos sin ver una distribución normal, y vemos que prácticamente todos los casos presentan un valor de Mitoses muy pequeño, lo cual nos hace indicar que probablemente este atributo no sea significativo para determinar si un cáncer es benigno o maligno.

#### Resumen

Tras analizar la distribución de cada variable, podemos ver que ninguna sigue una distribución normal.

Por otro lado, hay determinadas características como como el grosor, tamaño y forma de la célula las cuales a priori paracen que son determinantes para saber si un cáncer es benigno o maligno, también hay otras como el tamaño de la estructura de los nucleólos junto con si el núcleo está rodeado de citoplasma, que nos permite saber si el cáncer es benigno o maligno.

Sin embargo, hay características como la mitosis, o la textura del núcleo que no son tan determinantes a la hora de decidir si el cáncer de mama es benigno o maligno.

### Correlaciones

Una vez que hemos visto las distribuciones de cada variable, vamos a ver cómo de correlacionadas están entre ellas.

Para ello, lo primero que vamos a hacer es crear un dataset con las variables que nos interesan (todas, menos el Id y la Class) y vamos a eliminar aquellas filas cuyo Bare_Nuclei sea NA, tan solo son 16 filas, por lo que no perdemos mucha información, y aunque estas filas se usarán depués para hacer la limpieza, en un primer análisis del dataset no nos hacen falta:
```{r}
df_copy_noNA = df_copy[is.na(df_copy$Bare_Nuclei) == FALSE, c(2:10)]
cor(df_copy_noNA)
```

Podemos obtener también el siguiente gráfico, para poder visualizar las correlaciones de una mejor forma:
```{r}
library(corrplot)

corrplot(cor(df_copy_noNA), type = "upper", 
         order = "original", 
         tl.col = "black", 
         tl.srt = 45)
```

La forma de interpretar los resultados obtenidos de las dos anteriores ejecuciones es la siguiente, el valor de correlación es un número entre -1 y 1, si el valor es -1 indica que las variables están altamente correlacionadas pero negativamente, es decir, cuando una variable aumenta la otra disminuye, por otro lado, cuando el valor es 1 las variables están correlacionadas positivamente, esto significa que cuando una aumenta la otra también aumenta.

Entendiendo lo que es la correlación, vemos que tanto a nivel gráfico como a nivel númerico todas las variables están correlacionadas entre sí positivamente, con la excepción del atributo Mitoses, ya que aunque están correlacionadas positivamente con dicho atributo, esta correlación no es fuerte, es decir, todas las variables cuando aumenta una aumenta la otra en mayor o menor medida.

Por lo tanto, vamos a analizar las correlaciones más llamativas, es decir, aquellas en las que hay una correlación bastante elevada entre variables.

La correlaciones más altas las conseguimos a partir de la variable Size y Shape, es decir, del tamaño y la forma, por lo que vamos a analizar las correlaciones más significativas de estas dos variables.

La correlación de Cell_Size respecto a Cell_Shape es la siguiente:
```{r}
library(PerformanceAnalytics)

chart.Correlation(data.frame(df_copy_noNA$Cell_Size, df_copy_noNA$Cell_Shape))
```

Tal y como podemos apreciar, esta correlación es muy fuerte, es decir a medida que aumentamos el tamaño de la célula aumenta también la forma. Recordando que cuando aumenta el tamaño de la célula o de la forma o ambas significa que es más probable que el cáncer sea maligno, por lo tanto si aumenta una de las dos variables es muy probable que tengamos cáncer.

Otra correlación fuerte es la de Cell_Size respecto a Epithelial_Cell_Size es la siguiente:
```{r}
chart.Correlation(data.frame(df_copy_noNA$Cell_Size, df_copy_noNA$Epithelial_Cell_Size))
```

Lógicamente al aumentar el tamaño de las células va a aumentar también el tamaño de las células epiteliales, ya que siguen siendo células que se encuentran en la piel. Por lo tanto, si aumenta el tamaño de cualquiera de estas dos variabels el riesgo de que padezcamos cáncer de mama es elevado.

Otra correlación significativa es la de Cell_Size respecto a Epithelial_Cell_Size es la siguiente:
```{r}
chart.Correlation(data.frame(df_copy_noNA$Cell_Size, df_copy_noNA$Bland_Chromatin))
```

Bland_Chromatin hace referencia a la textura del núcleo de la célula, cuando ésta es maligna su núcleo tiende a ser más burdo, por lo que lógicamente si antes hemos dicho que el tamaño de la célula implica mayor riesgo de cáncer, al aumentar el tamaño la textura se ve afectada.

Por otro lado, una de las correlaciones más significativas respecto a la forma (Cell_Shape) es junto a el tamaño de la célula (Cell_Size) tal y como hemos visto en este punto, como esta gráfica ya ha sido representada vamos a mostrar la siguiente correlación más significativa, la cual es Cell_Shape respecto a Epithelial_Cell_Size:
```{r}
chart.Correlation(data.frame(df_copy_noNA$Cell_Shape, df_copy_noNA$Epithelial_Cell_Size))
```

Tal y como podemos apreciar al aumentar la forma de la célula aumenta también el tamaño, lo que tiene la mayor lógica posible, es decir, si la forma de un objeto aumenta va a aumentar su tamaño en mayor o menor medida. Recapitulando, a mayor forma o mayor tamaño de células epitetales implica un mayor riesgo de cáncer de mama.

Finalmente, la otra correlación más significativa de este dataset es la que forman Cell_Shape y Bland_Chromatin:
```{r}
chart.Correlation(data.frame(df_copy_noNA$Cell_Shape, df_copy_noNA$Bland_Chromatin))
```

Al igual que sucecía con el tamaño de la célula, al aumentar la forma mayor riesgo de cáncer y eso se traduce a que la textura del núcleo de la célula es más burda, por lo tanto, si la textura es más burda es más probable que haya aumentado la forma de la célula.

#### Resumen

Tal y como hemos podido ver en este apartado de las correlaciones, todas las variables están correlacionadas positivamente en mayor o menor medida, y hemos visto que la correlación más fuerte se producía entre el tamaño de la célula y su forma, ya que a mayor tamaño aumentaba también la forma. Con esto podemos identificar que la gran mayoría de atributos van de la mano, es decir, cuando aumenta el uno aumenta el otro, y por tanto en nuestro caso, si aumenta un valor esto supone un mayor riesgo de padecer cáncer de mama.

### Valores anormales/outliers

En este apartado vamos a analizar si hay outliers, o valores anormales dentro de cada distribución de variables.

Por lo tanto, en el siguiente boxplot podemos ver los outliers dentro de la variable Thickness:
```{r}
boxplot(df_copy_noNA$Thickness,
        main = "Boxplot de la variable Thickness",
        ylab = "Thickness")
```

Como era de apreciar, el valor mínimo está acotado a 1 y el máximo a 10 y vemos que el 75% de los datos de esta variable se mueven entre 1 y 6, por lo que de media el grosor no es muy elevado. Cabe destacar que no hay outliers ni por encima ni por debajo.

El siguiente boxplot muestra información sobre la variable Cell_Size:
```{r}
boxplot(df_copy_noNA$Cell_Size,
        main = "Boxplot de la variable Cell_Size",
        ylab = "Cell_Size")
```

En el anterior gráfico observamos que tampoco hay valores anormales, y que el 75% de los datos se mueven entre el 1 y el 5. Sin embargo, lo más destacado es que el valor mínimo coincide que el primer cuartil y con la mediana, esto lo que nos quiere indicar es que tenemos muchas observaciones cuya tamaño de célula es 1.

El siguiente boxplot muestra información sobre la variable Cell_Shape:
```{r}
boxplot(df_copy_noNA$Cell_Shape,
        main = "Boxplot de la variable Cell_Shape",
        ylab = "Cell_Shape")
```

Al igual que sucedía en los casos anteriores no tenemos valores outliers, pero el primer cuartil, la mediana y el mínimo coinciden, esto nos da a entender que la gran mayoría de valores respecto a la forma es igual a 1.

El siguiente boxplot muestra información sobre la variable Adhesion:
```{r}
boxplot(df_copy_noNA$Adhesion,
        main = "Boxplot de la variable Adhesion",
        ylab = "Adhesion")
```

Respecto a este atributo vemos que sí que tenemos valores outliers, para ser más exactos dos, es decir, que hay dos observaciones en las cuales el valor para medir cómo de separadas están esas células vemos que es elevado respecto a lo normal, sin embargo, eso no significa que sean valores outliers ya que el dataset está acotado a valores ente 1 y 10, por lo que estos valores son totalmente válidos.

Al igual que sucedía en los casos anteriores el primer cuartil, junto con la mediana y el mínimo coinciden, por lo que nos da a enteder que valor más común para este atributo es 1.

El siguiente boxplot muestra información sobre la variable Epithelial_Cell_Size:
```{r}
boxplot(df_copy_noNA$Epithelial_Cell_Size,
        main = "Boxplot de la variable Epithelial_Cell_Size",
        ylab = "Epithelial_Cell_Size")
```

En este caso tenemos también tres observaciones con outliers, es decir, tres observaciones cuyo valores se alejan de lo normal, pero al igual que sucedía antes esto no significa que sean valores anormales en sí ya que están acotados a 10.

Por otro lado el valor máximo en este atributo ha disminuido respecto a los otros, y por tanto el rango intercuartílico es menor, sin embargo, la mediana y el primer cuartil siguen coincidiendo, esto nos sigue indicando que el valor más común es bajo, en este caso 2.

El siguiente boxplot muestra información sobre la variable Bare_Nuclei:
```{r}
boxplot(df_copy_noNA$Bare_Nuclei,
        main = "Boxplot de la variable Bare_Nuclei",
        ylab = "Bare_Nuclei")
```

En este caso, no tenemos valores anormales y el rango intercuartílico es bastante grande, aunque el valor mínimo, el primer cuartil y la mediana coinciden, por lo que el valor más común sigue siendo 1.

El siguiente boxplot muestra información sobre la variable Bland_Chromatin:
```{r}
boxplot(df_copy_noNA$Bland_Chromatin,
        main = "Boxplot de la variable Bland_Chromatin",
        ylab = "Bland_Chromatin")
```

En este caso, tenemos una observación cuyo valor está alejado de lo normal, pero al estar acotado dichos valores entre 1 y 10 lo podemos considerar normal perfectamente.

Por otro lado, el rango intercuartílico no es muy grande, pero en este caso la mediana ya no coincide con el primer cuartil, esto nos permite saber que el valor más común está más alejado que antes, en este caso dicho valor es 3.

El siguiente boxplot muestra información sobre la variable Normal_Nucleoli:
```{r}
boxplot(df_copy_noNA$Normal_Nucleoli,
        main = "Boxplot de la variable Normal_Nucleoli",
        ylab = "Normal_Nucleoli")
```

En este caso, tenemos dos observaciones que tienen valores fuera de lo normal, pero al igual que en todas las variables anteriores los podemos considerar normales ya que están acotados.

Cabe destacar que en esta variable el valor mínimo, primer cuartil y mediana coinciden, lo cual nos indica que el valor más común es muy pequeño, en este caso 1.

El siguiente boxplot muestra información sobre la variable Mitores:
```{r}
boxplot(df_copy_noNA$Mitoses,
        main = "Boxplot de la variable Mitores",
        ylab = "Mitores")
```

A la vista está que este atributo es bastante especial, ya que apenas tiene rango intercuartílico, es decir, el 75% de los datos es el mismo, que en este caso es 1. Tenemos bastantes outliers, que los podríamos considerar como tal si no estuvieran acotados, es por ello que los consideramos normales.

#### Resumen

Aunque hay ciertas variables que tienen valores anormales, no los podemos considerar como tal ya que están acotados entre 1 y 10.

Por otro lado, hemos visto que por norma general la mediana de los atributos coinciden con el valor mínimo o con un valor muy próximo al mínimo, esto nos indica que de media los valores que suelen tomar cada variable son pequeños. De esto concluimos que la gran mayoría de los casos son benignos, ya que a menor valor de cada atributo menos riesgo de padecer cáncer de mama.

## Limpieza del dataset

Una vez que hemos realizado un primer análisis sobre el conjunto de los datos, vamos a proceder a la limpieza de los mismos para así poder hacer un uso posterior de ellos.

### Imputación de valores

Lo primero que vamos a comprobar es si hay valores nulos:
```{r}
colSums(is.na(df_copy))
```

Como vemos, todos los campos tienen valores excepto el atributo Bare_Nuclei, ya que éste tal y como viene en la documentación del dataset, hay determinadas observaciones que no tienen valor.

Por lo tanto, vamos a realizar una imputación de valores para solventar este problema con el fin de mejorar los resultados de nuestros modelos, para ello podríamos establecer que se tome el valor de la media o de la mediana, es decir, una medida de tendencia central. En nuestro caso hemos decidido que lo mejor es la mediana, ya que de introducir la media, tal y como vimos en el apartado anterior puede suponer que esté bastante alejada de la mediana y esto nos dé información sesgada respecto al valor "más común".

Es por ello, que calculamos la mediana de dicho campo, es decir, de Bare_Nuclei:
```{r}
nMediana = median(df_copy$Bare_Nuclei, na.rm = TRUE)

cat("El valor de la mediana es:",nMediana)
```

La mediana nos da un valor de uno, por lo tanto, vamos a imputar dicho valor en las obsevaciones que tienen como valor NA en el atributo Bare_Nuclei, lo primero que vamos a hacer es visualizar estos regitros sin imputar el valor:
```{r}
head(df_copy[is.na(df_copy$Bare_Nuclei),])
```

Ahora imputamos el valor de la mediana a todos estos registros y comprobamos que se ha realizado la imputación:
```{r}
df_copy$Bare_Nuclei[is.na(df_copy$Bare_Nuclei)] = nMediana
head(df_copy[is.na(df_copy$Bare_Nuclei),])
```

Como podemos comprobar ya no hay registros con valores NA, y cómo es lógico ha disminuido un poco la media y el tercer cuartil:
```{r}
summary(df_copy$Bare_Nuclei)
```

Comprobamos de nuevo que no hay observaciones sin valores:
```{r}
summary(df_copy)
```

### Normalización

Una vez que tenemos ya todos los datos completos, vamos a normalizar el dataset y generar un fichero, ya que en esta práctica al no saber qué datos van a funcionar mejor a cada modelo, vamos a generar un fichero con los datos numéricos normalizados, y otro fichero que va a ser como el fichero orignal pero con los datos completos y discretizados.

Por lo tanto, lo primero de todo es normalizar los datos, a priori no haría falta porque todos los datos están medidos en la misma escala, es decir, del 1 al 10, pero puede ser que si aplicamos la normalización funcione mejor en determinados modelos:
```{r}
library(scales)

df_normalizado = scale(df_copy)
head(df_normalizado)
```

Como podemos apreciar hemos normalizado también el atributo Id y Class, si en un futuro no hacen falta los podríamos ignorar.

Una vez normalizados los datos generamos un fichero con los mismos:
```{r}
write.csv(df_normalizado, file = "./data/breast-cancer-clean-normalized.csv", row.names = FALSE)
```

### Outliers

Otro aspecto a tener en cuenta en la limpieza del dataset son los outliers, es decir, esos valores que se elejan más de lo normal respecto a cada atributo. Tal y como vimos en el análisis de outliers, no podemos considerar que los valores extremos que tenemos sean realmente outliers, ya que los datos están acotados entre 1 y 10, por ello, no vamos a hacer nada al respecto, es decir, mantenemos la misma información.

### Discretización

Cuando hacemos la limpieza de los datos, debemos de tener en cuenta las discretizaciones. La discretización tiene como objetivo dividir los valores en diferentes grupos.

Al hacer uso de discretizaciones conseguimos mejorar la eficiencia de los modelos ya que van a tener que realizar menos comparaciones, cuando trabajamos con árboles de decisión, como va a ser nuestro caso, al hacer uso de variables numéricas se obtienen árboles con ramificaciones mucho más grandes que si estos valores estuvieran discretizados...

Por todo ello, realizamos la dicretización de todas los valores numéricos que están acotados entre 1 y 10, a demás de la clase, en la cual vamos a definir si es el cáncer es benigno o maligno (la terminología usada en la discretización se ha obtenido a partir de https://core.ac.uk/download/pdf/77274625.pdf):
```{r}
df_discretizado = df_copy

# Dicretización del grosor
df_discretizado["d-Thickness"] = ordered(cut(df_discretizado[["Thickness"]], breaks = c(0,4,7,10), labels = c("Delgado", "Mediano", "Grueso")))

# Dicretización del tamaño
df_discretizado["d-Cell_Size"] = ordered(cut(df_discretizado[["Cell_Size"]], breaks = c(0,4,7,10), labels = c("Pequeño", "Mediano", "Grande")))

# Dicretización del forma
df_discretizado["d-Cell_Shape"] = ordered(cut(df_discretizado[["Cell_Shape"]], breaks = c(0,4,7,10), labels = c("Clase1", "Clase2", "Clase3")))

# Dicretización a la adhesión marginal
df_discretizado["d-Adhesion"] = ordered(cut(df_discretizado[["Adhesion"]], breaks = c(0,5,10), labels = c("Débil", "Fuerte")))

# Dicretización del tamaño de las células epiteliales
df_discretizado["d-Epithelial_Cell_Size"] = ordered(cut(df_discretizado[["Epithelial_Cell_Size"]], breaks = c(0,4,7,10), labels = c("Pequeño", "Mediano", "Grande")))

# Dicretización los núcleos desnudos
df_discretizado["d-Bare_Nuclei"] = ordered(cut(df_discretizado[["Bare_Nuclei"]], breaks = c(0,4,7,10), labels = c("Ausente", "Ausente/Presente", "Presente")))

# Dicretización de la textura de la núcleos
df_discretizado["d-Bland_Chromatin"] = ordered(cut(df_discretizado[["Bland_Chromatin"]], breaks = c(0,4,7,10), labels = c("Uniforme", "Uniforme/Burda", "Burda")))

# Dicretización de la estructoras de los nucleólis
df_discretizado["d-Normal_Nucleoli"] = ordered(cut(df_discretizado[["Normal_Nucleoli"]], breaks = c(0,4,7,10), labels = c("Pequeño", "Mediano", "Grande")))

# Dicretización de la mitosis
df_discretizado["d-Mitoses"] = ordered(cut(df_discretizado[["Mitoses"]], breaks = c(0,4,7,10), labels = c("Mitosis1", "Mitosis2", "Mitosis3")))

# Creamos una columna respecto a la clase, aunque no es discretización como tal, es más fácil ver si un cáncer es benigno respecto a su nombre que no respecto a su valor.
df_discretizado$`d-Class`[df_discretizado$Class == 2] = "Benigno" 
df_discretizado$`d-Class`[df_discretizado$Class == 4] = "Maligno"
df_discretizado$`d-Class` = as.factor(df_discretizado$`d-Class`)

head(df_discretizado)
```

Una vez que hemos dicretizado los valores y hemos comprobado que está todo de forma correcta, generamos otro fichero para saber si los modelos fucionan mejor con los valores normalizados o con los valores discretizados:
```{r}
write.csv(df_discretizado, file = "./data/breast-cancer-clean-discretized.csv", row.names = FALSE)
```

Cabe destacar que generamos el fichero con todos los campos, si después no los necesitamos no haremos uso de ellos.

Con esto hemos finalizado el apartado de limpieza de datos, y en el siguiente vamos a realizar el análisis de componentes principales.

## Análisis de componentes principales (PCA)

Otro aspecto a tener en cuenta es el análisis de componentes principales, éste nos permite saber qué componentes representan la máxima variabilidad medida a partir de la varianza.

El análisis de componentes principales lo debemos hacer sobre las variable numéricas, por lo que vamos a hacer uso del dataframe correspondiente:
```{r}
df_PCA = df_copy[, c(2:10)]
```

Luego calculamos la matriz de coorrelaciones o de covarianzas, al estar todos los atributos normalizados del 1 al 10 y no sabemos la magnitud de cada atributo, es mejor hacer uso de la matriz de covarianzas, ya que la de correlaciones la usamos cuando tenemos diferentes magnitudes y estas variables toman valores muy dispares:
```{r}
cov(df_PCA)
```

Una vez que tenemos la matriz de covarianzas, podemos hacer el análisis de componentes principales:
```{r}
ACP.cov = prcomp(x = df_PCA, center = TRUE, scale. = FALSE)
summary(ACP.cov)
```

Con los resultados obtenidos vemos que la proporción de varianza es diferente para cada uno de los componentes.

Por lo tanto, si seleccionamos los cuatro primeros componentes podemos proyectar más o menos la misma información que en el dataset original, pero reduciendo la dimensionalidad. Con las cuatro primeras dimensiones conseguimos representar la mayor parte de la varianza total con un 86.72%, y si elegimos la cinco primeras dimensiones conseguimos representar el 90.60% de la variabilidad.

Pero a ojo no sabemos qué es lo mejor, si elegir 4 componentes, 5... Es por ello que existen modelos matemáticos que nos permiten saber qué elección de componentes es la mejor, para ello podemos hacer uso de la regla de Kaiser-Guttman:
```{r}
# Realizamos el cálculo de la regla de Kaiser-Guttman
valores_propios = ACP.cov$sdev^2
media_valores_propios = sum(valores_propios) / length(valores_propios)
cat("Los valores propios son: ", valores_propios)
```

```{r}
cat("La media de los valores propios es: ", media_valores_propios)
```

Según Kaiser-Guttman hay que elegir como componentes principales aquellos que cuyos valores propios superen la media de los mismos, en este caso solo el primer componente supera la media, por lo que eligiendo el primer componente conseguimos reducir la dimensionalidad y proyectar más o menos la misma información que en el dataset original.

## Análisis visual



******
# Rúbrica
******
* 30% Se plantea un problema propio de minería de datos, se detallan los objetivos analíticos y se explica detalladamente el procedimiento para darles solución.
* 10%. Justificación de la elección del juego de datos donde se detalle el potencial analítico que se intuye. El estudiante deberá visitar los siguientes portales de datos abiertos para seleccionar su juego de datos:
  + [Datos.gob.es](https://datos.gob.es/es/catalogo?q=&frequency=%7B"type"%3A+"months"%2C+"value"%3A+"1"%7D&sort=score+desc%2C+metadata_modified+desc)
  + [UCI Machine Learning](https://archive.ics.uci.edu/ml/datasets.php)
  + [Datasets Wikipedia](https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research)
  + [Datos abiertos Madrid](https://datos.madrid.es/portal/site/egob/)
  + [Datos abiertos Barcelona](https://opendata-ajuntament.barcelona.cat/es/)
  + [London Datastore](https://data.london.gov.uk/)
  + [NYC OpenData](https://opendata.cityofnewyork.us/)
* 20%. Información extraída del análisis exploratorio. Distribuciones, correlaciones, anomalías,... 
* 20%. Explicación clara de cualquier tarea de limpieza o acondicionado que se realiza. Justificando el motivo y mencionando las ventajas de la acción tomada.
* 20%. Se realiza un proceso de PCA o SVD donde se aprecia mediante explicaciones y comentarios que el estudiante entiende todos los pasos y se Scomenta extensamente el resultado final obtenido.


******
# Recursos de programación
******
* Incluimos en este apartado una lista de recursos de programación para minería de datos donde podréis encontrar ejemplos, ideas e inspiración:
  + [Material adicional del libro: Minería de datos Modelos y Algoritmos](http://oer.uoc.edu/libroMD/)
  + [Espacio de recursos UOC para ciencia de datos](http://datascience.recursos.uoc.edu/es/)
  + [Buscador de código R](https://rseek.org/)  
  + [Colección de cheatsheets en R](https://rstudio.com/resources/cheatsheets/)  
  

