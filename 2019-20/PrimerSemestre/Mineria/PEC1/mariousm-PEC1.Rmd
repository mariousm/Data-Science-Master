---
title: 'Minería de datos: PEC1'
author: "Autor: Mario Ubierna San Mamés"
date: "marzo 2021"
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
    includes:
      in_header: 75.584-PEC-header.html
  pdf_document:
    highlight: zenburn
    toc: yes
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

******
# Introducción
******
## Presentación
Esta prueba de evaluación continuada cubre el módulo 1,2 y 8 del programa de la asignatura.  

## Competencias
Las competencias que se trabajan en esta prueba son:

* Uso y aplicación de las TIC en el ámbito académico y profesional
* Capacidad para innovar y generar nuevas ideas.
* Capacidad para evaluar soluciones tecnológicas y elaborar propuestas de proyectos teniendo en cuenta los recursos, las alternativas disponibles y las condiciones de mercado.
* Conocer las tecnologías de comunicaciones actuales y emergentes, así como saberlas aplicar convenientemente para diseñar y desarrollar soluciones basadas en sistemas y tecnologías de la información.
* Aplicación de las técnicas específicas de ingeniería del software en las diferentes etapas del ciclo de vida de un proyecto.
* Capacidad para aplicar las técnicas específicas de tratamiento, almacenamiento y administración de datos.
* Capacidad para proponer y evaluar diferentes alternativas tecnológicas para resolver un problema concreto.
* Capacidad de utilizar un lenguaje de programación.  
* Capacidad para desarrollar en una herramienta IDE.  
* Capacidad de plantear un proyecto de minería de datos.  

## Objetivos
* Asimilar correctamente el módulo 1 y 2.
*	Qué es y qué no es MD.
*	Ciclo de vida de los proyectos de MD.
*	Diferentes tipologías de MD.
* Conocer las técnicas propias de una fase de preparación de datos y objetivos a alcanzar.  

## Descripción de la PEC a realizar
La prueba está estructurada en 1 ejercicio teórico/práctico y 1 ejercicio práctico que pide que se desarrolle la fase de preparación en un juego de datos.  
Deben responderse todos los ejercicios para poder superar la PEC.  

## Recursos

Para realizar esta práctica recomendamos como punto de partida la lectura de los siguientes documentos:

* Módulo 1, 2 y 8 del material didáctico.
* Ciclo de vida de un proyecto de minería de datos: https://es.wikipedia.org/wiki/cross_industry_standard_process_for_data_mining#Fases_principales
* A lo aparta del enunciado de la actividad disponéis de unos materiales de ggplot2
* El aula laboratorio de R para resolver dudas o problemas.
* RStudio Cheat Sheet: Disponible en el aula Laboratorio de Minería de datos.
* R Base Cheat Sheet: Disponible en el aula Laboratorio de Minería de datos.
  

## Formato y fecha de entrega
El formato de entrega es: usernameestudiant-PAC1.html (pdf o word) y rmd.
Fecha de Entrega: 31/03/2021.
Se tiene que depositar la PEC en el buzón de entregas del aula.


## Nota: Propiedad intelectual 

A menudo es inevitable, al producir una obra multimedia, hacer uso de recursos creados por terceras personas. Es por lo tanto comprensible hacerlo en el marco de una práctica de los estudios de Informática, Multimedia y Telecomunicación de la UOC, siempre y cuando esto se documente claramente y no suponga plagio en la práctica. 

Por lo tanto, al presentar una práctica que haga uso de recursos ajenos, se debe presentar junto con ella un documento en qué se detallen todos ellos, especificando el nombre de cada recurso, su autor, el lugar dónde se obtuvo y su estatus legal: si la obra está protegida por el copyright o se acoge a alguna otra licencia de uso (Creative Commons, licencia GNU, GPL ...). 
El estudiante deberá asegurarse de que la licencia no impide específicamente su uso en el marco de la práctica. En caso de no encontrar la información correspondiente tendrá que asumir que la obra está protegida por copyright. 

Deberéis, además, adjuntar los ficheros originales cuando las obras utilizadas sean digitales, y su código fuente si corresponde.  


****
# Ejemplo de solución del ejercicio 2
****
****
## Objetivos
****
Como ejemplo, trabajaremos con el juego de datos "Titanic.csv" que recoge datos sobre el famoso crucero.

Las actividades que llevaremos a cabo en esta práctica se realizan en las fases iniciales de un proyecto de minería de datos. Tienen como objetivo obtener un dominio de los datos con las que construiremos el modelo de minería. Tenemos que conocer los datos profundamente tanto en su formato como contenido. Tareas típicas en esta fase pueden ser la selección de características o variables, la preparación del juego de datos para posteriormente ser consumido por un algoritmo e intentar extraer el máximo conocimiento posible de los datos.

Desarrollaremos un subconjunto de tareas mínimas y de ejemplo. Podemos incluir muchas más y mucho más profundas, como hemos visto al material docente.

## Procesos iniciales con los datos

Primero contacto con el juego de datos

Instalamos y cargamos las librerías ggplot2 y dplry 

```{r echo=TRUE, message=FALSE, warning=FALSE}
# https://cran.r-project.org/web/packages/ggplot2/index.html
if (!require('ggplot2')) install.packages('ggplot2'); library('ggplot2')
# https://cran.r-project.org/web/packages/dplyr/index.html
if (!require('dplyr')) install.packages('dplyr'); library('dplyr')
```

Cargamos el fichero de datos

```{r}
totalData <- read.csv('./data/titanic.csv',stringsAsFactors = FALSE)
filas=dim(totalData)[1]
```

Cargamos los datos filtrados por tripulación para hacer estudios posteriores

```{r}
totalData_crew=subset(totalData, totalData$class=="engineering crew")
```

Verificamos la estructura del juego de datos principal

```{r}
str(totalData)
```

Vemos que tenemos 2207 registros que se corresponden a los viajeros y tripulación del Titánic y 11 variables que los caracterizan.

Revisamos la descripción de las variables contenidas en el fichero y si los tipos de variable se corresponde al que hemos cargado:

**name**
    string with the name of the passenger.
    
**gender**
    factor with levels male and female.
    
**age**
    numeric value with the persons age on the day of the sinking. The age of babies (under 12 months) is given as a fraction of one year (1/month).
    
**class**
    factor specifying the class for passengers or the type of service aboard for crew members.
    
**embarked**
    factor with the persons place of of embarkment.
    
**country**
    factor with the persons home country.
    
**ticketno**
    numeric value specifying the persons ticket number (NA for crew members).
    
**fare**
    numeric value with the ticket price (NA for crew members, musicians and employees of the shipyard company).
    
**sibsp**
    ordered factor specifying the number if siblings/spouses aboard; adopted from Vanderbild data set.
    
**parch**
    an ordered factor specifying the number of parents/children aboard; adopted from Vanderbild data set.
    
**survived**
    a factor with two levels (no and yes) specifying whether the person has survived the sinking.

Vamos ahora a sacar estadísticas básicas y después trabajamos los atributos con valores vacíos.

```{r echo=TRUE, message=FALSE, warning=FALSE}
summary(totalData)
```

Estadísticas de valores vacíos

```{r}
colSums(is.na(totalData))
colSums(totalData=="")
```

Asignamos valor "Desconocido" para los valores vacíos de la variable "country"

```{r}
totalData$country[is.na(totalData$country)] <- "Desconocido"
```

Asignamos la media para valores vacíos de la variable "age"

```{r}
totalData$age[is.na(totalData$age)] <- mean(totalData$age,na.rm=T)
```

De la información mostrada destacamos que el pasajero más joven tenía 6 meses y el más grande 74 años. La media de edad la tenían en 30 años. También podemos ver 891 sin billete. Revisaremos si se corresponde a la tripulación. También podemos observar el que se pagó por el billete. En este caso se entienden las discrepancias en la fiabilidad de este dato. Parece que los pasajeros que embarcaron a Southampton hacían transbordo de un barco que tenía la tripulación en huelga y por eso no pagaron lo que explicaría la diferencia. Recordemos que la tripulación no pagaba. Sibsp y parch también muestran datos interesantes del viajero con quien más familiares viajaba eran 8 hermanos o mujer y 9 hijos o padre/madre.

Si observamos los NA (valores nulos) vemos que los datos están bastante bien. Decidimos sustituir el valor NA de country por Desconocido por una mayor legibilidad. También proponemos sustituir los NA de age por la media a pesar de que realmente no hace falta.

Es curios como los valores NA de sibsp y parch nos permite deducir que viajaban muchas familias. De hecho, a simple vista, restante la tripulación la gente que viajaba sola era mínima. Este dato la podríamos contrastar también. Sería interesante relacionar la mortalidad del accidente con el tamaño de las familias que viajaban.

Ahora añadiremos un campo nuevo a los datos. Este campo contendrá el valor de la edad discretizada con un método simple de intervalos de igual amplitud.

```{r echo=TRUE, message=FALSE, warning=FALSE}

summary(totalData[,"age"])
```

Discretizamos

```{r}
totalData["segmento_edad"] <- cut(totalData$age, breaks = c(0,10,20,30,40,50,60,70,100), labels = c("0-9", "10-19", "20-29", "30-39","40-49","50-59","60-69","70-79"))
```

Observamos los datos discretizados

```{r}
head(totalData)
```

Vemos como se agrupaban por edad

```{r}
plot(totalData$segmento_edad)
```

Ahora repetimos por el proceso, pero solo por el subconjunto de tripulación filtrado antes

```{r}
totalData_crew["segmento_edad"] <- cut(totalData_crew$age, breaks = c(0,10,20,30,40,50,60,70,100), labels = c("0-9", "10-19", "20-29", "30-39","40-49","50-59","60-69","70-79"))
plot(totalData_crew$segmento_edad)
```

De la discretización de la edad observamos que realmente la gente que viajaba era muy joven. El segmento más grande es de 20 a 29 años. También vemos de la juventud de la tripulación.

## Procesos de análisis visuales del juego de datos

Nos proponemos analizar las relaciones entre las diferentes variables del juego de datos para ver si se relacionan y como.

Visualizamos la relación entre las variables "gender" y "survived":


```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(data=totalData[1:filas,],aes(x=gender,fill=survived))+geom_bar()
```

Otro punto de vista. Survived como función de Embarked:

```{r}
ggplot(data = totalData[1:filas,],aes(x=embarked,fill=survived))+geom_bar(position="fill")+ylab("Frequència")
```

En la primera gráfica podemos observar fácilmente la cantidad de mujeres que viajaban respecto hombres y observar los que no sobrevivieron. Numéricamente el número de hombres y mujeres supervivientes es similar.

A la segunda gráfica de forma porcentual observamos los puertos de embarque y los porcentajes de supervivencia en función del puerto. Se podría trabajar el puerto C (Cherburgo) para ver de explicar la diferencia en los datos. Quizás porcentualmente embarcaron más mujeres o niños... ¿O gente de primera clase?

Obtenemos ahora una matriz de porcentajes de frecuencia.
Vemos, por ejemplo, que la probabilidad de sobrevivir si se embarcó en "C" es de un 56.45%

```{r echo=TRUE, message=FALSE, warning=FALSE}
t<-table(totalData[1:filas,]$embarked,totalData[1:filas,]$survived)
for (i in 1:dim(t)[1]){
    t[i,]<-t[i,]/sum(t[i,])*100
}
```

Veamos ahora como en un mismo gráfico de frecuencias podemos trabajar con 3 variables: Embarked, Survived y class.

Mostramos el gráfico de embarcados por Pclass:

```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(data = totalData[1:filas,],aes(x=embarked,fill=survived))+geom_bar(position="fill")+facet_wrap(~class)
```

Aquí ya podemos extraer mucha información. Como propuesta de mejora se podría hacer un gráfico similar trabajando solo la clase. Habría que unificar toda la tripulación a una única categoría.

Comparamos ahora dos gráficos de frecuencias: Survived-SibSp y Survived-Parch


```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(data = totalData[1:filas,],aes(x=sibsp,fill=survived))+geom_bar()
ggplot(data = totalData[1:filas,],aes(x=parch,fill=survived))+geom_bar()
```

Vemos como la forma de estos dos gráficos es similar. Este hecho nos puede indicar presencia de correlaciones altas. Hecho previsible en función de la descripción de las variables.

Veamos un ejemplo de construcción de una variable nueva: Tamaño de familia

```{r echo=TRUE, message=FALSE, warning=FALSE}
totalData$FamilySize <- totalData$sibsp + totalData$parch +1;
totalData1<-totalData[1:filas,]
ggplot(data = totalData1[!is.na(totalData[1:filas,]$FamilySize),],aes(x=FamilySize,fill=survived))+geom_histogram(binwidth =1,position="fill")+ylab("Freqüència")
```

Se confirma el hecho de que los pasajeros viajaban mayoritariamente en familia. No podemos afirmar que el tamaño de la familia tuviera nada que ver con la posibilidad de sobrevivir pues nos tememos que estadísticamente el hecho de haber más familias de alrededor de cuatro miembros debería de ser habitual. Es un punto de partida para investigar más.

Veamos ahora dos gráficos que nos compara los atributos Age y Survived.
Observamos como el parámetro position="fill" nos da la proporción acumulada de un atributo dentro de otro.

```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(data = totalData1[!(is.na(totalData[1:filas,]$age)),],aes(x=age,fill=survived))+geom_histogram(binwidth =3)
ggplot(data = totalData1[!is.na(totalData[1:filas,]$age),],aes(x=age,fill=survived))+geom_histogram(binwidth = 3,position="fill")+ylab("Frequencia")
```

Observamos como el parámetro position="fill" nos da la proporción acumulada de un atributo dentro de otro. Parece que los niños tuvieron más posibilidad de salvarse.

## Conclusiones finales

Los datos tienen una calidad correcta y están mayoritariamente muy informadas. Disponen de una variable de clase "survived" que las hace aptas por un clasificador.
A parte de la mayor supervivencia de mujeres y niños y de pasajeros de primera clase podemos observar la juventud de los pasajeros y la tripulación. Se observa también una gran cantidad de personas que viajaban con sus familias.

****
# Ejercicios
****

## Ejercicio 1:

Propón un proyecto completo de minería de datos. La respuesta tiene que coincidir con las fases típicas del ciclo de vida de un proyecto de minería de datos. No hay que desarrollar las tareas de cada fase. Para cada fase indica cuál es el objetivo de la fase y el producto que se obtendrá. Utiliza ejemplos de qué y cómo podrían ser las tareas. Si hay alguna característica que hace diferente el ciclo de vida de un proyecto de minería respecto a otros proyectos indícalo.


Cuando hablamos de un proyecto de minería de datos, nos estamos refiriendo a las diferentes fases que nos encontramos en dicho proyecto desde el paso de los datos a la sabiduría de los mismos. Es por ello que se puede dividir el ciclo de vida de un proyecto de minería de datos en las siguientes fases, cabe destacar que no todos los proyectos tienen que seguir estas fases:


### Definición de la tarea de data mining
El principal objetivo de esta fase es el identificar qué se busca estudiar/dar respuesta con el proyecto. Esta fase es la más importante en el ciclo de vida, ya que si no conseguimos definir bien dicho objetivo, por mucho que el resto de fases estén perfectas no vamos a llegar a las conclusiones adecuadas.

Por lo tanto, en este fase tenemos que analizar el problema a resolver y cómo lo vamos a resolver. En nuestro caso, el ejemplo que vamos a usar va a ser el estudio del cáncer da mama, es decir, saber si una persona tiene cáncer de mama o no a partir de las características de sus células (tamaño de las células, grosor de las paredes celulares...).

Viendo el objetivo de nuestro caso de estudio, podemos identificar que es un problema de clasificación, es decir, indentificar si una persona está en uno de estos dos grupos bien definidos: si su cáncer es maligno, o si su cáncer es benigno. Por otro lado, este problema de clasificación es del tipo aprendizaje supervisado, hace referencia a que vamos a tener un conjunto de datos que ya están "clasificados", es decir, vamos a tener un dataset en el que para cada persona a parte de los datos correpondientes a la célula, vamos a saber si eso significa que el cáncer es benigno o maligno. De tal forma, que nos permita hacer uso de un algoritmo de clasificación, para que así clasifique de forma correcta, y posteriormente predecir si el cáncer de una persona es benigno o maligno.

Al hacer uso de los algoritmos de clasifación podemos usar: árboles de decisión, redes neuronales y las reglas de clasificación.


### Origen de los datos
Una vez que tenemos claro el objetivo de nuestro proyecto de minería, tenemos que obtener los datos para así poder construir el modelo. A la hora de obtener dichos datos lo podemos hacer haciendo uso de: web scraping, de ficheros, de bases de datos, de repositorios de datos...

Respecto a nuestro caso de estudio, vamos a considerar que somos un grupo de científicos dentro de un hospital y que dicha información es de fácil acceso a partir de un data warehouse, es decir, podemos obtener todos esos datos necesarios para poder alcanzar el objetivo del proyecto. Por otro lado, esos datos se irán consiguiendo a partir de los pacientes que acudirán a nuestro hospital.


### Preparación de datos
Cuando ya tenemos todos los datos necesarios para construir el modelo, nos podemos encontrar con varios inconvenientes, como por ejemplo: hay registros en lo que en determinados campos no hay un valor, columnas que son numéricas y la información viene en formato string, puede ser que un mismo atributo esté medido en la misma magnitud pero en diferentes unidades como por ejemplo nanómetros y micrómetros...

El objetivo de esta fase es solucionar todos estos problemas que nos vamos encontrando, de tal forma que los datos tengan la mayor calidad posible, para que así el modelo funcione correctamente.

Dentro de esta fase podemos realizar la limpieza de los datos, es decir, eliminar aquellos datos que son innecesarios o redundantes. En nuestro caso, podría ser que hay registros de pacientes duplicados, o que hay registros en los que el valor del grosor de la columna de la cécula está indefinido...

Por otro lado, podemos realizar la transformación de algunos datos, en nuestro caso, los datos numéricos pueden venir en formato string, y realmente si vamos a trabajar con ellos es mejor que sean numéricos. Además, podemos tener datos que estén medidos en diferentes escalas y si vamos a usar una red neuronal para realizar el modelo de clasificación, vamos a tener que normalizar los valores en el intervalo entre 0 y 1. También tenemos que etiquetar cada registro, es decir, indicar si un paciente con X datos tiene un cáncer de mama benigno o maligno, de tal forma que luego nos permita clasificar bien y poder predecir...

Finalmente, puede ser que tengamos demasiada información, tanto a nivel de registros como a nivel de atributos, esto supone un gran coste de computación. Por lo tanto, tenemos que reducir la dimensionalidad del dataset, para que con menos datos obtengamos el mismo resultado o lo más parecido posible.

A la hora de seleccionar menos registros, hay que tener cuidado ya que puede quedar una población muy sesgada, hay que ser preciso para poder representar la misma información con menos registros. Por otro lado, a la hora de reducir el número de atributos es lo que se denomina selección de variables, podemos hacer uso también del análisis de componentes princiaples para ver que componentes representan la mayor varianza posible con menos dimensiones.


### La contrucción de modelos
Una vez que ya tenemos el dataset con la mayor calidad posible, podemos construir el modelo para resolver el problema que nos hemos planteado. El objetivo de esta fase es seleccionar el modelo que mejor se adapte a nuestros datos, es decir dentro de nuestro caso, en la clasifcación podemos hacer uso de: árboles de decisión, redes neuornales y reglas de clasificación. Tenemos que elegir el modelo que mejor creemos que va a funcionar, esto no significa que no podamos hacer uso de los demás, de hecho deberíamos para saber cuál es el modelo que ofrece los mejores resultados para nuestro problema.

Respecto a nuestro caso, tenemos que tratar de minimizar el error, ya que si no conseguimos clasificar muy bien puede ser un modelo muy desastroso, los falsos negativos son un peligro, ya que podríamos indicar a una persona que no tiene cáncer de mama cuando realmente sí que lo tiene, por lo tanto tenemos que minimizar ese porcentaje.


### Evaluación e interpretación del modelo
Cuando ya hemos construido el modelo, debemos preguntarnos si ese modelo es el mejor, es decir, si tiene la suficiente calidad, si tenemos que volver a empezar, si tenemos que elegir otro modelo...

El objetivo de esta fase es evaluar el modelo, para ello se suele utilizar dos conjuntos de datos a partir de un conjunto incial, un conjunto se va a usar para entrenar el modelo y el segundo para comprobar que el modelo realmente funciona y presenta la suficiente calidad.

En nuestro caso, podríamos dividir el dataset inicial en tres subconjuntos, uno para entrenar el modelo, otro para validar el modelo y un tercero para evaluar el modelo.

Una vez que el modelo se ha evaluado, tenemos que realizar una interpretación de los resultados que hemos obtenido, y en este punto hay que tener mucho cuidado porque podemos estar considerando unos resultados que no son lo suficientemente buenos pero realmente nos están proporcionando mucha información y viceversa.


### Integración de los resultados
Esta fase es la final, cuando ya tenemos un modelo que cumple con los objetivos hay que integrar dicho modelo dentro de la cadena de la organización. En nuestro caso, una vez que tenemos el modelo de clasificación, tenemos que proporcionar esa herramienta a los médicos, de tal forma, que con un simple programa e introduciendo los datos de diferentes análisis le permita predecir si el paciente que ha venido a la consulta tiene cáncer de mama o no.

Por último, cabe destacar que esto es un proceso iterativo, es decir, si ya tenemos un modelo "perfecto" no significa que ya hemos ternimando nuestro trabajo, sino que hay que seguir mejorándolo y dar nuevas respustas a otros problemas. 


## Ejercicio 2:
A partir del juego de datos disponible en el siguiente enlace http://archive.ics.uci.edu/ml/datasets/adult , realiza las tareas previas a la generación de un modelo de minería de datos explicados en el Módulo 2. Puedes utilizar de referencia el ejemplo del Titánic.

Nota: Si lo deseas puedes utilizar otro conjunto de datos propio o de algún repositorio datos abiertos siempre que sea similar en diversidad de tipo de variables al propuesto.

### Objetivos
Trabajaremos con el juego de datos “http://archive.ics.uci.edu/ml/datasets/adult” que recoge datos sobre el censo de los Estados Unidos de América en el año 1994.

Las actividades que llevaremos a cabo en esta práctica se realizan en las fases iniciales de un proyecto de minería de datos. Tienen como objetivo obtener un conocimiento sobre los datos, además de preparar el dataset para poder ser usado posteriormente en los modelos de minería de datos.

Algunas de las tareas que se van a realizar son la carga y limpieza del dataset, análisis de las variables, transformación de datos, análisis visuales de datos...


### Procesos iniciales con los datos

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Cargamos el juego de datos
df = read.table(file="./data/adult.data", fileEncoding="UTF-8", sep=",", na.strings = "NA")

# Cambiamos el nombre de las columnas tal y como se llaman según la página web
colnames(df) = c("age", "workclass", "fnlwgt", "education", "education-num", "marital-status", "occupation", "relationship", "race", "sex", "capital-gain", "capital-loss", "hours-per-week", "native-country", "salary")

# Creamos una copia del dataframe, ya que va a ser la copia sobre la que vamos a realizar las modificaciones
df_copy = df

# Visualizamos los datos
str(df_copy)

# https://rdrr.io/cran/arules/man/Adult.html
# https://lwmachinelearning.wordpress.com/portfolio/adults-data-set/
```

Vemos que tenemos 32561 registros y 15 variables que los caracterizan.

Revisamos la descripción de las variables y si los tipos de variable se corresponden al que hemos cargado:

**age**
    integer - edad de la persona.
    
**workclass**
    factor - clase de trabajo.
    
**fnlwgt**
    integer - peso final de la población que representa.
    
**education**
    ordered factor - nivel de educación.
    
**education-num**
    integer - número de nivel de educación.
    
**marital-status**
    factor - estado civil de la persona.
    
**occupation**
    factor - ocupación de la persona.
    
**relationship**
    factor - tipo de relación de la persona.
    
**race**
    factor - raza de la persona.
    
**sex**
    factor - sexo de la persona.
    
**capital-gain**
    integer - ganacias obtenidas.
    
**capital-loss**
    integer - pérdidas.
    
**hours-per-week**
    integer - media de horas trabajadas por semana.
    
**native-country**
    integer - país de origen.
    
**salary**
    ordered factor - nivel de ingresos.
    
Tal y como podemos observar, las variables numéricas sí que están bien definidas, sin embargo, las variables no numéricas las reconoce como chr en vez de factor, por lo que vamos a solucionar eso, pero antes como podemos comprobar en los atributos de tipo chr sus valores se guardan con un espacio delante, así que hay que solucionar este problema antes:
```{r}
# Método que elimina los espacios en blanco del principio y del final
trim = function (x) gsub("^\\s+|\\s+$", "", x)

# Quitamos los espacios para cada columna de tipo factor
df_copy$workclass = trim(df_copy$workclass)
df_copy$education = trim(df_copy$education)
df_copy$`marital-status` = trim(df_copy$`marital-status`)
df_copy$occupation = trim(df_copy$occupation)
df_copy$relationship = trim(df_copy$relationship)
df_copy$race = trim(df_copy$race)
df_copy$sex = trim(df_copy$sex)
df_copy$`native-country` = trim(df_copy$`native-country`)
df_copy$salary = trim(df_copy$salary)

str(df_copy)
```

Una vez hemos eliminado los espacios de la variable de tipo character, las podemos convertir a factor:
```{r}
# Convertirmos las columna a factor
df_copy$workclass = as.factor(df_copy$workclass)
df_copy$education = factor(df_copy$education, ordered = TRUE, levels = c("Preschool", "1st-4th", "5th-6th", "7th-8th", "9th", "10th", "11th", "12th", "HS-grad", "Prof-school", "Assoc-acdm", "Assoc-voc", "Some-college", "Bachelors", "Masters", "Doctorate"))
df_copy$`marital-status` = as.factor(df_copy$`marital-status`)
df_copy$occupation = as.factor(df_copy$occupation)
df_copy$relationship = as.factor(df_copy$relationship)
df_copy$race = as.factor(df_copy$race)
df_copy$sex = as.factor(df_copy$sex)
df_copy$`native-country` = as.factor(df_copy$`native-country`)
df_copy$salary = factor(df_copy$salary, ordered = TRUE, levels = c("<=50K", ">50K"))

str(df_copy)
```

Lo primero que vamos a hacer es eliminar la columna education-num, ya que dicha información viene representada con la columna education, gracias a esto reducimos la dimensionalidad del dataset:

```{r}
df_copy = df_copy[, -c(5)]
str(df_copy)
```

Estadísticas básicas sobre el conjuntos de datos, nos permite tener una guía sobre los datos que estamos manejando, algunas de estas estadísticas se usarán a lo largo de la práctica:

```{r}
summary(df_copy)
```

Destacar que este dataset recoge el censo en los Estados Unidos de América en los años 90, y por aquel entonces un ciudadano medio sería el siguiente: hombre blanco estadounidense, con una edad entre 35 y 40 años, que trabaja en una empresa (no es autónomo), su educación seguramente sea el graduado escolar o estudios universitarios, posiblemente estará casado por lo civil o soltero, trabajara de media entre 40 y 45 horas semanales, y su salario anual será inferior a 50K dólares.

Cosas que me han llamado bastante la atención de las anteriores estadísticas son:

- Estados Unidos se vende como el país en el que se cumplen los sueños, y para ser el país en el que se cumplen los sueños en la gran mayoría de la gente su clase de trabajo es privada, no son emprendedores.

- Hay mucha población cuyo nivel de educación es de instituto, es verdad que allí la universidad es muy cara y no todo el mundo se lo puede permitir, pero aún así viendo las estadísticas es que solo a partir de la clase media-alta puedes ir a la universidad.

- Respecto a la raza pensaba que había más negros, creo que dentro de los blancos están considerando a los latinos (lo cual tanto en la costa oeste como en el sur hay un gran número de latinos), pero aún así hay casi 10 veces más blancos que negros. También puede ser que los negros por norma general no participen en el censo, por los motivos x.

- En cuanto al sexo, es llamativo ver que hay el doble de hombres que de mujeres, no sé si es porque solo las mujeres independientes realizaron el censo, o lo mejor la proporción de hombres respecto al de mujeres es el doble.

Por otro lado, una vez eliminada la columna education-num, realizamos la comprobación si hay elementos duplicados, para ellos usamos la función proporcionada en R:

```{r}
cat("El número de registros duplicados son: ", nrow(df_copy[duplicated(df_copy),]))
```

```{r}
head(df_copy[duplicated(df_copy),])
```

Analizando con más detalle estos registros vemos que realmento no son duplicados, tienen muchos valores iguales pero no todos los atributos tienen el mismo valor, por lo tanto, nos los podemos considerar como duplicados.

Sabiendo que no hay elementos duplicados, ahora nos toca ver si tenemos valores perdidos, es decir, si hay atributos vacíos (en la documentación del dataset, indica que los valores desconocidos son tratados con un interrogante "?"):

```{r}
colSums(is.na(df_copy))
colSums(df_copy=="?")
```

Como podemos ver, las variables workclass, occupation y native-country, son las únicas variables que toman valores desconocidos. Estos valores podrían dejar de ser desconocidos si por ejemplo establecemos que el valor sea el más frecuente. Como en la práctica no se indica nada, considero que es mejor dejarlos como desconocidos, ya que de lo contario puede afectar en el estudio de los registros que sí que tienen todos los datos.

Posteriormente, tenemos que normalizar algunas variables numéricas, en nuestro caso lo mejor es normalizar el capital ganado y el capital perdido, ya que el rango de ambas variables es diferente y ambas se refieren a lo mismo, el dinero, ya sea de ganacias o de pérdidas:

```{r}
library(scales) # Usamos la librería scales para normalizar por la diferencia

df_copy["n-capital-gain"] = rescale(df_copy$`capital-gain`) # Reescalamos y creamos un nuevo campo
df_copy["n-capital-loss"] = rescale(df_copy$`capital-loss`) # Reescalamos y creamos un nuevo campo

# Visualizamos los cambios
head(df_copy[(df_copy$`capital-gain` != 0 | df_copy$`capital-loss` != 0), c("capital-gain", "capital-loss", "n-capital-gain", "n-capital-loss")])
```

Otro elemento a tener en cuenta, es el crear nuevas variables a partir de las existentes en el dataset, en nuestro caso creo que sería interesante estimar el salario anual, según las horas trabajadas. Para ello, la página oficial del censo en los Estados Unidos (https://www.census.gov/library/publications/1996/demo/p60-189.html), indica que el ingreso medio en los hogares de los EEUU era de 34445 dólares.

Si dividimos ese valor por 12 meses, 30 días y 8 horas de trabajo al día (eso es lo legal), obtenemos que el ingreso por hora laboral es de 11.96 dólares por hora.

```{r}
incomePerHour = round(34445 / (12 * 30 * 8), 2)
incomePerHour
```

Una vez que sabemos el dinero por hora podemos estimar para cada persona el salario anual que tendría según sus horas trabajadas, para ello múltiplicamos por 4 semanas y por 12 meses:

```{r}
df_copy["salary-estimated"] = df_copy$`hours-per-week` * incomePerHour * 4 * 12
head(df_copy[, c("hours-per-week", "salary-estimated", "salary")], 10)
```

Como podemos apreciar en la anterior tabla, el salario estimado hay veces en las que no supera la variable salario con la que viene categorizada el dataset, esto puede ser por varios motivos:

- Se desconoce si el salario de 34445$ en EEUU en 1994 es antes de impuestos o no.
- No se sabe si la variable salario del dataset se refiere a antes de impuestos o no.
- Si se considera como salario en el dataset los ingresos del hogar, tanto el sueldo de la mujer como el del hombre (si están casados). O si hay más miembros de la familia que trabajan. No se especifica en ningún lado en la documentación del dataset, no está del todo bien explicada.
- Es una estimación, no es lo mismo el dinero que se paga a alguién con un trabajo cualificado que no...

En resumen, esta estimación nos permite hacernos una idea del sueldo medio que correspondería según las horas trabajadas, no significa que sea exactamente lo que ganaban.

Por último, es interesante discretizar determinados valores, para que así luego sea más fácil de analizar:

```{r}
# Discretizamos la edad
df_copy["d-age"] = ordered(cut(df_copy[[ "age"]], breaks = c(15,25,45,65,100), labels = c("Young", "Middle-aged", "Senior", "Old")))

# Discretizamos la jornada laboral
df_copy["d-hours-per-week"] = ordered(cut(df_copy[[ "hours-per-week"]], c(0,25,40,60,168)), labels = c("Part-time", "Full-time", "Over-time", "Workaholic"))

# Discretizamos el capital ganado
df_copy["d-capital-gain"] = ordered(cut(df_copy[[ "capital-gain"]],
                                        c(-Inf,0,median(df_copy[["capital-gain"]][df_copy[["capital-gain"]]>0]), Inf)), 
                                    labels = c("None", "Low", "High"))

# Discretizamos el capital perdido
df_copy["d-capital-loss"] = ordered(cut(df_copy[[ "capital-loss"]],
                                        c(-Inf,0, median(df_copy[["capital-loss"]][df_copy[[ "capital-loss"]]>0]), Inf)),
                                    labels = c("None", "Low", "High"))

str(df_copy)
```

En la siguiente tabla podemos ver cómo está quedando nuestro dataset:
```{r}
head(df_copy)
```

Finalmente, antes de terminar vamos a realizar un análisis de componentes principales, para así saber qué componentes nos permite representar las máxima variablidad medida a partir de la varianza.

Lo primero que debemos hacer es quedarnos con la variables numéricas, para así calcular la matriz de correlaciones o de covarianzas:

```{r}
df_copy_numeric = df_copy[, unlist(lapply(df_copy, is.numeric))]
head(df_copy_numeric)
```

Al ser variables con diferentes magnitudes, es mejor elegir la matriz de correlaciones:

```{r}
cor(df_copy_numeric)
```

Una vez que tenemos la matriz de correlaciones, podemos hacer el análisis de componentes principales:

```{r}
ACP.cor = prcomp(x=df_copy_numeric, center = TRUE, scale. = TRUE)
summary(ACP.cor)
```

Con los resultados obtenidos vemos que la proporción de varianza es diferente para cada uno de los componentes.

Por lo tanto, seleccionado los tres primeros componentes podemos proyectar más o menos la misma información que en el dataset original, pero reduciendo la dimensionalidad. Como podemos observar, las tres primeras dimensiones representan la mayor parte de la varianza total con un 75.32%. Y con la cuarta componente conseguimos respresentar el 88.58% de la variabilidad.

Para ver qué componentes principales son los que debemos elegir podemos hacer uso de la regla de Kaiser-Guttman o un scree plot:

```{r}
# Realizamos el cálculo de la regla de Kaiser-Guttman
valores_propios = ACP.cor$sdev^2
media_valores_propios = sum(valores_propios) / length(valores_propios)
cat("Los valores propios son: ", valores_propios)
cat("La media de los valores propios es: ", media_valores_propios)
```

Como podemos apreciar, según Kaiser-Guttman hay que elegir como componentes princiales aquellos que cuyos valores propios superen la media de los mismos, en este caso los componentes 1, 2, 3 y 4 lo hacen, por lo que sí que es adecuado elegir estos cuatro componentes. De tal forma, que una vez que tenemos claro que éstos son los cuatro componentes princiaples ya conseguimos reducir la dimensionalidad del dataset original.



### Análisis visual de los datos

Una vez que ya hemos limpiado el dataset y le hemos dado el formato que nosotros queremos, podemos realizar diferentes análisis visuales, para entender mejor los datos.

Cabe destacar que, aunque este dataset está enfocado al salario de los ciudadanos de los EEUU en 1994, se puede responder muchísimas preguntas, como por ejemplo: ¿saber si la sociedad es racista?, ¿si es machista?, ¿si es clasista?... Por lo tanto, el análisis que se puede hacer de este dataset es muy amplio, es por ello que nos vamos a ir centrando poco a poco en estas preguntas.

El primer punto en el que nos vamos a centrar es en el estudio del salario, es decir, si el salario depende de otros factores (si hay una relación). A priori, cuando pensamos en la gente joven suele ganar menos que la gente con más experiencia, por lo que vamos a comprobar si esto es verdad:

```{r}
ggplot(data = df_copy, aes(x=`d-age`,fill=salary)) + geom_bar(position = "fill") +
  xlab("Edad") +
  ylab("Frecuencia") +
  scale_fill_discrete(name = "Salario") +
  ggtitle("Salario respecto a la edad")
```

Tal y como era de esperar, mientras que los jóvenes están estudiando y formándose, y lo señores mayores están jubilados viviendo en este caso de su plan de pensiones, la gente entre 25 y 65 años suelen ser los trabajadores de cualquier sociedad, es por ello, que son los que más ganan.

Algo llamativo del anterior gráfico, es que no hay ninguna clase de edad que supere el 50% de frecuencia respecto a un salario mayor de 50k, seguramente esto sea así ya que, los datos son de 1994 y por aquel entonces en EEUU no se tenía unos sueldos demasiados elevados (cabe destacar que hasta 1991 estuvieron en un guerra fría con la URSS, lo cual supuso una gran inversión a nivel milatar, eso significa inflación).

Estudiando el cómo varía el salario, podemos ver si la clase de trabajo influye:

```{r}
ggplot(data = df_copy, aes(x=workclass,fill=salary)) + geom_bar(position = "fill") +
  xlab("Clase de trabajo") +
  ylab("Frecuencia") +
  scale_fill_discrete(name = "Salario") +
  ggtitle("Salario respecto a la clase de trabajo") + 
  theme(axis.text.x = element_text(angle = 45, vjust = 0.6))
```

Como vemos en la anterior gráfica, la gente que trabaja de forma autónoma en una entidad corporativa tienen la mejor calidad de vida, ya que más del 50% de la gente que es de este tipo de clase cobra más de 50K dólares anuales. El resto de clases tienen un nivel de sueldos similar.

Algo llamativo es ver cómo son los sueldos dentro del gobierno de EEUU, podemos apreciar que aquellos miembros del gobierno federal, es decir el gobierno de la nación, tienen mejores sueldos que los que trabajan para el gobierno estatal o local, estos dos últimos tienen sueldos similares.

Una pregunta que nos podemos hacer es si el nivel de educación afecta al salario, a priori diríamos que sí, pero ésto lo tenemos que comprobar:

```{r}
ggplot(data = df_copy, aes(x=education,fill=salary)) + geom_bar(position = "fill") +
  xlab("Nivel educativo") +
  ylab("Frecuencia") +
  scale_fill_discrete(name = "Salario") +
  ggtitle("Salario respecto al nivel educativo") + 
  theme(axis.text.x = element_text(angle = 45, vjust = 0.6))
```

Vemos una gráfica muy curiosa está altamente correlacionado, podemos afirmar que sí que es verdad que a mayor nivel de estudios más dinero se gana. Aunque prof-school se encuentre en el medio de la gráfica con una frecuencia de 75% este nivel de estudios es equiparable a un máster o un doctorado.

La principal diferencia entre professional school y master/doctarado es que el primero busca preparar a estudiantes en un campo específico (medicina, farmacia, business...), mientras que los segundos son cursos académicos más avanzados más allá de un graduado.

Por lo tanto, aunque nos muestra la información según un orden que nosotros hemos establecido, ambos niveles son similares, cada uno en su campo.

Otra pregunta que nos podríamos hacer es, ¿el salario está relacionado con la raza?, ¿es una socidad racista la estadounidense?:

```{r}
ggplot(data = df_copy, aes(x=race,fill=salary)) + geom_bar(position = "fill") +
  xlab("Raza") +
  ylab("Frecuencia") +
  scale_fill_discrete(name = "Salario") +
  ggtitle("Salario respecto a la raza") + 
  theme(axis.text.x = element_text(angle = 45, vjust = 0.6))
```

```{r}
summary(df_copy$race)
```

Como podemos apreciar, están bastante nivelados los sueldos y la raza, por lo que no podemos concluir que sea una socidad racista. Sin embargo, cabe destacar que los sueldos de los asiáticos son muy altos respecto a la población que hay de ellos, por lo tanto, tienen que ser por norma general gente de negocios.

Otro aspecto a destacar es que, los negros tienen unos salarios más bajos respecto a los asiáticos, siendo el triple de personas. También los blancos tienen el mismo nivel respecto a los asiáticos siendo 27 veces más que ellos. Por lo tanto, como bien hemos mencionado esto se debe a que en la raza asiática hay mucho poder.

Analizando la gráfica anterior no podemos garantizar que sea una sociedad racista, por lo tanto vamos a seguir analizando. ¿Dependiendo de la raza se tiene más oportunidades en la educación?

```{r}
ggplot(data = df_copy, aes(x=education,fill=race)) + geom_bar(position = "fill") +
  xlab("Educación") +
  ylab("Frecuencia") +
  scale_fill_discrete(name = "Raza") +
  ggtitle("Raza respecto a la educación") + 
  theme(axis.text.x = element_text(angle = 45, vjust = 0.6))
```

```{r}
summary(df_copy$race)
```

Tal y como podemos apreciar hay un mayor número de la raza blanca, por lo que en la gráfica observamos que el mayor número de alumnos en cada nivel educativo es el de la raza blanca. Respecto a la raza negra vemos que en proporción al final de la etapa educativa abandonan muchos, mientras que en la raza asiática es al contrario.

Esto se puede deber a que tal y como hemos visto antes, la raza blanca y la asiática son los que más ingresos tienen, por lo tanto, a medida que aumenta el nivel educativo más caro se convierte, es por ello, que la raza negra abandona en la etapa final educativa, no tienen los recursos necesarios como para permitírselo.

Por lo tanto, encontrar el origen del porqué la raza negra tienen de media un salario menor, es díficl de saber, ya que no tenemos toda la información necesaria sobre la causa del problema. Entonces, no podemos considerar que sea una sociedad racista, pero podemos mirar si en otros ámbitos lo es, como en el laboral:

```{r}
ggplot(data = df_copy, aes(x=race,fill=salary)) + geom_bar(position = "fill") +
  facet_wrap(~occupation) +
  xlab("Raza") +
  ylab("Frecuencia") +
  scale_fill_discrete(name = "Salario") +
  ggtitle("Salario respecto a la raza y el sector de trabajo") + 
  theme(axis.text.x = element_text(angle = 45, vjust = 0.6))
```

Dada la anterior gráfica, entendemos la causa de que los asiáticos y blancos tengan un salario elevado respecto a otras razas, y es que la gran parte de los ejecutivos (exec-managerial) y de profesionales especializados (prof-speciality) son de raza asiática y blanca. Sin embargo, los trabajos manuales y no espcializados como: trabajo artesanal (craft-repair), agricultura (farming-fishing), entre otros, son realizados por personas de raza negra.

Por lo tanto, esta gráfica y la anterior estén relacionadas, ya que la raza negra al no tener tanto poder adquisitivo y no poder permitirse ir a la universidad no pueden asipirar a realizar trabajos cualificados, sin embargo, la raza asiática y la blanca sí. Con esto llegamos a la conclusión de que la sociedad estadounidense de 1994 no eran racistas, pero hay un par de razas la negra y la hindú que por motivos que desconocemos tienen un menor poder adquisitivo (probablemnte, respecto a la primera raza sea por la esclavitud que vivieron en el pasado, ya que no parten del mismo punto de salida que los asiáticos o los blancos, es decir, hace 200 años sí que eran racistas pero en 1994 no).

También nos podríamos preguntar si la sociedad de EEUU es una sociedad machista, vamos a comprobar si esto es cierto:

```{r}
ggplot(data = df_copy, aes(x=sex,fill=salary)) + geom_bar(position = "fill") +
  xlab("Sexo") +
  ylab("Frecuencia") +
  scale_fill_discrete(name = "Salario") +
  ggtitle("Salario respecto al sexo")
```

Como bien vemos sí que hay una difenrecia significativa entre sexos pero, ¿se podría considerar machista? Hay que analizar con más profundiad:

```{r}
ggplot(data = df_copy, aes(x=sex,fill=salary)) + geom_bar(position = "fill") +
  facet_wrap(~`marital-status`) +
  xlab("Sexo") +
  ylab("Frecuencia") +
  scale_fill_discrete(name = "Salario") +
  ggtitle("Salario respecto al sexo y estado civil") + 
  theme(axis.text.x = element_text(angle = 45, vjust = 0.6))
```

Al realizar la anterior gráfica teniendo en cuenta el estado civil, se podría observar si las mujeres que están casadas cobran menos que los hombres (ya sea porque tienen redución de jornada, o directamente porque no trabajan), lo cual vemos que no es cierto, de hecho todo lo contrario, cuando más están nivelados los sueldos son cuando están casados. Por lo tanto, hemos visto que sí que hay una ventaja en el nivel del salario de los hombres respecto al de las mujeres.

Esa ventaja en los salarios de los hombres respecto a las mujeres podría darse porque tienen niveles de educación diferntes:

```{r}
ggplot(data = df_copy, aes(x=sex,fill=salary)) + geom_bar(position = "fill") +
  facet_wrap(~education) +
  xlab("Sexo") +
  ylab("Frecuencia") +
  scale_fill_discrete(name = "Salario") +
  ggtitle("Salario respecto al sexo y la educación") + 
  theme(axis.text.x = element_text(angle = 45, vjust = 0.6))
```

Pero a la vista está, que hombres y mujeres con el mismo nivel de estudios, en todos los casos son los hombres los que tienen un sueldo superior que las mujeres. Por lo tanto, podemos concluir que sí que es una sociedad machista, por lo menos en cuanto al nivel de salarios.

Otro aspecto a tener en cuenta, es si las mujeres tienen las mismas oportunidades que los hombres en la educación:

```{r}
ggplot(data = df_copy, aes(x=education,fill=sex)) + geom_bar(position = "fill") +
  xlab("Education") +
  ylab("Frecuencia") +
  scale_fill_discrete(name = "Sexo") +
  ggtitle("Sexo respecto a la educación") + 
  theme(axis.text.x = element_text(angle = 45, vjust = 0.6))
```

Aunque a priori hay más hombres que mujeres que tienen una educación mayor, esto también se debe a que el número de registros que tenemos de hombres es mayor que el de mujeres, para ser más exacto es el doble de los datos, por lo tanto, lógicamente va a haber una desproporción en cuanto a cantidad, pero si analizamos las gráficas ambas siguen una tendencia similar al principio hay más hombre que mujeres pero a partir del 7-8 grado empieza a subir el número de mujeres, seguramente sea por el abandono de los hombres. También es verdad que al final de la etapa educativa vuelve a subir la proporción de hombres respecto a la de mujeres.

En la siguiente ejecución podemos ver el número de registros de mujeres respecto hombres, y vemos que es el doble:

```{r}
summary(df_copy$sex)
```

Finalmente, para determinar cómo de machista es la sociedad, podríamos ver si en el ámbito laboral es similar la proporción entre hombres y mujeres:

```{r}
ggplot(data = df_copy, aes(x=occupation,fill=sex)) + geom_bar(position = "fill") +
  xlab("Sector de trabajo") +
  ylab("Frecuencia") +
  scale_fill_discrete(name = "Sexo") +
  ggtitle("Sexo respecto al trabajo") + 
  theme(axis.text.x = element_text(angle = 45, vjust = 0.6))
```

Respecto al trabajo en el que suelen destacar las mujeres son tres grupos: el trabajo administrativo (adm-clerical), servicios del hogar (priv-house-serv) y otros servicios (other-service).

Como podemos apreciar, los trabajos asociados a las mujeres son los típicos de cuidados de la casa, limpieza, secretaria... Por lo tanto, respecto al mundo laboral la sociedad estadounidense en 1994 era machista.


### Conclusiones
Dado el dataset que se nos ha proporcionado para esta práctica, podemos realizarnos muchas preguntas para saber si dos o más variables están relacioandas.

Mi enfoque en este punto ha sido en un principio el ver si determinadas variables estaban relacioandas con el salario de un persona. Hemos llegado a la conclusión de que: 

- Salarios altos solo se ven a partir de 25 años, esto quiere decir, que la gente joven por norma general estudia y de forma complentaria puede trabajar, lógicamente va a trabajar menos horas y su salario va a ser menor debido a la inexperiencia. Por otro lado, es lógico que los salarios más altos estén entre 25 y 65 años, debido a que ya se junta el conomiento y la experiencia.
- Hemos relacionado el salario con la clase de trabajo, y hemos visto que la gente que más dinero gana son autónomos, también hemos podido ver la diferencia de salarios entre el gobierno federal, estatal y local.
- Posteriormente, hemos comprobado si el sueldo de una peronsa dependía del nivel educativo, y hemos visto que estaban muy correlacionadas positivamente ambas variables, es decir, que a mayor nivel educativo mayor salario.
- Hemos comprobado si el salario se veía afectado por la raza.
- Hemos comprobado si el salario se veía afectado por el sexo.

De los dos último puntos anteriores, nos ha surgido una par de preguntas. La primera ha sido, ¿la sociedad estadounidense es racista?

- Tras hacer un primer análisis, hemos visto que sí que había diferencias entre razas en cuanto a nivel de salarios, tanto la raza asiática como la blanca son los que mayores salarios tenían. Pero esto no significa que sea una sociedad racista.
- Hemos analizado si el nivel educativo se veía influenciado por la raza, y hemos visto que ligeramente sí. La educación en EEUU es muy cara y por lo tanto a medida que avanzas a nivel educativo más cara se vuelve, es por ello que razas como la negra y la hindú reducían el número de personas en el ámbito educativo, sin embargo, la raza asiática y la blanca no se veían afectadas. Aunque sí que hay una ligera diferencia entre razas, no podemos determinar que sea una sociedad racista.
- Hemos comprobado el sector de trabajo en cada raza, y hemos visto que la raza negra realiza trabajos menos culificados que las razas blanca y asiática. Lógicamente, hemos visto el porqué la raza negra tenía un descenso de personas en el ámbito universitario, y es que los salarios entre razas eran diferentes. Es verdad que la raza negra hacía trabajos no cualificados, pero eso no significa que una sociedad sea racista, por lo tanto, hemos intuido que el punto de partida de la raza negra respecto a las demás es diferente, debido a la esclavitud que vivió en su pasado, esto significa que en el pasado sí que era una sociedad racista pero que en 1994 no.

La segunda pregunta que nos hemos hecho ha sido, ¿la sociedad es machista?

- Primero hemos hecho un análisis básico sobre los salarios entre el género masculino y femenino, y nos hemos dado cuenta que sí que había una diferencia significativa.
- Después hemos comprobado si era verdad que las mujeres dependiendo de su estado civil variaba su salario, por si estaban casadas y en el caso de tener hijos reducian su jornada laboral, o dejaban el trabajo. Hemos visto que no era cierta la suposoción, sino que cuando más equiparados estaban los salarios eran cuando estaban casadas las mujeres.
- Nos hemos planteado si la diferencia entre salarios era por el nivel educativo, y nos hemos dado cuenta que sí que hay diferencia entre el hombre y la mujer, ya que el primero siempre tiene un salario mayor que el segundo. Por lo tanto, llegamos a la conclusión de que sí que eran machistas respecto al ámbito salarial.
- Hemos comprobado si tanto hombres como mujeres tenían las mismas oportunidades educativas, y hemos visto que sí.
- Finalmente, hemos analizado si los sectores en los que predomina la mujer son los típicos de admnistración, limpieza de hogares... Y la respuesta era que sí, por lo tanto, hemos concluido que sí que era una sociedad machista en el ámbito laboral.

En resumen, son muchas las preguntas que nos podemos hacer pero siendo sinceros creo que las anteriores son las más interesantes, nos ha permitido tener una visión de cómo era la sociedad en los Estados Unidos de América en 1994.


***
# Criterios de evaluación
***

Ejercicio 1

Concepto y peso en la nota final

El objetivo del proyecto está correctamente definido con suficiente concreción y se puede resolver con técnicas de minería de datos. 15%

Las fases del ciclo de vida están correctamente expresadas. Los ejemplos son clarificadores. La justificación y argumentación de las decisiones que se han tomado. 20%

Ejercicio 2

Se carga la base de datos, se visualiza su estructura y se explica los hechos básicos que explican los datos. 5%

Se estudia si existen atributos vacíos o en diferentes escalas que haya que normalizar. Si es el caso se adoptan medidas para tratar estos atributos. Se construye una nueva variable útil a partir de las existentes. Se discretiza algún atributo. 20%

Se analizan los datos de forma visual y extraen conclusiones tangibles. Hay que elaborar un discurso coherente y con conclusiones claras. 30%

Se trata en profundidad algún otro aspecto respecto a los datos presentado en el Módulo 2 10%
