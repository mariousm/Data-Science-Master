---
title: 'Minería de datos: PEC1'
author: "Autor: Mario Ubierna San Mamés"
date: "marzo 2021"
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
    includes:
      in_header: 75.584-PEC-header.html
  pdf_document:
    highlight: zenburn
    toc: yes
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

******
# Introducción
******
## Presentación
Esta prueba de evaluación continuada cubre el módulo 1,2 y 8 del programa de la asignatura.  

## Competencias
Las competencias que se trabajan en esta prueba son:

* Uso y aplicación de las TIC en el ámbito académico y profesional
* Capacidad para innovar y generar nuevas ideas.
* Capacidad para evaluar soluciones tecnológicas y elaborar propuestas de proyectos teniendo en cuenta los recursos, las alternativas disponibles y las condiciones de mercado.
* Conocer las tecnologías de comunicaciones actuales y emergentes, así como saberlas aplicar convenientemente para diseñar y desarrollar soluciones basadas en sistemas y tecnologías de la información.
* Aplicación de las técnicas específicas de ingeniería del software en las diferentes etapas del ciclo de vida de un proyecto.
* Capacidad para aplicar las técnicas específicas de tratamiento, almacenamiento y administración de datos.
* Capacidad para proponer y evaluar diferentes alternativas tecnológicas para resolver un problema concreto.
* Capacidad de utilizar un lenguaje de programación.  
* Capacidad para desarrollar en una herramienta IDE.  
* Capacidad de plantear un proyecto de minería de datos.  

## Objetivos
* Asimilar correctamente el módulo 1 y 2.
*	Qué es y qué no es MD.
*	Ciclo de vida de los proyectos de MD.
*	Diferentes tipologías de MD.
* Conocer las técnicas propias de una fase de preparación de datos y objetivos a alcanzar.  

## Descripción de la PEC a realizar
La prueba está estructurada en 1 ejercicio teórico/práctico y 1 ejercicio práctico que pide que se desarrolle la fase de preparación en un juego de datos.  
Deben responderse todos los ejercicios para poder superar la PEC.  

## Recursos

Para realizar esta práctica recomendamos como punto de partida la lectura de los siguientes documentos:

* Módulo 1, 2 y 8 del material didáctico.
* Ciclo de vida de un proyecto de minería de datos: https://es.wikipedia.org/wiki/cross_industry_standard_process_for_data_mining#Fases_principales
* A lo aparta del enunciado de la actividad disponéis de unos materiales de ggplot2
* El aula laboratorio de R para resolver dudas o problemas.
* RStudio Cheat Sheet: Disponible en el aula Laboratorio de Minería de datos.
* R Base Cheat Sheet: Disponible en el aula Laboratorio de Minería de datos.
  

## Formato y fecha de entrega
El formato de entrega es: usernameestudiant-PAC1.html (pdf o word) y rmd.
Fecha de Entrega: 31/03/2021.
Se tiene que depositar la PEC en el buzón de entregas del aula.


## Nota: Propiedad intelectual 

A menudo es inevitable, al producir una obra multimedia, hacer uso de recursos creados por terceras personas. Es por lo tanto comprensible hacerlo en el marco de una práctica de los estudios de Informática, Multimedia y Telecomunicación de la UOC, siempre y cuando esto se documente claramente y no suponga plagio en la práctica. 

Por lo tanto, al presentar una práctica que haga uso de recursos ajenos, se debe presentar junto con ella un documento en qué se detallen todos ellos, especificando el nombre de cada recurso, su autor, el lugar dónde se obtuvo y su estatus legal: si la obra está protegida por el copyright o se acoge a alguna otra licencia de uso (Creative Commons, licencia GNU, GPL ...). 
El estudiante deberá asegurarse de que la licencia no impide específicamente su uso en el marco de la práctica. En caso de no encontrar la información correspondiente tendrá que asumir que la obra está protegida por copyright. 

Deberéis, además, adjuntar los ficheros originales cuando las obras utilizadas sean digitales, y su código fuente si corresponde.  


****
# Ejemplo de solución del ejercicio 2
****
****
## Objetivos
****
Como ejemplo, trabajaremos con el juego de datos "Titanic.csv" que recoge datos sobre el famoso crucero.

Las actividades que llevaremos a cabo en esta práctica se realizan en las fases iniciales de un proyecto de minería de datos. Tienen como objetivo obtener un dominio de los datos con las que construiremos el modelo de minería. Tenemos que conocer los datos profundamente tanto en su formato como contenido. Tareas típicas en esta fase pueden ser la selección de características o variables, la preparación del juego de datos para posteriormente ser consumido por un algoritmo e intentar extraer el máximo conocimiento posible de los datos.

Desarrollaremos un subconjunto de tareas mínimas y de ejemplo. Podemos incluir muchas más y mucho más profundas, como hemos visto al material docente.

## Procesos iniciales con los datos

Primero contacto con el juego de datos

Instalamos y cargamos las librerías ggplot2 y dplry 

```{r echo=TRUE, message=FALSE, warning=FALSE}
# https://cran.r-project.org/web/packages/ggplot2/index.html
if (!require('ggplot2')) install.packages('ggplot2'); library('ggplot2')
# https://cran.r-project.org/web/packages/dplyr/index.html
if (!require('dplyr')) install.packages('dplyr'); library('dplyr')
```

Cargamos el fichero de datos

```{r}
totalData <- read.csv('./data/titanic.csv',stringsAsFactors = FALSE)
filas=dim(totalData)[1]
```

Cargamos los datos filtrados por tripulación para hacer estudios posteriores

```{r}
totalData_crew=subset(totalData, totalData$class=="engineering crew")
```

Verificamos la estructura del juego de datos principal

```{r}
str(totalData)
```

Vemos que tenemos 2207 registros que se corresponden a los viajeros y tripulación del Titánic y 11 variables que los caracterizan.

Revisamos la descripción de las variables contenidas en el fichero y si los tipos de variable se corresponde al que hemos cargado:

**name**
    string with the name of the passenger.
    
**gender**
    factor with levels male and female.
    
**age**
    numeric value with the persons age on the day of the sinking. The age of babies (under 12 months) is given as a fraction of one year (1/month).
    
**class**
    factor specifying the class for passengers or the type of service aboard for crew members.
    
**embarked**
    factor with the persons place of of embarkment.
    
**country**
    factor with the persons home country.
    
**ticketno**
    numeric value specifying the persons ticket number (NA for crew members).
    
**fare**
    numeric value with the ticket price (NA for crew members, musicians and employees of the shipyard company).
    
**sibsp**
    ordered factor specifying the number if siblings/spouses aboard; adopted from Vanderbild data set.
    
**parch**
    an ordered factor specifying the number of parents/children aboard; adopted from Vanderbild data set.
    
**survived**
    a factor with two levels (no and yes) specifying whether the person has survived the sinking.

Vamos ahora a sacar estadísticas básicas y después trabajamos los atributos con valores vacíos.

```{r echo=TRUE, message=FALSE, warning=FALSE}
summary(totalData)
```

Estadísticas de valores vacíos

```{r}
colSums(is.na(totalData))
colSums(totalData=="")
```

Asignamos valor "Desconocido" para los valores vacíos de la variable "country"

```{r}
totalData$country[is.na(totalData$country)] <- "Desconocido"
```

Asignamos la media para valores vacíos de la variable "age"

```{r}
totalData$age[is.na(totalData$age)] <- mean(totalData$age,na.rm=T)
```

De la información mostrada destacamos que el pasajero más joven tenía 6 meses y el más grande 74 años. La media de edad la tenían en 30 años. También podemos ver 891 sin billete. Revisaremos si se corresponde a la tripulación. También podemos observar el que se pagó por el billete. En este caso se entienden las discrepancias en la fiabilidad de este dato. Parece que los pasajeros que embarcaron a Southampton hacían transbordo de un barco que tenía la tripulación en huelga y por eso no pagaron lo que explicaría la diferencia. Recordemos que la tripulación no pagaba. Sibsp y parch también muestran datos interesantes del viajero con quien más familiares viajaba eran 8 hermanos o mujer y 9 hijos o padre/madre.

Si observamos los NA (valores nulos) vemos que los datos están bastante bien. Decidimos sustituir el valor NA de country por Desconocido por una mayor legibilidad. También proponemos sustituir los NA de age por la media a pesar de que realmente no hace falta.

Es curios como los valores NA de sibsp y parch nos permite deducir que viajaban muchas familias. De hecho, a simple vista, restante la tripulación la gente que viajaba sola era mínima. Este dato la podríamos contrastar también. Sería interesante relacionar la mortalidad del accidente con el tamaño de las familias que viajaban.

Ahora añadiremos un campo nuevo a los datos. Este campo contendrá el valor de la edad discretizada con un método simple de intervalos de igual amplitud.

```{r echo=TRUE, message=FALSE, warning=FALSE}

summary(totalData[,"age"])
```

Discretizamos

```{r}
totalData["segmento_edad"] <- cut(totalData$age, breaks = c(0,10,20,30,40,50,60,70,100), labels = c("0-9", "10-19", "20-29", "30-39","40-49","50-59","60-69","70-79"))
```

Observamos los datos discretizados

```{r}
head(totalData)
```

Vemos como se agrupaban por edad

```{r}
plot(totalData$segmento_edad)
```

Ahora repetimos por el proceso, pero solo por el subconjunto de tripulación filtrado antes

```{r}
totalData_crew["segmento_edad"] <- cut(totalData_crew$age, breaks = c(0,10,20,30,40,50,60,70,100), labels = c("0-9", "10-19", "20-29", "30-39","40-49","50-59","60-69","70-79"))
plot(totalData_crew$segmento_edad)
```

De la discretización de la edad observamos que realmente la gente que viajaba era muy joven. El segmento más grande es de 20 a 29 años. También vemos de la juventud de la tripulación.

## Procesos de análisis visuales del juego de datos

Nos proponemos analizar las relaciones entre las diferentes variables del juego de datos para ver si se relacionan y como.

Visualizamos la relación entre las variables "gender" y "survived":


```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(data=totalData[1:filas,],aes(x=gender,fill=survived))+geom_bar()
```

Otro punto de vista. Survived como función de Embarked:

```{r}
ggplot(data = totalData[1:filas,],aes(x=embarked,fill=survived))+geom_bar(position="fill")+ylab("Frequència")
```

En la primera gráfica podemos observar fácilmente la cantidad de mujeres que viajaban respecto hombres y observar los que no sobrevivieron. Numéricamente el número de hombres y mujeres supervivientes es similar.

A la segunda gráfica de forma porcentual observamos los puertos de embarque y los porcentajes de supervivencia en función del puerto. Se podría trabajar el puerto C (Cherburgo) para ver de explicar la diferencia en los datos. Quizás porcentualmente embarcaron más mujeres o niños... ¿O gente de primera clase?

Obtenemos ahora una matriz de porcentajes de frecuencia.
Vemos, por ejemplo, que la probabilidad de sobrevivir si se embarcó en "C" es de un 56.45%

```{r echo=TRUE, message=FALSE, warning=FALSE}
t<-table(totalData[1:filas,]$embarked,totalData[1:filas,]$survived)
for (i in 1:dim(t)[1]){
    t[i,]<-t[i,]/sum(t[i,])*100
}
```

Veamos ahora como en un mismo gráfico de frecuencias podemos trabajar con 3 variables: Embarked, Survived y class.

Mostramos el gráfico de embarcados por Pclass:

```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(data = totalData[1:filas,],aes(x=embarked,fill=survived))+geom_bar(position="fill")+facet_wrap(~class)
```

Aquí ya podemos extraer mucha información. Como propuesta de mejora se podría hacer un gráfico similar trabajando solo la clase. Habría que unificar toda la tripulación a una única categoría.

Comparamos ahora dos gráficos de frecuencias: Survived-SibSp y Survived-Parch


```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(data = totalData[1:filas,],aes(x=sibsp,fill=survived))+geom_bar()
ggplot(data = totalData[1:filas,],aes(x=parch,fill=survived))+geom_bar()
```

Vemos como la forma de estos dos gráficos es similar. Este hecho nos puede indicar presencia de correlaciones altas. Hecho previsible en función de la descripción de las variables.

Veamos un ejemplo de construcción de una variable nueva: Tamaño de familia

```{r echo=TRUE, message=FALSE, warning=FALSE}
totalData$FamilySize <- totalData$sibsp + totalData$parch +1;
totalData1<-totalData[1:filas,]
ggplot(data = totalData1[!is.na(totalData[1:filas,]$FamilySize),],aes(x=FamilySize,fill=survived))+geom_histogram(binwidth =1,position="fill")+ylab("Freqüència")
```

Se confirma el hecho de que los pasajeros viajaban mayoritariamente en familia. No podemos afirmar que el tamaño de la familia tuviera nada que ver con la posibilidad de sobrevivir pues nos tememos que estadísticamente el hecho de haber más familias de alrededor de cuatro miembros debería de ser habitual. Es un punto de partida para investigar más.

Veamos ahora dos gráficos que nos compara los atributos Age y Survived.
Observamos como el parámetro position="fill" nos da la proporción acumulada de un atributo dentro de otro.

```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(data = totalData1[!(is.na(totalData[1:filas,]$age)),],aes(x=age,fill=survived))+geom_histogram(binwidth =3)
ggplot(data = totalData1[!is.na(totalData[1:filas,]$age),],aes(x=age,fill=survived))+geom_histogram(binwidth = 3,position="fill")+ylab("Frequencia")
```

Observamos como el parámetro position="fill" nos da la proporción acumulada de un atributo dentro de otro. Parece que los niños tuvieron más posibilidad de salvarse.

## Conclusiones finales

Los datos tienen una calidad correcta y están mayoritariamente muy informadas. Disponen de una variable de clase "survived" que las hace aptas por un clasificador.
A parte de la mayor supervivencia de mujeres y niños y de pasajeros de primera clase podemos observar la juventud de los pasajeros y la tripulación. Se observa también una gran cantidad de personas que viajaban con sus familias.

****
# Ejercicios
****

## Ejercicio 1:

Propón un proyecto completo de minería de datos. La respuesta tiene que coincidir con las fases típicas del ciclo de vida de un proyecto de minería de datos. No hay que desarrollar las tareas de cada fase. Para cada fase indica cuál es el objetivo de la fase y el producto que se obtendrá. Utiliza ejemplos de qué y cómo podrían ser las tareas. Si hay alguna característica que hace diferente el ciclo de vida de un proyecto de minería respecto a otros proyectos indícalo.


Cuando hablamos de un proyecto de minería de datos, nos estamos refiriendo a las diferentes fases que nos encontramos en dicho proyecto desde el paso de los datos a la sabiduría de los mismos. Es por ello que se puede dividir el ciclo de vida de un proyecto de minería de datos en las siguientes fases, cabe destacar que no todos los proyectos tienen que seguir estas fases:


### Definición de la tarea de data mining
El principal objetivo de esta fase es el identificar qué se busca estudiar/dar respuesta con el proyecto. Esta fase es la más importante en el ciclo de vida, ya que si no conseguimos definir bien dicho objetivo, por mucho que el resto de fases estén perfectas no vamos a llegar a las conclusiones adecuadas.

Por lo tanto, en este fase tenemos que analizar el problema a resolver y cómo lo vamos a resolver. En nuestro caso, el ejemplo que vamos a usar va a ser el estudio del cáncer da mama, es decir, saber si una persona tiene cáncer de mama o no a partir de las características de sus células (tamaño de las células, grosor de las paredes celulares...).

Viendo el objetivo de nuestro caso de estudio, podemos identificar que es un problema de clasificación, es decir, indentificar si una persona está en uno de estos dos grupos bien definidos: si su cáncer es maligno, o si su cáncer es benigno. Por otro lado, este problema de clasificación es del tipo aprendizaje supervisado, hace referencia a que vamos a tener un conjunto de datos que ya están "clasificados", es decir, vamos a tener un dataset en el que para cada persona a parte de los datos correpondientes a la célula, vamos a saber si eso significa que el cáncer es benigno o maligno. De tal forma, que nos permita hacer uso de un algoritmo de clasificación, para que así clasifique de forma correcta, y posteriormente predecir si el cáncer de una persona es benigno o maligno.

Al hacer uso de los algoritmos de clasifación podemos usar: árboles de decisión, redes neuronales y las reglas de clasificación.


### Origen de los datos
Una vez que tenemos claro el objetivo de nuestro proyecto de minería, tenemos que obtener los datos para así poder construir el modelo. A la hora de obtener dichos datos lo podemos hacer haciendo uso de: web scraping, de ficheros, de bases de datos, de repositorios de datos...

Respecto a nuestro caso de estudio, vamos a considerar que somos un grupo de científicos dentro de un hospital y que dicha información es de fácil acceso a partir de un data warehouse, es decir, podemos obtener todos esos datos necesarios para poder alcanzar el objetivo del proyecto. Por otro lado, esos datos se irán consiguiendo a partir de los pacientes que acudirán a nuestro hospital.


### Preparación de datos
Cuando ya tenemos todos los datos necesarios para construir el modelo, nos podemos encontrar con varios inconvenientes, como por ejemplo: hay registros en lo que en determinados campos no hay un valor, columnas que son numéricas y la información viene en formato string, puede ser que un mismo atributo este medido en la misma magnitud pero en diferentes unidades como por ejemplo nanómetros y micrómetros...

El objetivo de esta fase es solucionar todos estos problemas que nos vamos encontrando, de tal forma que los datos tengan la mayor calidad posible, para que así el modelo funcione correctamente.

Dentro de esta fase podemos realizar la limpieza de los datos, es decir, eliminar aquellos datos que son innecesarios o redundantes. En nuestro caso, podría ser que hay registros de pacientes duplicados, o que hay registros en los que el valor del grosor de la columna de la cécula está indefinido...

Por otro lado, podemos realizar la transformación de algunos datos, en nuestro caso, los datos numéricos pueden venir en formato string, y realmente si vamos a trabajar con ellos es mejor que sean numéricos. Además, podemos tener datos que estén medidos en diferentes escalas y si vamos a usar una red neuronal para realizar el modelo de clasificación, vamos a tener que normalizar los valores en el intervalo entre 0 y 1. También tenemos que etiquetar cada registro, es decir, indicar si un paciente con X datos tiene un cáncer de mama benigno o maligno, de tal forma que luego nos permita clasificar bien y poder predecir...

Finalmente, puede ser que tengamos demasiada información, tanto a nivel de registros como a nivel de atributos, esto supone un gran coste de computación. Por lo tanto, tenemos que reducir la dimensionalidad del dataset, para que con menos datos obtengamos el mismo resultado o lo más parecido posible.

A la hora de seleccionar menos registros, hay que tener cuidado ya que puede quedar una población muy sesgada, hay que ser preciso para poder representar la misma información con menos registros. Por otro lado, a la hora de reducir el número de atributos es lo que se denomina selección de variables, podemos hacer uso también del análisis de componentes princiaples para ver que componentes representan la mayor varianza posible con menos dimensiones.


### La contrucción de modelos
Una vez que ya tenemos el dataset con la mayor calidad posible, podemos construir el modelo para resolver el problema que nos hemos planteado. El objetivo de esta fase es seleccionar el modelo que mejor se adapte a nuestros datos, es decir dentro de nuestro caso, en la clasifcación podemos hacer uso de: árboles de decisión, redes neuornales y reglas de clasificación. Tenemos que elegir el modelo que mejor creemos que va a funcionar, esto no significa que no podamos hacer uso de los demás, de hecho deberíamos para saber cuál es el modelo que ofrece los mejores resultados para nuestro problema.

Respecto a nuestro caso, tenemos que tratar de minimizar el error, ya que si no conseguimos clasificar muy bien puede ser un modelo muy desastroso, los falsos negativos son un peligro, ya que podríamos indicar a una persona que no tiene cáncer de mama cuando realmente sí que lo tiene, por lo tanto tenemos que minimizar ese porcentaje.


### Evaluación e interpretación del modelo
Cuando ya hemos construido el modelo, debemos preguntarnos si ese modelo es el mejor, es decir, si tiene la suficiente calidad, si tenemos que volver a empezar, si tenemos que elegir otro modelo...

El objetivo de esta fase es evaluar el modelo, para ello se suele utilizar dos conjuntos de datos a partir de un conjunto incial, un conjunto se va a usar para entrenar el modelo y el segundo para comprobar que el modelo realmente funciona y presenta la suficiente calidad.

En nuestro caso, podríamos dividir el dataset inicial en tres subconjuntos, uno para entrenar el modelo, otro para validar el modelo y un tercelo para para evaluar el modelo.

Una vez que el modelo se ha evaluado, tenemos que realizar una interpretación de los resultados que hemos obtenido, y en este punto hay que tener mucho cuidado porque podemos estar considerando unos resultados que no son lo suficientemente buenos pero realmente nos están proporcionando mucha información y viceversa.


### Integración de los resultados
Esta fase es la final, cuando ya tenemos un modelo que cumple con los objetivos hay que integrar dicho modelo dentro de la cadena de la organización. En nuestro caso, una vez que tenemos el modelo de clasificación, tenemos que proporcionar esa herramienta a los médicos, de tal forma, que con un simple programa e introduciendo los datos de diferentes análisis le permita predecir si el paciente que ha venido a la consulta tiene cáncer de mama o no.

Por último, cabe destacar que esto es un proceso iterativo, es decir, si ya tenemos un modelo "perfecto" no significa que ya hemos ternimando nuestro trabajo, sino que hay que seguir mejorándolo y dar nuevas respustas a otros problemas. 


## Ejercicio 2:
A partir del juego de datos disponible en el siguiente enlace http://archive.ics.uci.edu/ml/datasets/adult , realiza las tareas previas a la generación de un modelo de minería de datos explicados en el Módulo 2. Puedes utilizar de referencia el ejemplo del Titánic.

Nota: Si lo deseas puedes utilizar otro conjunto de datos propio o de algún repositorio datos abiertos siempre que sea similar en diversidad de tipo de variables al propuesto.




```{r echo=TRUE, message=FALSE, warning=FALSE}
# Cargamos el juego de datos
df = read.table(file="./data/adult.data", fileEncoding="UTF-8", sep=",", na.strings = "NA")

# Cambiamos el nombre de las columnas tal y como se llaman según la página web
colnames(df) = c("age", "workclass", "fnlwgt", "education", "education-num", "marital-status", "occupation", "relationship", "race", "sex", "capital-gain", "capital-loss", "hours-per-week", "native-country", "salary")

# Creamos una copia del dataframe, ya que va a ser la copia sobre la que vamos a realizar las modificaciones
df_copy = df

# Visualizamos los datos
str(df_copy)

# https://rdrr.io/cran/arules/man/Adult.html
# https://lwmachinelearning.wordpress.com/portfolio/adults-data-set/
```

Vemos que tenemos 32561 registros y 15 variables que los caracterizan.

Revisamos la descripción de las variables y si los tipos de variable se corresponden al que hemos cargado:

**age**
    integer - edad de la persona.
    
**workclass**
    factor - clase de trabajo.
    
**fnlwgt**
    integer - peso final de la población que representa.
    
**education**
    ordered factor - nivel de educación.
    
**education-num**
    integer - número de nivel de educación.
    
**marital-status**
    factor - estado civil de la persona.
    
**occupation**
    factor - ocupación de la persona.
    
**relationship**
    factor - tipo de relación de la persona.
    
**race**
    factor - raza de la persona.
    
**sex**
    factor - sexo de la persona.
    
**capital-gain**
    integer - ganacias obtenidas.
    
**capital-loss**
    integer - pérdidas.
    
**hours-per-week**
    integer - media de horas trabajadas por semana.
    
**native-country**
    integer - país de origen.
    
**salary**
    ordered factor - nivel de ingresos.
    
Tal y como podemos observar, las variables numéricas sí que están bien definidas, sin embargo, las variables no numéricas las reconoce como chr en vez de factor, por lo que vamos a solucionar eso, pero antes como podemos comprobar en los atributos de tipo chr sus valores se guardan con un espacio delante, asi que hay que solucionar este problema antes:
```{r}
# Método que elimina los espacios en blanco del principio y del final
trim = function (x) gsub("^\\s+|\\s+$", "", x)

# Quitamos los espacios para cada columna de tipo factor
df_copy$workclass = trim(df_copy$workclass)
df_copy$education = trim(df_copy$education)
df_copy$`marital-status` = trim(df_copy$`marital-status`)
df_copy$occupation = trim(df_copy$occupation)
df_copy$relationship = trim(df_copy$relationship)
df_copy$race = trim(df_copy$race)
df_copy$sex = trim(df_copy$sex)
df_copy$`native-country` = trim(df_copy$`native-country`)
df_copy$salary = trim(df_copy$salary)

str(df_copy)
```

Una vez hemos eliminado los espacios de la variable de tipo character, las podemos convertir a factor:
```{r}
# Convertirmos las columna a factor
df_copy$workclass = as.factor(df_copy$workclass)
df_copy$education = factor(df_copy$education, ordered = TRUE, levels = c("Preschool", "1st-4th", "5th-6th", "7th-8th", "9th", "10th", "11th", "12th", "HS-grad", "Prof-school", "Assoc-acdm", "Assoc-voc", "Some-college", "Bachelors", "Masters", "Doctorate"))
df_copy$`marital-status` = as.factor(df_copy$`marital-status`)
df_copy$occupation = as.factor(df_copy$occupation)
df_copy$relationship = as.factor(df_copy$relationship)
df_copy$race = as.factor(df_copy$race)
df_copy$sex = as.factor(df_copy$sex)
df_copy$`native-country` = as.factor(df_copy$`native-country`)
df_copy$salary = factor(df_copy$salary, ordered = TRUE, levels = c("<=50K", ">50K"))

str(df_copy)
```

    

***
# Criterios de evaluación
***

Ejercicio 1

Concepto y peso en la nota final

El objetivo del proyecto está correctamente definido con suficiente concreción y se puede resolver con técnicas de minería de datos. 15%

Las fases del ciclo de vida están correctamente expresadas. Los ejemplos son clarificadores. La justificación y argumentación de las decisiones que se han tomado. 20%

Ejercicio 2

Se carga la base de datos, se visualiza su estructura y se explica los hechos básicos que explican los datos. 5%

Se estudia si existen atributos vacíos o en diferentes escalas que haya que normalizar. Si es el caso se adoptan medidas para tratar estos atributos. Se construye una nueva variable útil a partir de las existentes. Se discretiza algún atributo. 20%

Se analizan los datos de forma visual y extraen conclusiones tangibles. Hay que elaborar un discurso coherente y con conclusiones claras. 30%

Se trata en profundidad algún otro aspecto respecto a los datos presentado en el Módulo 2 10%
