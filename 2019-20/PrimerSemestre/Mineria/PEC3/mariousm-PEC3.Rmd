

---
title: 'Minería de datos: PEC3 - Clasificación con árboles de decisión'
author: "Autor: Mario Ubierna San Mamés"
date: "Mayo 2021"
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
    includes:
      in_header: 75.584-PEC-header.html
  pdf_document:
    highlight: zenburn
    toc: yes
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval=T, echo=T)
```


******
# Introducción
******
## Presentación
Esta prueba de evaluación continua cubre los Módulos 3 (Clasificación: árboles de decisión) y el Módulo 8 (Evaluación de modelos) del programa de la asignatura.

## Competencias

* Uso y aplicación de las TIC en el ámbito académico y profesional.
* Capacidad para innovar y generar nuevas ideas.
* Capacidad para evaluar soluciones tecnológicas y elaborar propuestas de proyectos teniendo en cuenta los recursos, las alternativas disponibles y las condiciones de mercado.
* Conocer las tecnologías de comunicaciones actuales y emergentes así como saberlas aplicar convenientemente para diseñar y desarrollar soluciones basadas en sistemas y tecnologías de la información.
* Aplicación de las técnicas específicas de ingeniería del software en las diferentes etapas del ciclo de vida de un proyecto.
* Capacidad para aplicar las técnicas específicas de tratamiento, almacenamiento y administración de datos.
* Capacidad para proponer y evaluar diferentes alternativas tecnológicas para resolver un problema concreto.

## Objetivos
La correcta asimilación del Módulo 3. En esta PEC trabajaremos la generación e interpretación de un árbol de decisión con el software de prácticas. Seguiremos con la preparación de los datos y la extracción inicial de conocimiento.

## Descripción de la PEC a realizar
La prueba está estructurada en un total de un único ejercicio práctico.

## Recursos Básicos
**Material docente proporcionado por la UOC.** 

Módulo 3 y 8 del material didáctico.

Complementarios:

* Los descritos para la anterior PEC.
* Fichero titanic.csv.
* R package C5.0 (Decision Trees and Rule-Based Models): https://cran.r-project.org/web/packages/C50/index.html
* Fichero de "German Credit": credit.csv: https://www.kaggle.com/shravan3273/credit-approval


## Criterios de valoración

Todos los ejercicios deben ser presentados de forma razonada y clara, especificando todos y cada uno de los pasos que se hayan llevado a cabo para su resolución. No se aceptará ninguna respuesta que no está claramente justificada.

## Formato y fecha de entrega
El formato de entrega es: usernameestudiant-PECn.html/doc/docx/odt/pdf.
Se recomienda la entrega en formato html y también el Rmd que genera el html entregado.
Fecha de Entrega: 19/05/2021.
Se debe entregar la PEC en el buzón de entregas del aula.


## Nota: Propiedad intelectual 

> A menudo es inevitable, al producir una obra multimedia, hacer uso de recursos creados por terceras personas. Es por lo tanto comprensible hacerlo en el marco de una práctica de los estudios de Informática, Multimedia y Telecomunicación de la UOC, siempre y cuando esto se documente claramente y no suponga plagio en la práctica. 

> Por lo tanto, al presentar una práctica que haga uso de recursos ajenos, se debe presentar junto con ella un documento en que se detallen todos ellos, especificando el nombre de cada recurso, su autor, el lugar dónde se obtuvo y su estatus legal: si la obra está protegida por el copyright o se acoge a alguna otra licencia de uso (Creative Commons, licencia GNU, GPL ...). 
>El estudiante deberá asegurarse de que la licencia  no impide específicamente su uso en el marco de la práctica. En caso de no encontrar la información correspondiente tendrá que asumir que la obra está protegida por copyright. 

> Deberéis, además, adjuntar los ficheros originales cuando las obras utilizadas sean digitales, y su código fuente si corresponde.  

******
# Enunciado  
******

En este ejercicio vamos a seguir los pasos del ciclo de vida de un proyecto de minería de datos, para el caso de un algoritmo de clasificación y más concretamente un árbol de decisión. Primero y a modo de ejemplo sencillo lo haremos con el archivo titanic.csv, que se encuentra adjunto en el aula. Este archivo contiene un registro por cada pasajero que viajaba en el Titanic. En las variables se caracteriza si era hombre o mujer, adulto o menor (niño), en qué categoría viajaba o si era miembro de la tripulación.
Se mostrará un ejemplo sencillo de solución con estos datos pero los alumnos deberéis responder a las preguntas de la rúbrica para otro conjunto: German Credit. Para este conjunto, tomaréis como referencia la variable "default" que indica el impago de créditos.

**Objetivos:**

*	Estudiar los datos, por ejemplo: ¿Número de registros del fichero? ¿Distribuciones de valores por variables? ¿Hay campos mal informados o vacíos?
*	Preparar los datos. En este caso ya están en el formato correcto y no es necesario discretizar ni generar atributos nuevos. Hay que elegir cuáles son las variables que se utilizarán para construir el modelo y cuál es la variable que clasifica. En este caso la variable por la que clasificaremos es el campo de si el pasajero sobrevivia o no.
*	Instalar, si es necesario, el paquete C5.0  Se trata de una implementación más moderna del algoritmo ID3 de Quinlan. Tiene los principios teóricos del ID3 más la poda automática. Con este paquete generar un modelo de minería.
*	¿Cuál es la calidad del modelo?
*	Generar el árbol gráfico.
* Generar y extraer las reglas del modelo.
*	En función del modelo, el árbol y las reglas: ¿Cuál es el conocimiento que obtenemos?
*	Probar el modelo generado presentándole nuevos registros. ¿Clasifica suficientemente bien?

A continuación, se plantean los puntos a realizar en la PEC 3 y, tomando como ejemplo el conjunto de datos de Titanic, se obtendrán, a modo de ejemplo, algunos resultados que pretender servir a  modo de inspiración para los estudiantes.
Los estudiantes deberán utilizar el conjunto de datos de "German Credit Data" que se pueden conseguir en este enlace: https://www.kaggle.com/shravan3273/credit-approval
  
Revisión de los datos, extracción visual de información y preparación de los datos

Carga de los datos:

```{r message= FALSE, warning=FALSE}
data<-read.csv("./data/titanic.csv",header=T,sep=",")
attach(data)
```

## Análisis inicial

Empezaremos haciendo un breve análisis de los datos ya que nos interesa tener una idea general de los datos que disponemos. Por ello, primero calcularemos las dimensiones de nuestra base de datos y analizaremos qué tipos de atributos tenemos.

Para empezar, calculamos las dimensiones de la base de datos mediante la función dim(). Obtenemos que disponemos de 2201 registros o pasajeros (filas) y 4 variables (columnas). 

```{r}
dim(data)
```

¿Cuáles son esas variables? Gracias a la función str() sabemos que las cuatro variables son categóricas o  discretas, es decir, toman valores en un conjunto finito. La variable CLASS hace referencia a la clase en la que viajaban los pasajeros (1ª, 2ª, 3ª o crew), AGE determina si era adulto o niño (Adulto o Menor), la variable SEX si era hombre o mujer (Hombre o Mujer) y la última variable (SURVIVED) informa si el pasajero murió o sobrevivió en el accidente (Muere o Sobrevive).

```{r}
str(data)
```

Es de gran interés saber si tenemos muchos valores nulos (campos vacíos) y la distribución de valores por variables. Es por ello recomendable empezar el análisis con una visión general de las variables. Mostraremos para cada atributo la cantidad de valores perdidos mediante la función summary.  

```{r}
summary(data)
```

Como parte de la preparación de los datos, miraremos si hay valores missing.

```{r}
missing <- data[is.na(data),]
dim(missing)
```
Observamos fácilmente que no hay valores missing y, por tanto, no deberemos preparar los datos en este sentido. En caso de haberlos, habría que tomar decisiones para tratar los datos adecuadamente.

Disponemos por tanto de un data frame formado por cuatro variables categóricas sin valores nulos. Para un conocimiento mayor sobre los datos, tenemos a nuestro alcance unas herramientas muy valiosas: las herramientas de visualización. Para dichas visualizaciones, haremos uso de los paquetes ggplot2, gridExtra y grid de R. 

```{r}
if(!require(ggplot2)){
    install.packages('ggplot2', repos='http://cran.us.r-project.org')
    library(ggplot2)
}
if(!require(ggpubr)){
    install.packages('ggpubr', repos='http://cran.us.r-project.org')
    library(ggpubr)
}
if(!require(grid)){
    install.packages('grid', repos='http://cran.us.r-project.org')
    library(grid)
}
if(!require(gridExtra)){
    install.packages('gridExtra', repos='http://cran.us.r-project.org')
    library(gridExtra)
}
if(!require(C50)){
    install.packages('C50', repos='http://cran.us.r-project.org')
    library(C50)
}

```

Siempre es importante analizar los datos que tenemos ya que las conclusiones dependerán de las características de la muestra.

```{r}
grid.newpage()
plotbyClass<-ggplot(data,aes(CLASS))+geom_bar() +labs(x="Class", y="Passengers")+ guides(fill=guide_legend(title=""))+ scale_fill_manual(values=c("blue","#008000"))+ggtitle("Class")
plotbyAge<-ggplot(data,aes(AGE))+geom_bar() +labs(x="Age", y="Passengers")+ guides(fill=guide_legend(title=""))+ scale_fill_manual(values=c("blue","#008000"))+ggtitle("Age")
plotbySex<-ggplot(data,aes(SEX))+geom_bar() +labs(x="Sex", y="Passengers")+ guides(fill=guide_legend(title=""))+ scale_fill_manual(values=c("blue","#008000"))+ggtitle("Sex")
plotbySurvived<-ggplot(data,aes(SURVIVED))+geom_bar() +labs(x="Survived", y="Passengers")+ guides(fill=guide_legend(title=""))+ scale_fill_manual(values=c("blue","#008000"))+ggtitle("SURVIVED")
grid.arrange(plotbyClass,plotbyAge,plotbySex,plotbySurvived,ncol=2)

```
Claramente vemos cómo es la muestra analizando la distribución de las variables disponibles. De cara a los informes, es mucho más interesante esta información que la obtenida en summary, que se puede usar para complementar.


Nos interesa describir la relación entre la supervivencia y cada uno de las variables mencionadas anteriormente. Para ello, por un lado graficaremos mediante diagramas de barras la cantidad de muertos y supervivientes según la clase en la que viajaban, la edad o el sexo. Por otro lado, para obtener los datos que estamos graficando utilizaremos el comando table para dos variables que nos proporciona una tabla de contingencia.

```{r}
grid.newpage()
plotbyClass<-ggplot(data,aes(CLASS,fill=SURVIVED))+geom_bar() +labs(x="Class", y="Passengers")+ guides(fill=guide_legend(title=""))+ scale_fill_manual(values=c("black","#008000"))+ggtitle("Survived by Class")
plotbyAge<-ggplot(data,aes(AGE,fill=SURVIVED))+geom_bar() +labs(x="Age", y="Passengers")+ guides(fill=guide_legend(title=""))+ scale_fill_manual(values=c("black","#008000"))+ggtitle("Survived by Age")
plotbySex<-ggplot(data,aes(SEX,fill=SURVIVED))+geom_bar() +labs(x="Sex", y="Passengers")+ guides(fill=guide_legend(title=""))+ scale_fill_manual(values=c("black","#008000"))+ggtitle("Survived by Sex")
grid.arrange(plotbyClass,plotbyAge,plotbySex,ncol=2)

```

De estos gráficos obtenemos información muy valiosa que complementamos con las tablas de contingencia (listadas abajo). Por un lado, la cantidad de pasajeros que sobrevivieron es similar en hombres y mujeres (hombres: 367 y mujeres 344). No, en cambio, si tenemos en cuenta el porcentaje respecto a su sexo. Es decir, pese a que la cantidad de mujeres y hombres que sobrevivieron es pareja, viajaban más hombres que mujeres (470 mujeres y 1731 hombres), por lo tanto, la tasa de muerte en hombres es muchísimo mayor (el 78,79% de los hombres murieron mientras que en mujeres ese porcentaje baja a 26,8%). 

En cuanto a la clase en la que viajaban, los pasajeros que viajaban en primera clase fueron los únicos que el porcentaje de supervivencia era mayor que el de mortalidad. El 62,46% de los viajeros de primera clase sobrevivió, el 41,4% de los que viajaban en segunda clase mientras que de los viajeros de tercera y de la tripulación solo sobrevivieron un 25,21% y 23,95% respectivamente. Para finalizar, destacamos que la presencia de pasajeros adultos era mucho mayor que la de los niños (2092 frente a 109) y que la tasa de supervivencia en niños fue mucho mayor (52,29% frente a 31,26%), no podemos obviar, en cambio, que los únicos niños que murieron fueron todos pasajeros de tercera clase (52 niños). 

```{r}
tabla_SST <- table(SEX, SURVIVED)
tabla_SST
prop.table(tabla_SST, margin = 1)
```

```{r}
tabla_SCT <- table(CLASS,SURVIVED)
tabla_SCT
prop.table(tabla_SCT, margin = 1)
```

```{r}
tabla_SAT <- table(AGE,SURVIVED)
tabla_SAT
prop.table(tabla_SAT, margin = 1) 
```

```{r}
tabla_SAT.byClass <- table(AGE,SURVIVED,CLASS)
tabla_SAT.byClass
```

Los resultados anteriores muestran los datos de forma descriptiva, podemos añadir algún test estadístico para validar el grado de significancia de la relación. La librería "DescTools nos permite instalarlo fácilmente"


```{r}
if(!require(DescTools)){
    install.packages('DescTools', repos='http://cran.us.r-project.org')
    library(DescTools)
}
```
```{r}
Phi(tabla_SST) 
CramerV(tabla_SST) 
```
```{r}
Phi(tabla_SAT) 
CramerV(tabla_SAT) 
```

```{r}
Phi(tabla_SCT) 
CramerV(tabla_SCT) 
```
En todas las combinaciones analizadas, no se encuentra una relación estadística significativa.

Una alternativa interesante a las barras de diagramas, es el plot de las tablas de contingencia. Obtenemos la misma información pero para algunos receptores puede resultar más visual.  

```{r}
par(mfrow=c(2,2))
plot(tabla_SCT, col = c("black","#008000"), main = "SURVIVED vs. CLASS")
plot(tabla_SAT, col = c("black","#008000"), main = "SURVIVED vs. AGE")
plot(tabla_SST, col = c("black","#008000"), main = "SURVIVED vs. SEX")
```

Nuestro objetivo es crear un árbol de decisión que permita analizar qué tipo de pasajero del Titanic tenía probabilidades de sobrevivir o no. Por lo tanto, la variable por la que clasificaremos es el campo de si el pasajero sobrevivió o no. De todas maneras, al imprimir las primeras (con head) y últimas 10 (con tail) filas nos damos cuenta de que los datos están ordenados.

```{r}
head(data,10)
tail(data,10)
```

Nos interesa "desordenarlos". Guardaremos los datos con el nuevo nombre como "data_random".

```{r}
set.seed(1)
data_random <- data[sample(nrow(data)),]
```

## Preparación de los datos para el modelo

Para la futura evaluación del árbol de decisión, es necesario dividir el conjunto de datos en un conjunto de entrenamiento y un conjunto de prueba. El conjunto de entrenamiento es el subconjunto del conjunto original de datos utilizado para construir un primer modelo; y el conjunto de prueba, el subconjunto del conjunto original de datos utilizado para evaluar la calidad del modelo. 

Lo más correcto será utilizar un conjunto de datos diferente del que utilizamos para construir el árbol, es decir, un conjunto diferente del de entrenamiento. No hay ninguna proporción fijada con respecto al número relativo de componentes de cada subconjunto, pero la más utilizada acostumbra a ser 2/3 para el conjunto de entrenamiento y 1/3, para el conjunto de prueba. 

La variable por la que clasificaremos es el campo de si el pasajero sobrevivió o no, que está en la cuarta columna. De esta forma, tendremos un conjunto de datos para el entrenamiento y uno para la validación

```{r}
set.seed(666)
y <- data_random[,4] 
X <- data_random[,1:3] 
```


De forma dinámica podemos definir una forma de separar los datos en función de un parámetro, en este caso del "split_prop".
Definimos un parámetro que controla el split de forma dinámica en el test.

```{r}
split_prop <- 3 
max_split<-floor(nrow(X)/split_prop)
tr_limit <- nrow(X)-max_split
ts_limit <- nrow(X)-max_split+1

trainX <- X[1:tr_limit,]
trainy <- y[1:tr_limit]
testX <- X[ts_limit+1:nrow(X),]
testy <- y[ts_limit+1:nrow(X)]
```

En la segunda opción podemos crear directamente un rango utilizando el mismo parámetro anterior.

```{r}
split_prop <- 3 
indexes = sample(1:nrow(data), size=floor(((split_prop-1)/split_prop)*nrow(data)))
trainX<-X[indexes,]
trainy<-y[indexes]
testX<-X[-indexes,]
testy<-y[-indexes]
```

Después de una extracción aleatoria de casos es altamente recomendable efectuar un análisis de datos mínimo para asegurarnos de no obtener clasificadores sesgados por los valores que contiene cada muestra. En este caso, verificaremos que la proporción del supervivientes es más o menos constante en los dos conjuntos:

```{r}
summary(trainX);
summary(trainy)
summary(testX)
summary(testy)
```
Verificamos fácilmente que no hay diferencias graves que puedan sesgar las conclusiones.

## Creación del modelo, calidad del modelo y extracción de reglas

Se crea el árbol de decisión usando los datos de entrenamiento (no hay que olvidar que la variable outcome es de tipo factor):

```{r}
trainy = as.factor(trainy)
model <- C50::C5.0(trainX, trainy,rules=TRUE )
summary(model)
```

Errors muestra el número y porcentaje de casos mal clasificados en el subconjunto de entrenamiento. El árbol obtenido clasifica erróneamente 317 de los 1467 casos dados, una tasa de error del 21.6%.

A partir del árbol de decisión de dos hojas que hemos modelado, se pueden extraer las siguientes reglas de decisión (gracias a rules=TRUE podemos imprimir las reglas directamente):

SEX = "Hombre" → Muere. Validez: 78,1%

CLASS "1ª", "2ª" y AGE = "Menor" → Sobrevive. Validez: 95,5%

SEX = "Mujer" → Sobrevive. Validez: 74,7%

Por tanto, podemos concluir que el conocimiento extraído y cruzado con el análisis visual se resume en "las mujeres y los niños primero a excepción de que fueras de 3ª clase".

A continuación, mostramos el árbol obtenido.

```{r}
model <- C50::C5.0(trainX, trainy)
plot(model)
```


## Validación del modelo con los datos reservados
Una vez tenemos el modelo, podemos comprobar su calidad prediciendo la clase para los datos de prueba que nos hemos reservado al principio. 

```{r}
predicted_model <- predict( model, testX, type="class" )
print(sprintf("La precisión del árbol es: %.4f %%",100*sum(predicted_model == testy) / length(predicted_model)))
```

Cuando hay pocas clases, la calidad de la predicción se puede analizar mediante una matriz de confusión que identifica los tipos de errores cometidos. 

```{r}
mat_conf<-table(testy,Predicted=predicted_model)
mat_conf
```

Otra manera de calcular el porcentaje de registros correctamente clasificados usando la matriz de confusión:

```{r}

porcentaje_correct<-100 * sum(diag(mat_conf)) / sum(mat_conf)
print(sprintf("El %% de registros correctamente clasificados es: %.4f %%",porcentaje_correct))

```

Además, tenemos a nuestra disposición el paquete gmodels para obtener información más completa:

```{r}
if(!require(gmodels)){
    install.packages('gmodels', repos='http://cran.us.r-project.org')
    library(gmodels)
}
```

```{r}
CrossTable(testy, predicted_model,prop.chisq  = FALSE, prop.c = FALSE, prop.r =FALSE,dnn = c('Reality', 'Prediction'))
```

## Prueba con una variación u otro enfoque algorítmico

En este apartado buscaremos probar con las variaciones que nos ofrece el paquete C5.0 para analizar cómo afectan a la creación de los árboles generados. Existen muchas posibles variaciones con otras funciones que podéis investigar. La idea es seguir con el enfoque de árboles de decisión explorando posibles opciones.
Una vez tengamos un método alternativo, debemos analizar cómo se modifica el árbol y cómo afecta a la capacidad predictiva en el conjunto de test.

A continuación, utilizamos otro enfoque para comparar los resultados: incorpora como novedad "adaptative boosting", basado en el trabajo Rob Schapire and Yoav Freund (1999). La idea de esta técnica es generar varios clasificadores, con sus correspondientes arboles de decisión y su ser de reglas. Cuando un nuevo caso va a ser clasificado, cada clasificador vota cual es la clase predicha. Los votos son sumados y determina la clase final.

```{r}
modelo2 <- C50::C5.0(trainX, trainy, trials = 10)
plot(modelo2)
```

En este caso, dada la simplicidad del conjunto de ejemplo, no se aprecian diferencias, pero aparecerán en datos de mayor complejidad y modificando el parámetro "trials" se puede intentar mejorar los resultados.

Vemos a continuación cómo son las predicciones del nuevo árbol:

```{r}
predicted_model2 <- predict( modelo2, testX, type="class" )
print(sprintf("La precisión del árbol es: %.4f %%",100*sum(predicted_model2 == testy) / length(predicted_model2)))
```
Observamos como se modifica levemente la precisión del modelo a mejor.

```{r}
mat_conf<-table(testy,Predicted=predicted_model2)
mat_conf
```

Otra manera de calcular el porcentaje de registros correctamente clasificados usando la matriz de confusión:

```{r}

porcentaje_correct<-100 * sum(diag(mat_conf)) / sum(mat_conf)
print(sprintf("El %% de registros correctamente clasificados es: %.4f %%",porcentaje_correct))

```

El algoritmo C5.0 incorpora algunas opciones para ver la importancia de las variables (ver documentación para los detalles entre los dos métodos):

```{r}
importancia_usage <- C50::C5imp(modelo2, metric = "usage")
importancia_splits <- C50::C5imp(modelo2, metric = "splits")
importancia_usage
importancia_splits
```
Curiosamente y aunque el conjunto de datos es muy sencillo, se aprecian diferencias en los métodos de importancia de las variables. Se recomienda en vuestro ejercicio mejorar la visualización de los resultados con la función ggplo2 o similar.

# Solución del ejercicio

Los alumnos deben completar aquí los siguientes puntos que hacen referencia a los apartados de la rúbrica.

## Estudio de los datos de entrada

Antes de comenzar con la práctica en sí, tenemos que hacer la lectura del fichero sobre el que vamos a hacer el estudio:
```{r}
df = read.csv(file = "./data/credit_kaggle.csv", header = TRUE, sep = ",", stringsAsFactors = TRUE)

# Creamos el dataframe de copia sobre el que vamos a trabajar
df_copy = df

head(df_copy)
```

A continuación observamos el número de registros y atributos que tiene nuestro dataset:
```{r}
str(df_copy)
```

Observamos que tenemos 1000 observaciones y 21 variables, dentro de estas variables pueden ser de dos tipos o categóricas o numéricas.

Puesto que no viene por internet nada de información sobre este dataset de lo que significa cada variable y los valores que pueden tomar y qué significan estos, hemos decidido seleccionar aquellas que realmente sabemos su significado y que a priori son interestantes para ver si el conceder un préstamo a una persona genera un riesgo a la entidad bancaria  o no.

Aunque más adelante analizaremos las correlaciones del dataset, vamos a ver tres posibles variables numéricas que podemos eliminar si no están correlacionadas con default:

```{r}
library(corrplot)
corrplot(cor(df_copy[, c("installment_rate", "residence_history", "dependents", "default")]), type = "upper", 
         order = "original", 
         tl.col = "black", 
         tl.srt = 45)
```

De la anterior ejecución observamos, que dichas variables no están correlacionadas entre sí ni con default, por lo que procedemos a eliminar estas variables del dataset ya que no nos aportan mucha información.

Por otro lado, la variable installment numérica y su variable categórica a parte de que no sabemos realmente lo que significan hemos visto que no aportan mucha información por lo que las vamos a eliminar también. Además, la variable proporty tampoco nos proporciona mucha información respecto al análisis que vamos a realizar, es decir, el saber si la propiedad es desconocida, o del estado no nos aporta mucha información, ya que housing nos está indicando si el cliente tiene propiedad o no y de qué tipo.

Por lo tanto, una vez hecho este pequeño análisis seleccionamos las variables más interesantes, las cuales son las siguientes:
```{r}
columns = c("checking_balance", "months_loan_duration", "credit_history", "purpose", "amount", "savings_balance", "employment_length", "personal_status", "age", "housing", "existing_credits", "foreign_worker", "job", "default")

df_copy = df_copy[, columns]

str(df_copy)
```

Ahora vemos que hemos reducido la dimensionalidad del dataset de 21 variables a 14, pero seguimos manteniendo el mismo número de registros.

El significado de las variables que hemos seleccionado es el siguiente:

**checking_balance**
    Factor - Balance de gastos, desconocemos el significado de DM.
    
**months_loan_duration**
    Integer - Duración del préstamos en meses.
   
**credit_history**
    Factor - Estado del préstamo, si es crítico, está completamente pagado, una parte pagada... 
    
**purpose**
    Factor - Propósito del préstamo.
    
**amount**
    Integer - Cantidad del préstamo.
    
**savings_balance**
    Factor - Balance de ahorros, desconocemos el significado de DM.
    
**employment_length**
    Factor - Años que lleva en el empleo el cliente.
    
**personal_status**
    Factor - Estado civil.
    
**age**
    Integer - Edad del cliente.
    
**housing**
    Factor - Tipo de vivienda.
    
**existing_credits**
    Integer - Número de créditos del cliente.
    
**foreign_worker**
    Factor - Trabajor extranjero.
    
**job**
    Factor - Tipo de trabajo del cliente.
    
**default**
    Integer - Variable objetivo, nos indica si el préstamo no es de riesgo 1, o si es de riesgo 2.
    
Una vez que tenemos clara la información que compone al dataset, vamos a realizar un pequeño análisis de sus datos:
```{r}
summary(df_copy)
```

De la anterior ejecución podemos obtener la siguiente información:

- Respecto a checking_balance, los clientes suelen tener un balance de gastos repartidos, es decir, hay bastantes regisros en los que el balance es negativo, o si es positivo es inferior a 200 DM.

- Respecto a la duración del préstamo, por norma general los préstamos tienen una duración entre 18 y 21 meses, siendo el mayor préstamos de 72 meses y el menor de 4 meses.

- Respecto a credit_history, la gran mayaría de préstamos están en una situación normal, es decir, todavía nse sigue pagando  el préstamo pero no se producen retrasos. Sin embargo, hay una gran cantidad de préstamos en estado crítico.

- Respecto al propósito del préstamo, podríamos decir que por norma general se usan con fines de radio/televisión, y para comprar un coche nuevo.

- La cantidad que se suele solicitar tiene un valor de media de 3271, siendo el máixmo 18424 y su mediana 2320 podemos ver, que este banco suele conceder préstamos de pequeñas cantidades.

- El balance de ahorros se mueve principalmente en un valor inferior a 100 DM.

- Respecto al tiempo trabajando podemos observar que la gran mayoría o llevan relativamente poco (4 - 7 años) o relativamente mucho (> 7 años).

- En cuanto al estado civil, vemos que la gran mayoría de los préstamos lo solicitan hombres, para ser más exactos por cada préstamo de mujer hay dos de hombres, siendo el hombre soltero el que más préstamos realiza.

- Respecto a la edad, los clientes suelen ser mayoritariamente de mediana edad entre unos 33 y 35 años.

- El tipo de vivienda es llamativo, ya que los clientes que suelen pedir el préstamo tienden a tener una casa en propiedad, es decir, no viven tanto de alquiler como de viviendas de protección oficial.

- Respecto a los préstamos que suelen tener los clientes, la gran mayoría solo tiene un préstamos, aunque hay alguno que tiene más de uno, incluso hasta 4 préstamos.

- También observamos que la gran mayoría que solicita los préstamos en este banco son gente extranjera, es decir, gente que no es del país en el que se recogió los datos.

- En cuanto al tipo de trabajo, los clientes suelen estar especializados en su campo y no son autónomos.

- Finalmente default nos dictamina que la gran mayoría de préstamos son de no riesgo, ya que el valor de la media está próximo a uno y el de la mediana es 1. 1 significa que el préstamo no es de riesgo y 2 que sí que lo es.

Para finalizar con este apartado antes de realizar el siguiente, vamos a ver si tenemos datos perdidos, es decir, registros que no contegan datos:
```{r}
colSums(is.na(df_copy))
```

Tal y como podemos ver no tenenemos valores perdidos, lo que significa que todos los datos están completos y podemos pasar a realizar  el análisis descriptivo y de correlaciones.

## Análisis descriptivo y de correlaciones

Una vez que hemos hecho un pequeño análisis sobre los datos que tenemos con el fin de saber qué campos hay, cuales son sus valores, si hay valores perdidos... Lo siguiente es hacer un análisis en profundiad de los mismos.

En este apartado lo vamos a dividir en tres puntos: el análisis de las distribuciones de cada variable, el cómo afecta cada variable respecto al riesgo de conceder un préstamo, y por último las correlaciones entre las variables numéricas.

### Distribuciones

Lo primero que debemos de hacer es analizar las distribuciones de cada variable, para saber si sigue una distribución normal, si hay valores atípicos...

La primera variable a analizar es checking_balance o el balance de gastos:
```{r}
library (ggplot2)

ggplot(data=df_copy,aes(x=checking_balance)) +
  geom_bar(fill = "#FF6666") + 
  ylab("Frecuencia") +
  xlab("Checking_balance") +
  ggtitle("Frecuencia según el checking_balance")
```

Observamos que la gran mayoría de balances de gastos con la excepción de los desconocidos, o el balance es < 0 DM o está entre 1 y 200 DM. Cabe destacar que los valores desconocidos podríamos sustituirlos por los más frecuentes o usar otra medida, pero si hacemos eso vamos a sesgar la información real que tenemos del dastaset, es por ello que hemos decidido mantener dicho nivel.

La siguiente variable a analizar es months_loan_duration:
```{r}
ggplot(data=df_copy,aes(x=months_loan_duration)) +
  geom_bar(fill = "#FF6666") + 
  ylab("Frecuencia") +
  xlab("Months_loan_duration") +
  ggtitle("Frecuencia según el months_loan_duration")
```

Respecto a esta variable vemos que no sigue una distribución normal, y que lo valores más comunes se encuentran entre 15 y 23 meses, es decir, por norma general la duración de los préstamos es pequeña año y medio / dos años.

La siguiente variable a analizar es credit_history:
```{r}
ggplot(data=df_copy,aes(x=credit_history)) +
  geom_bar(fill = "#FF6666") + 
  ylab("Frecuencia") +
  xlab("Credit_history") +
  ggtitle("Frecuencia según el credit_history")
```

Esta variable es curiosa ya que vemos que la gran mayoría de casos son préstamos repagados, es decir, que no se han terminado de pagar pero se sigue cumpliendo con los pagos, por otro lado, vemos que no hay muchos préstamos retrasados pero sí críticos, por lo que es interesante analizar que en esta entidad bancaria algo menos de un tercio de los datos son préstamos críticos.

Continuamos analizando la distribución de la variable purpose:
```{r}
ggplot(data=df_copy,aes(x=purpose)) +
  geom_bar(fill = "#FF6666") + 
  ylab("Frecuencia") +
  xlab("Purpose") +
  ggtitle("Frecuencia según el purpose")
```

Vemos que en esta entidad bancaria, la gran mayoría de los préstamos son cubiertos para radio/tv y para comprarse un coche nuevo. Aunque también es llamativo los préstamos para comprar un coche de segunda mano y para invertir en un negocio. Los demás propósitos del préstamo son prácticamente insignificantes.

La siguiente variable a analizar es amount o la cantidad del préstamo:
```{r}
ggplot(data=df_copy,aes(x=amount)) +
  geom_bar(fill = "#FF6666") + 
  ylab("Frecuencia") +
  xlab("Amount") +
  ggtitle("Frecuencia según el amount")
```

Aunque la anterior gráfica no la hemos discretizado, es decir, podríamos crear diferentes categorías para ver mejor el gráfico, mantenemos esta gráfica ya que queda muy claro a nivel visual que prácticamente la mayoría de las cantidades para un préstamo se mueven en el intervalo 250 dólares a 5000 dólares, es decir, la cantidad que se suele conceder en este banco es baja, ya sea por el uso que se va a dar al préstamo, porque esta entidad tiene criterios más exigentes... Aún así vemos uqe no sigue una distribución normal, además no podemos considerar que las cantidades grandes sean anómalas, ya que éstas pueden ser totalmente lícitas.

La siguiente variable a analizar es savings_balance o el balance de los ahorros:
```{r}
ggplot(data=df_copy,aes(x=savings_balance)) +
  geom_bar(fill = "#FF6666") + 
  ylab("Frecuencia") +
  xlab("Savings_balance") +
  ggtitle("Frecuencia según el savings_balance")
```

Del anterior gráfico observamos que la gran mayoría de los clientes tienen un balance de ahorros bajo, además aunque el porcentaje de desconocidos no sea muy elevado sigue siendo significante respecto a los otros balances.

También podemos analizar los años trabajados por cada cliente:
```{r}
ggplot(data=df_copy,aes(x=employment_length)) +
  geom_bar(fill = "#FF6666") + 
  ylab("Frecuencia") +
  xlab("Employment_length") +
  ggtitle("Frecuencia según el employment_length")
```

Vemos que los datos de esta variable están más repartidos, por norma general tener una experiencia baja o muy grande es lo normal, aunque el resto de niveles son también significativos.

La siguiente variable a analizar es personal_status:
```{r}
ggplot(data=df_copy,aes(x=personal_status)) +
  geom_bar(fill = "#FF6666") + 
  ylab("Frecuencia") +
  xlab("Personal_status") +
  ggtitle("Frecuencia según el personal_status")
```

Es curioso ver que la gran mayoría de veces que se pide un préstamo es por un hombrem y éste suele estar soltero. Respecto a las mujeres, el dataset no determina si el nivel female hace referencia a todas las mujeres (divorciadas, casadas o solteras) pero vamos a considerar que es así, que dicho nivel representa todos los casos al igual que en los hombres.

En cuanto a la variable age:
```{r}
ggplot(data=df_copy,aes(x=age)) +
  geom_bar(fill = "#FF6666") + 
  ylab("Frecuencia") +
  xlab("Age") +
  ggtitle("Frecuencia según el age")
```

Vemos que no sigue una distribución normal, sin embargo, la gran mayoría de gente que pide un préstamo se encuentra entre los 25 a los 35, esto tiene toda la lógica del mundo ya que es aquí donde uno se independiza, se compra su coche, su casa...

Podemos analizar también la variable housing:
```{r}
ggplot(data=df_copy,aes(x=housing)) +
  geom_bar(fill = "#FF6666") + 
  ylab("Frecuencia") +
  xlab("Housing") +
  ggtitle("Frecuencia según el housing")
```

Este gráfico es significate, ya que vemos que en la gran mayoría de los casos los cliente tienen su propia casa, es decir, no viven ni de alquier ni en viviendas del estado.

Podemos analizar también los créditos que tiene cada cliente:
```{r}
ggplot(data=df_copy,aes(x=existing_credits)) +
  geom_bar(fill = "#FF6666") + 
  ylab("Frecuencia") +
  xlab("Existing_credits") +
  ggtitle("Frecuencia según el existing_credits")
```

Vemos que por norma general los clientes tienen uno o como mucho dos créditos, ya que hay muy poca gente con 3 y 4 créditos al mismo tiempo.

Podemos analizar si el tipo de cliente que solicita el préstamo es extranjero o no:
```{r}
ggplot(data=df_copy,aes(x=foreign_worker)) +
  geom_bar(fill = "#FF6666") + 
  ylab("Frecuencia") +
  xlab("Foreign_worker") +
  ggtitle("Frecuencia según el foreign_worker")
```

Y vemos que sí, que esta entidad bancaria tiene un público mayor sobre trabajadores extranjeros, realmente casi todos son inmigrantes.

También podemos analizar qué tipo de trabajo tienen los clientes:
```{r}
ggplot(data=df_copy,aes(x=job)) +
  geom_bar(fill = "#FF6666") + 
  ylab("Frecuencia") +
  xlab("Job") +
  ggtitle("Frecuencia según el job")
```

Vemos que la gran mayoría está especializada en su trabajo y que hay muy pocos autónomos. Además, apenas existe el cliente que no sea residente en el país y esté desempleado.

Finalmente, vamos a analizar la variable objetivo default, ésta nos indica el riesgo al conceder un préstamo o no, siendo 1 que no hay riesgo y 2 todo lo contrario:
```{r}
ggplot(data=df_copy,aes(x=default)) +
  geom_bar(fill = "#FF6666") + 
  ylab("Frecuencia") +
  xlab("Default") +
  ggtitle("Frecuencia según el default")
```

Tal y como podemos ver la gran mayoría de casos son préstamos que no hay riesgo, es decir, dos tercios de los datos son datos en lo que no tenemos riesgo al conceder el préstamo, sin embargo, hay un tercio en el que sí, por lo tanto vamos a analizar cuán de probable es que un préstamo sea de riesgo dependiendo de cada una de las variables independientes.

### Análisis del riesgo del préstamo respecto a las variables independientes

En este apartado vamos a realizar el estudio del riesgo de conceder un préstamo según cada una de las variables independiente que tenemos, para ello también analizaremos las diferentes tablas de contigencias para que nos proporcionen información más precisa.

Antes de realizar el análisis vamos a dicretizar tres variables: months_loan_duration, amount y age, ya que éstas las vamos a usar ahora de otra forma y queda mejor visualmente ver estas variables dicretizas que no.
```{r}
# Discretizamos la duración del préstamos
df_copy["d-months_loan_duration"] = ordered(cut(df_copy[[ "months_loan_duration"]], breaks = c(0,12,36,72), labels = c("Corto", "Medio", "Largo")))

# Discretizamos la cantidad del préstamo
df_copy["d-amount"] = ordered(cut(df_copy[[ "amount"]], breaks = c(0,2000,6000,19000), labels = c("Pequeña", "Mediana", "Grande")))

# Discretiazamos la edad
df_copy["d-age"] = ordered(cut(df_copy[[ "age"]], breaks = c(15,25,45,65,100), labels = c("Joven", "Mediana-edad", "Mayor", "Viejo")))

# Visualizamos los cambios
head(df_copy[, c("months_loan_duration", "d-months_loan_duration", "amount", "d-amount", "age", "d-age")])
```


Empezamos realizando el análisis sobre checking_balance:
```{r}
# Gráfico
ggplot(data = df_copy, aes(x=checking_balance,fill=as.factor(default))) + geom_bar(position = "fill") +
  xlab("Checking_balance") +
  ylab("Frecuencia") +
  scale_fill_manual(name = "Default", values=c("#00bfc4","#f8766d")) +
  ggtitle("Riesgo respecto a checking_balance")

# Tabla de contingencias
tabla_frec_abs <- table(df_copy$checking_balance, df_copy$default)
colnames(tabla_frec_abs) = c("No riesgo", "Sí riesgo")
tabla_frec_abs
prop.table(tabla_frec_abs, margin = 1)
```

De las dos anteriores ejecuciones vemos que respecto al balance, es prácticamente igual de probable que si tienes un balance de gastos negativo el riesgo de concer del préstamo sea igual, ya que no riesgo es un 51% y riesgo un 49%. Por otro lado, si tenemos un balance > 200 DM o entre 1-200 DM es más probable que no haya riesgo a que haya, con unos porcentajes de no riesgo 0.78 y 0.61 y de riesgo de 0.22 y 0.39 respectivamente.

Continuamos el análisis respecto a los meses que dura el préstamo:
```{r}
# Gráfico
ggplot(data = df_copy, aes(x=`d-months_loan_duration`,fill=as.factor(default))) + geom_bar(position = "fill") +
  xlab("d-months_loan_duration") +
  ylab("Frecuencia") +
  scale_fill_manual(name = "Default", values=c("#00bfc4","#f8766d")) +
  ggtitle("Riesgo respecto a d-months_loan_duration")

# Tabla de contingencias
tabla_frec_abs <- table(df_copy$`d-months_loan_duration`, df_copy$default)
colnames(tabla_frec_abs) = c("No riesgo", "Sí riesgo")
tabla_frec_abs
prop.table(tabla_frec_abs, margin = 1)
```

Observamos que a medida que el periodo aumenta el riesgo también, por lo que intuimos que estas dos variables están muy correlacionadas positivamente. Cabe destacar que cuando el periodo del préstamo es largo el riesgo de conceder el préstamo es mayor, para ser más exactos cuando el préstamos dura más tenemos un riesgo del 52%.

Realizamos el análisis según el credit_history:
```{r}
# Gráfico
ggplot(data = df_copy, aes(x=credit_history,fill=as.factor(default))) + geom_bar(position = "fill") +
  xlab("Credit_history") +
  ylab("Frecuencia") +
  scale_fill_manual(name = "Default", values=c("#00bfc4","#f8766d")) +
  ggtitle("Riesgo respecto a credit_history")

# Tabla de contingencias
tabla_frec_abs <- table(df_copy$credit_history, df_copy$default)
colnames(tabla_frec_abs) = c("No riesgo", "Sí riesgo")
tabla_frec_abs
prop.table(tabla_frec_abs, margin = 1)
```

Es curioso ver que cuando más riesgo tenemos de que no nos concedan un préstamos es cuando está completamente pagado, ya que el porcentaje de riesgo tanto en fully repaid como fully repaid this bank es de 63% y 57% respectivamente. En internet no encontramos nada sobre qué significa dichos niveles, ya que a apriori pensaba que si algo está completamente pagado es mejor, pero parace ser que no, que esto implica un mayor riesgo.

Analizamos el propósito del préstamo:
```{r}
# Gráfico
ggplot(data = df_copy, aes(x=purpose,fill=as.factor(default))) + geom_bar(position = "fill") +
  xlab("Purpose") +
  ylab("Frecuencia") +
  scale_fill_manual(name = "Default", values=c("#00bfc4","#f8766d")) +
  ggtitle("Riesgo respecto a purpose") + 
  theme(axis.text.x = element_text(angle = 45, vjust = 0.6))

# Tabla de contingencias
tabla_frec_abs <- table(df_copy$purpose, df_copy$default)
colnames(tabla_frec_abs) = c("No riesgo", "Sí riesgo")
tabla_frec_abs
prop.table(tabla_frec_abs, margin = 1) 
```

Vemos que por norma general no suele haber mucho riesgo dependiendo del propósito del préstamo, pero es verdad que si buscamos un préstamo para estudiar hay un riesgo un poco elevado de un 44%, es decir, tenemos 22 registros de este tipo de propósito que se consideran como riesgo.

Analizamos la cantidad del préstamo:
```{r}
# Gráfico
ggplot(data = df_copy, aes(x=`d-amount`,fill=as.factor(default))) + geom_bar(position = "fill") +
  xlab("d-amount") +
  ylab("Frecuencia") +
  scale_fill_manual(name = "Default", values=c("#00bfc4","#f8766d")) +
  ggtitle("Riesgo respecto a d-amount") + 
  theme(axis.text.x = element_text(angle = 45, vjust = 0.6))

# Tabla de contingencias
tabla_frec_abs <- table(df_copy$`d-amount`, df_copy$default)
colnames(tabla_frec_abs) = c("No riesgo", "Sí riesgo")
tabla_frec_abs
prop.table(tabla_frec_abs, margin = 1) 
```

En este caso vemos que a medida que aumenta la cantidad no implica que el riesgo sea mayor, ya que por ejemplo hay más riesgo cuando la cantidad es pequeña un 28% que cuando es mediana un 26%. Sin embago, si el préstamo solicitado es de cantidades elevadas el riesgo es mucho mayor, tanto que es casi un 46%.

Analizamos la variable savings_balance o balance de ahorros:
```{r}
# Gráfico
ggplot(data = df_copy, aes(x=savings_balance,fill=as.factor(default))) + geom_bar(position = "fill") +
  xlab("Savings_balance") +
  ylab("Frecuencia") +
  scale_fill_manual(name = "Default", values=c("#00bfc4","#f8766d")) +
  ggtitle("Riesgo respecto a savings_balance") + 
  theme(axis.text.x = element_text(angle = 45, vjust = 0.6))

# Tabla de contingencias
tabla_frec_abs <- table(df_copy$savings_balance, df_copy$default)
colnames(tabla_frec_abs) = c("No riesgo", "Sí riesgo")
tabla_frec_abs
prop.table(tabla_frec_abs, margin = 1) 
```

Otro de los condicionantes a que no nos concedan el préstamos es si tenemos pocos ahorros < 100 DM o si nuestros ahorros están entre 101 y 500 DM, en ambos casos tenemos un riesgo de 36% y 33% respectivamente.

Analizamos el tiempo que llevan trabajando los clientes y si esto supone un riesgo o no:
```{r}
# Gráfico
ggplot(data = df_copy, aes(x=employment_length,fill=as.factor(default))) + geom_bar(position = "fill") +
  xlab("Employment_length") +
  ylab("Frecuencia") +
  scale_fill_manual(name = "Default", values=c("#00bfc4","#f8766d")) +
  ggtitle("Riesgo respecto a employment_length") + 
  theme(axis.text.x = element_text(angle = 45, vjust = 0.6))

# Tabla de contingencias
tabla_frec_abs <- table(df_copy$employment_length, df_copy$default)
colnames(tabla_frec_abs) = c("No riesgo", "Sí riesgo")
tabla_frec_abs
prop.table(tabla_frec_abs, margin = 1) 
```

Vemos que a medida que aumentamos el tiempo que llevan trabajando los clientes, menos riesgo tenemos, esto lo que significa es que en mayor o menor medida estas dos variables estén correlacionadas negativamente, es decir, cuando aumenta el riesgo diminuye el tiempo y viceversa. Cabe destacar que entonces cuando tenemos de 0 a 1 años de experiencia obtenemos el mayor riesgo con un 41%.

Comprobamos si el estado civil influye en el riesgo:
```{r}
# Gráfico
ggplot(data = df_copy, aes(x=personal_status,fill=as.factor(default))) + geom_bar(position = "fill") +
  xlab("Personal_status") +
  ylab("Frecuencia") +
  scale_fill_manual(name = "Default", values=c("#00bfc4","#f8766d")) +
  ggtitle("Riesgo respecto a personal_status") + 
  theme(axis.text.x = element_text(angle = 45, vjust = 0.6))

# Tabla de contingencias
tabla_frec_abs <- table(df_copy$personal_status, df_copy$default)
colnames(tabla_frec_abs) = c("No riesgo", "Sí riesgo")
tabla_frec_abs
prop.table(tabla_frec_abs, margin = 1) 
```

Vemos que no hay una clara relación entre el estado civil y el riesgo ya que más o menos todos los estados civiles tienen un riesgo más o menos parecido, salvando la distancia con los hombre divorciados, ya que éstos son los que tienen mayor riesgo de que no se les conceda un préstamo con un 40%.

Analizamos la edad y si influye en el riesgo:
```{r}
# Gráfico
ggplot(data = df_copy, aes(x=`d-age`,fill=as.factor(default))) + geom_bar(position = "fill") +
  xlab("d-age") +
  ylab("Frecuencia") +
  scale_fill_manual(name = "Default", values=c("#00bfc4","#f8766d")) +
  ggtitle("Riesgo respecto a d-age")

# Tabla de contingencias
tabla_frec_abs <- table(df_copy$`d-age`, df_copy$default)
colnames(tabla_frec_abs) = c("No riesgo", "Sí riesgo")
tabla_frec_abs
prop.table(tabla_frec_abs, margin = 1) 
```

Vemos que como es lógico, cuanto más joven eres más riesgo tiene de que no te concedan un préstamo, a esas edad apenas tienen una situación estable económica. Cabe destascar que a medida que aumenta la edad el riesgo no diminuye, ya que aunque los mayores tienen menos riesgo sobre los de mediana-edad, son los viejos los que tienen un mayor riesgo con un 27%.

Analizamos si el tipo de vivienda afecta al riesgo:
```{r}
# Gráfico
ggplot(data = df_copy, aes(x=housing,fill=as.factor(default))) + geom_bar(position = "fill") +
  xlab("Housing") +
  ylab("Frecuencia") +
  scale_fill_manual(name = "Default", values=c("#00bfc4","#f8766d")) +
  ggtitle("Riesgo respecto a housing")

# Tabla de contingencias
tabla_frec_abs <- table(df_copy$housing, df_copy$default)
colnames(tabla_frec_abs) = c("No riesgo", "Sí riesgo")
tabla_frec_abs
prop.table(tabla_frec_abs, margin = 1) 
```

Tal y como podemos apreciar aquellos clientes con casa propia tienen un menor riesgo, con un valor del 47% de que no le concedan el préstamo, esto es lógico, cuando pedimos un préstamo dependiendo de la cantidad se solicita un aval, y la propia casa suele ser el aval, si vives alquilado o en una vivienda del estado no dispones de tu propio aval.

Vamos a analizar el riesgo según el número de créditos que tiene un cliente:
```{r}
# Gráfico
ggplot(data = df_copy, aes(x=existing_credits,fill=as.factor(default))) + geom_bar(position = "fill") +
  xlab("Existing_credits") +
  ylab("Frecuencia") +
  scale_fill_manual(name = "Default", values=c("#00bfc4","#f8766d")) +
  ggtitle("Riesgo respecto a existing_credits") + 
  theme(axis.text.x = element_text(angle = 45, vjust = 0.6))

# Tabla de contingencias
tabla_frec_abs <- table(df_copy$existing_credits, df_copy$default)
colnames(tabla_frec_abs) = c("No riesgo", "Sí riesgo")
tabla_frec_abs
prop.table(tabla_frec_abs, margin = 1) 
```

Vemos que no influye mucho el número de créditos respecto al riesgo de que no te concedan un préstamos, de hecho los riesgos son relativamente bajos, ya que solamente cuando tenemos 4 créditos nuestro riesgo es de un 33%, siendo éste el más alto.

Analizamos si el ser extranjero influye en la concesión de un préstamo:
```{r}
# Gráfico
ggplot(data = df_copy, aes(x=foreign_worker,fill=as.factor(default))) + geom_bar(position = "fill") +
  xlab("Foreign_worker") +
  ylab("Frecuencia") +
  scale_fill_manual(name = "Default", values=c("#00bfc4","#f8766d")) +
  ggtitle("Riesgo respecto a foreign_worker") + 
  theme(axis.text.x = element_text(angle = 45, vjust = 0.6))

# Tabla de contingencias
tabla_frec_abs <- table(df_copy$foreign_worker, df_copy$default)
colnames(tabla_frec_abs) = c("No riesgo", "Sí riesgo")
tabla_frec_abs
prop.table(tabla_frec_abs, margin = 1) 
```

Vemos que es verdad que si eres extranjero tienes mayor riesgo de que no te concedan el préstamos, pero esto es relativo, ya que hay muy pocos registros de clientes que no sean extranjeros, para ser más exactos solo 37 registros de 1000 se corresponden con registros de clientes no extranjeros.

Finalmente, analizamos si el trabajo influye o no:
```{r}
# Gráfico
ggplot(data = df_copy, aes(x=job,fill=as.factor(default))) + geom_bar(position = "fill") +
  xlab("Job") +
  ylab("Frecuencia") +
  scale_fill_manual(name = "Default", values=c("#00bfc4","#f8766d")) +
  ggtitle("Riesgo respecto a job") + 
  theme(axis.text.x = element_text(angle = 45, vjust = 0.6))

# Tabla de contingencias
tabla_frec_abs <- table(df_copy$job, df_copy$default)
colnames(tabla_frec_abs) = c("No riesgo", "Sí riesgo")
tabla_frec_abs
prop.table(tabla_frec_abs, margin = 1) 
```

Vemos que la especialización del trabajo no influye a si se concede un préstamo o no, ya que todos los niveles presentan un riesgo relativamente bajo, siendo el más alto el de mangement self-employed con un 34%.

### Correlaciones

Para terminar con el apartado del análisis y así poder realizar el modelo, vamos a ver cómo de correlacionadas están las variables numéricas de nuestro dataset, esto nos va a permitir hacenos una idea de si las variables independientes de nuestro modelo van a afectar a la variable objetivo, que en nuestro caso es predecir si dados unos datos el cliente supone un riesgo o no concederle el préstamos.

Para ello lo primero que debemos de hacer es quedarnos con las variables numéricas:
```{r}
df_copy_numeric = df_copy[, unlist(lapply(df_copy, is.numeric))]

head(df_copy_numeric)
```

Vemos que salvando la variable objetivo que es default, solo tenemos cuatro variable numéricas según el proyecto que estamos realizando, por lo que vamos a ver cómo de correlacionadas están:
```{r}
cor(df_copy_numeric)

corrplot(cor(df_copy_numeric), type = "upper", 
         order = "original", 
         tl.col = "black", 
         tl.srt = 45)
```

De la anterior ejecución podemos observar que amount y months_loan_duration están altamente correlacionadas positivamente, es decir, cuando aumenta una variable la otra aumenta. Y respecto al análisis de default vemos que months_loan_duration y amount están un poco correlacionadas positivamente con default, y la edad respecto a default está muy poco correlacioanda negativamente con default.

Todas estas correlaciones no son del todo significativas, ya que solo podemos considerarlas como tal si la correlación en valor absoluto es superior a 0.7, pero tal y como podemos observar no es así, por lo que es interesante saber que estas variables están un poco correlacionadas pero no del todo (no sabemos cómo va a afectar a nuestro modelo si incluimos dichas variables o no).

## Primer árbol de decisión

Una vez que ya hemos realizado todo el análisis del conjunto de datos, vamos a modelar el problema a partir de un árbol de decisión.

Cabe destacar que en este apartado vamos a crear un modelo sencillo de tres variables, ya que se nos pide la representación gráfica y ésta tiene que ser legible. En el apartado seis, se crearán dos modelos diferentes es aquí donde jugaremos con más variables para saber qué modelo se comporta mejor.

Por lo tanto, lo primero de todo es dividir el conjunto de datos inicial, en dos conjuntos: uno para entrenar el modelo y otro para realizar predicciones, pero antes debemos dividir los datos según la variable dependiente (default) y el resto de variables independientes:
```{r}
set.seed(0)

df_vObjetivo = df_copy$default
df_vIndependientes = df_copy[, -c(14)]
```

Ahora sí, creamos el conjunto de train y el de test, para el de train se usa 2/3 de los datos y para el de test 1/3:
```{r}
separacion = 3

indexes = sample(1:nrow(df_copy), size=floor(((separacion-1)/separacion)*nrow(df_copy)))
trainX = df_vIndependientes[indexes,]
trainy = df_vObjetivo[indexes]
testX = df_vIndependientes[-indexes,]
testy = df_vObjetivo[-indexes]
```

Comprobamos que no vamos a tener datos sesgados:
```{r}
summary(trainX)
summary(trainy)
summary(testX)
summary(testy)
```

De la ejecución anterior vemos que tanto el conjunto de entrenamiento como el conjunto de test no presentan una gran diferencia entre sus valores, por lo que la informacón no nos va a quedar sesgada.

Una vez que ya tenemos los conjuntos de dastos podemos crear y entrenar el modelo. Aunque el conjunto de datos de las variables independientes tienen 16 atributos, si introducimos todos ellos al modelo no nos va a proporcionar un árbol visualmente legible, es por ello que hemos decidido seleccionar tres variables.

Las tres variables seleccionadas son: savings_balance, checking_balance y months_loan_duration. Se han elegido estas variables y no otras por el análisis que hemos hecho de los datos anteriormente. Además, a modo de prueba se creó un modelo con todas las variables y vimos que las que mejor discriminaban los datos eran estas.

Una vez seleccionadas las variables creamos el modelo, el cual nos va a dar un punto de salida:
```{r}
set.seed(0)

modelo_c50_basico = C50::C5.0(x = trainX[, c("savings_balance", "checking_balance", "months_loan_duration")], y = as.factor(trainy))

plot(modelo_c50_basico)
```

Tal y como podemos apreciar en la anterior ilustración, el nodo raíz es el balance de gastos, si éste es mayor a 200 DM o unknown entonces el préstamo tiene un probabilidad aproximada del 86% de que pertenezca a la clase 1, es decir, que no haya riesgo en el préstamo.

Si el balance de gasto es inferior a < 0 DM o está entre 1 y 200 DM y la duración del préstamo es inferior o igual a 22 meses, entonces el préstamo es de la clase 1, es decir, no hay riesngo en el préstamo con una probabilidad de 78%.

Si el balance de gasto es inferior a < 0 DM o está entre 1 y 200 DM y la duración del préstamo es superior a 22 meses, entonces dependiendo de si el balance de ahorros es superior a > 1000 DM o unknown, el préstamo pertenece a la clase 1, no hay riesgo en el préstamo.

Finalmmente, si el balance de gasto es inferior a < 0 DM o está entre 1 y 200 DM y la duración del préstamo es superior a 22 meses, entonces dependiendo de si el balance de ahorros es inferior a < 100 DM o está entre 101 y 500 DM o está entre 501 y 1000 DM, el préstamo pertenece a la clase 2, sí que hay riesgo en el préstamo.

Por otro lado, podemos hacer un summary del modelo para ver cómo está definido:
```{r}
summary(modelo_c50_basico)
```

Vemos que el modelo no es perfecto, sino que contiene errores, para ser más exactos el 23.6% de los datos de entramiento el modelo no ha sido bueno clasificándolos, es decir, 157 registros se han predicho como erróneos.

Esto lo podemos ver en el apartado errors y justo debajo de él nos indica que para nuestro modelo ha clasificado 109 registros como clase 1 y realmente eran clase 2, pero también ha clasificado 48 registros como clase 2 cuando realmente eran clase 1.

En el siguiente apartado vamos a analizar el mismo modelo pero a partir de las reglas que se han generado, y en el siguiente vamos a predecir los valores para el conjunto de datos del test, permitiéndonos así hacer una estimación del error del modelo junto a sus tipos de errores.

## Explicación de las reglas obtenidas

Una vez que hemos analizado el modelo de forma visual, vamos a modelar de nuevo pero indicando que nos muestre las reglas obtenidas:
```{r}
set.seed(0)

modelo_c50_basico = C50::C5.0(x = trainX[, c("savings_balance", "checking_balance", "months_loan_duration")], y = as.factor(trainy), rules = TRUE)

summary(modelo_c50_basico)
```

De la anterior ejecución podemos observar las reglas obtenidas:

- Cuando checking_balance es igual a > 200 DM o unknown => Clase 1 con un 86% (no hay riesgo en el préstamo).

- Cuando checking_balance es igual a < 0 DM o 1-200 DM y su months_loan_duration es menor o igual a 22 => Clase 1 con un 78%.

- Cuando checking_balance es igual a < 0 DM o 1-200 DM, su months_loan_duration es mayor que 22 y su savings_balance es > 1000 DM o unknown => Clase 1 con un 86%.

- Cuando checking_balance es igual a < 0 DM o 1-200 DM, su months_loan_duration es mayor que 22 y su savings_balance es < 100 DM o está entre 101-500 DM o está entre 501-1000 DM => Clase 2 con un 60% (hay riesgo en el préstamo).

Resumiendo, el último punto es el único que nos determina que hay un riesgo en el préstamo, por lo que se recomendaría a la entidad bancaría no conceder el mismo.


## Análisis de la bondad de ajuste sobre el conjunto de test y matriz de confusión


## Modelos complementarios


## Conclusiones obtenidas

******
# Rúbrica
******
* (Obligatorio) Se debe realizar un breve informe (PDF, Html.... ) donde se respondan a las preguntas concretas, mostrando en primer lugar el código utilizado, luego los resultados y posteriormente los comentarios que se consideren pertinentes para cada apartado.  
* 10% Hay un estudio sobre los datos de los que se parte, las variables que componen los datos. Los datos son preparados correctamente.
* 10% Se realiza un análisis descriptivo univariante (o análisis de relevancia) de algunas variables una vez se han tratado vs el target a nivel gráfico, comentando las que aparentemente son más interesantes. Análogamente se realiza un análisis de correlaciones.
* 20% Se aplica un árbol de decisión de forma correcta y se obtiene una estimación del error, mostrando gráficamente el árbol obtenido. La visualización debe ser comprensible y adecuada al problema a resolver.
* 15% Se explican las reglas que se obtienen en términos concretos del problema a resolver.
* 15% Se usa el modelo para predecir con muestras no usadas en el entrenamiento (holdout) y se obtiene una estimación del error. En base a la matriz de confusión, se comentan los tipos de errores y se valora de forma adecuada la capacidad predictiva del algoritmo.
* 15% Se prueba otro modelo de árbol o variantes diferentes del C50 y se comparan los resultados obtenidos, valorando si son mejores.
* 10% Se presenta unas conclusiones donde se expone un resumen de los diferentes modelos utilizados (al menos 3) así como el conocimiento adquirido tras el trabajo realizado y los descubrimientos más importantes realizados en el conjunto de datos.
* 5% Se presenta el código y es fácilmente reproducible.

