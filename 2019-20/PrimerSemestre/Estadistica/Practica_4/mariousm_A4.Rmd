---
title: "A4 - Análisis de varianza y repaso del curso"
author: "Mario Ubierna San Mamés"
date: '`r format(Sys.Date(),"%e de %B, %Y")`'
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Importación de librerías:
```{r}
library(ggplot2)
library(nortest)
library(lsr)
library(agricolae)
```

# Lectura del fichero y preparación de los datos

Leed el fichero trainCLEAN.csv y guardad los datos en un objeto con identificador denominado claim. A contiunación, verificad que los datos se han cargado correctamente.
```{r}
claim = read.csv(file = "./data/trainCLEAN.csv", sep = ",", stringsAsFactors = TRUE, na.strings = NA)

head(claim)
```

Comprobamos los tipos de las variables y que se han cargado todas:
```{r}
str(claim)
```

Vemos que seguimos manteniendo las 15 variables iniciales, por lo que la lectura se ha realizado de forma correcta.

## Preparación de los datos

Cambiamos el nombre de las variables a castellano. En concreto, se pide que se denominen de la siguiente forma: Id, Ocurrencia, Apertura, Edad, Sexo, Estado, Dependientes, OtrosDepend, Salario, Jornada, CosteInicio, CosteFinal, HorasSemana, DiasSemana y Descripcion:
```{r}
colnames(claim) = c("Id", "Ocurrencia", "Apertura", "Edad", "Sexo", "Estado", "Dependientes", "OtrosDepend", "Salario", "Jornada", "HorasSemana", "DiasSemana", "Descripcion", "CosteInicio", "CosteFinal")

str(claim)
```

### Las variables ‘Ocurrencia‘ y ‘Apertura‘ están clasificadas como factor. Para poder trabajar con ellas hay que convertirlas en fechas:
```{r}
claim$Ocurrencia = as.Date(claim$Ocurrencia)
claim$Apertura = as.Date(claim$Apertura)

head(claim[, c("Ocurrencia", "Apertura")])
```

### Crear una variable denominada ‘tiempo‘ que contabilice en días el tiempo que tarda en abrirse un siniestro por la compañía desde su ocurrencia:
```{r}
claim$tiempo = as.integer(claim$Apertura - claim$Ocurrencia)

head(claim[, c("tiempo", "Ocurrencia", "Apertura")])
```

## Clasificación de tiempo

La variable tiempo indica la duración de apertura del siniestro de la siguiente forma: “Muy rápido” si se apertura en 15 días o menos, “Rápido” si se apertura entre 16 y 30 días, “Lento” si se apertura entre 31 y 89 días, y “Muy lento” si tarda 90 días o más en aperturarse el siniestro. Cread una variable categórica denominada Clasificacion, que clasifique el siniestro según estas categorías:
```{r}
claim$Clasificacion = as.factor(cut(claim$tiempo, breaks = c(-1, 15, 30, 89, 1095), labels = c("Muy rápido", "Rápido", "Lento", "Muy lento")))

head(claim[, c("tiempo", "Clasificacion")])
```

## Valores ausentes

### Analizad el número de categorías distintas en las variables ‘Descripcion‘, ‘Sexo‘ y ‘Estado‘. ¿Cuántas descripciones distintas hay de los siniestros?
```{r}
summary(claim)
```

Respecto a la variable Descripción vemos que tenemos muchos tipos de categorías, para ser más exactos hay 52417 categorías diferentes.

En cuanto a la variable Sexo, hay tres categorías: F, M y U. Éste último tipo nos indica que se desconoce si es de género femenino o masculino.

Finalmente, la variable Estado hay cuatro tipos: Sin valor (NA), M, S y U. Al igual que antes este último tipo nos indica que es deconocido el estado para dicha observación.

### Representad los observaciones con la categoría "U" (U=unknown) en las variables ‘Sexo‘y ‘Estado‘ como missings.

```{r}
claim[claim$Sexo == "U", "Sexo"] = NA 
claim[claim$Estado == "U", "Estado"] = NA
```


Comprobamos que se han realizado los cambios:
```{r}
summary(claim$Sexo)
summary(claim$Estado)
```

Ahora vemos que ya tenemos todos los valores perdidos como NA.

### Comprobad la proporción de observaciones que tienen valores ausentes y sacad conclusiones sobre cómo de serio es el problema de valores ausentes en estos datos.

En el anterior apartado hemos visto que hay 2 observaciones con valor NA para la variable Sexo, realmente son observaciones insignificantes ya que hay un total de 54000 registros y solo dos son NA. Por lo que, podríamos eliminar dichas observaciones sin problemas.

Respecto a la variable Estado vemos que tenemos más registros como NA, para ser más exactos hay 5294 observaciones de un total de 54000 registros, esto representa un 9.8% de los datos, es decir, en este caso sí que es significativo. Podríamos crear un modelo de regresión lineal para predecir dicho valor, imputar el valor más frecuente, pero en el siguiente apartado se nos pide que se eliminen dichas observaciones.

### Eliminad los valores ausentes del conjunto de datos. Denominamos al conjunto de datos claimNet.

Ejecutamos el summary para ver qué variables tienen valores ausentes, los cuales son distintos de los missings, los primeros son aquellos en los que no viene un valor, mientras que lo segundo sí que viene un valor pero es del tipo unknown:
```{r}
summary(claim)
```

De la anterior ejecución vemos que la variable Estado tiene 29 como vacíos (valores ausentes), por lo que procedemos la eliminación de dichos registros, también elimanos las observaciones cuyo valores sea NA:
```{r}
claimNet = claim[claim$Estado != "", ]
claimNet = claimNet[!is.na(claimNet$Estado),]
claimNet = claimNet[!is.na(claimNet$Sexo),]

summary(claimNet)
```

Vemos que ahora el dataset ha pasado de 54000 registros a 48675.

## Saluda mental

La compañía está preocupada por las bajas por salud mental. Por este motivo, quiere monitorizar las bajas que incluyan las palabras Stress, Anxiety, Harassment o Depression. Se pide:

### Crear la variable dicotómica denominada ‘RiesgoSM‘ si la variable ‘Descripcion‘ incluye alguna de estas palabras.

Lo primero de todo es definir la función que nos devuelve un vector con la variable dicotimizada, un 1 si encuentra la palabra en la descripción o 0 en caso contrario:
```{r}
getRiesgoSM = function(descripciones) {
  riesgoSM = c()

  for (desc in descripciones) {
    if (grepl("STRESS", toupper(desc)) == TRUE ||
        grepl("ANXIETY", toupper(desc)) == TRUE || 
        grepl("HARASSMENT", toupper(desc)) == TRUE || 
        grepl("DEPRESSION", toupper(desc)) == TRUE) {
     riesgoSM[length(riesgoSM) + 1] = 1
   } else {
     riesgoSM[length(riesgoSM) + 1] = 0
   } 
  }
  
  return(riesgoSM)
}
```

Una vez definida la función la llamamos y creamos la variables en el dataframe:
```{r}
claimNet$RiesgoSM = getRiesgoSM(claimNet$Descripcion)

#La convertimos a factor
claimNet$RiesgoSM = as.factor(claimNet$RiesgoSM)

# Comprobamos que se ha creado correctamente
str(claimNet)
```

## Análisis visual

Mostrad con diversos diagramas de caja la distribución de la variable ‘CosteFinal‘ en escala logarítmica según la variable ‘Sexo‘, según ‘Estado‘, según ‘Clasificacion‘ y según ‘RiesgoSM‘. E interpretad los gráficos brevemente.

Lo primero de todo es crear una nueva variable para convertir a escala logarítmica el CosteFinal:
```{r}
claimNet$LogCosteFinal = log(claimNet$CosteFinal)

summary(claimNet)
```

Una vez que ya tenemos la variable CosteFinal en escala logarítmica, hacemos el análisis de la misma respecto al sexo:
```{r}
boxplot(claimNet[claimNet$Sexo == "F", "LogCosteFinal"],
        claimNet[claimNet$Sexo == "M", "LogCosteFinal"],
        main = "CosteFinal respecto al Sexo",
        xlab = "Femenino - Masculino",
        ylab = "Valor de CosteFinal"
        )
```

Del anterior gráfico podemos observar que el género femenino tiene un valor mínimo algo mayor que el masculino, es por ello que su mediana es un pelín más elevada, esto lo que significa es que de media las mujeres tienen un CosteFinal algo mayor, en otras palabras la indemnización suele ser algo mayor si eres mujer.

Por otro lado, observamos que hay bastantes valores atípicos en los dos sexos, pero especialemnte más en el masculino, es decir, suele haber más casos "especiales" si eres hombre que si eres mujer.

Continuiamos analizando el CosteFinal, pero ahora respecto a el estado civil:
```{r}
boxplot(claimNet[claimNet$Estado == "M", "LogCosteFinal"],
        claimNet[claimNet$Estado == "S", "LogCosteFinal"],
        main = "CosteFinal respecto al Estado",
        xlab = "Casado - Soltero",
        ylab = "Valor de CosteFinal"
        )
```

En este caso, vemos que sucede algo parecido, si eres casado tu valor en el límite inferior es algo mayor que si estás soltero haciendo que la mediana sea algo mayor, es decir, si estás casado sueles obtener una mejor indemnización que si estás soltero.

Por otro lado, vemos que también hay muchos valores atípicos, pero en este caso está bastante bien equilibrado entre ambos grupos.

Analizamos el CosteFinal según la variable Clasificación:
```{r}
boxplot(claimNet[claimNet$Clasificacion == "Muy rápido", "LogCosteFinal"],
        claimNet[claimNet$Clasificacion == "Rápido", "LogCosteFinal"],
        claimNet[claimNet$Clasificacion == "Lento", "LogCosteFinal"],
        claimNet[claimNet$Clasificacion == "Muy lento", "LogCosteFinal"],
        main = "CosteFinal respecto a la Clasificación",
        xlab = "Muy rápido - Rápido - Lento - Muy lento",
        ylab = "Valor de CosteFinal"
        )
```

El anterior gráfico es muy interesante, ya que nos indica que si la duración de apertura del siniestro es muy lenta, podemos entender que se realiza un estudio muy detallado, por lo que apenas tenemos valores atípicos y éstos están muy bien contralados. Sin embargo, con el resto de variables sucede todo lo contrario.

También es curioso ver que si la clasificación es rápida tiende a tener una mayor indemnización, es decir, el coste que le supone a la asegurado es mayor, esto lo apreciamos gracias a la mediana correspondiente.

Por último, vamos a analizar el CosteFinal según el RiesgoSM:
```{r}
boxplot(claimNet[claimNet$RiesgoSM == "1", "LogCosteFinal"],
        claimNet[claimNet$RiesgoSM == "0", "LogCosteFinal"],
        main = "CosteFinal respecto al RiesgoSM",
        xlab = "Hay riesgo (1) - No hay riesgo (0)",
        ylab = "Valor de CosteFinal"
        )
```

Este gráfico es el más llamativo de todos ya que en los anteriores estaban más o menos nivelados, pero en este caso vemos que el riesgo de padecer un enfermedad de salud mental influye bastante en el coste final, siendo el límite inferior más elevado y su mediana también.

Por otro lado, si no hay riesgo tenemos más valores atípicos, es decir, hay más situacioens "especiales" que por los motivos que sean se dan una indemnización mayor o menor.

## Comprobación de normalidad

¿Podemos asumir que la variable CosteFinal tiene una distribución normal? Debéis justificar la respuesta a partir de métodos visuales y contrastes.

### Realizad la insepección visual de normalidad de CosteFinal

Lo primero de todo es analizar la variables CosteFinal de forma gráfica:
```{r}
ggplot(mapping = aes(x=claimNet$CosteFinal)) + geom_density()
```

Como podemos apreciar de la anterior ilustración, CosteFinal no sigue una distribución normal, pero vamos a anilizarlo en el siguiente punto a partir de un test.

### Realizad contraste de normalidad de Lilliefors de CosteFinal

Aplicamos el test de Lilliefors, en dicho test la hipótesis nula es que la variable sigue una distribución norma, y la alternativa en caso contrario:
```{r}
lillie.test(claimNet$CosteFinal)
```

Como nos da un pvalue más pequeño que alfa (0.05), es decir, pvalue < alfa, podemos rechazar la hipótesis nula a favor de la alternativa, es decir, que nuestra variable no sigue una distribución normal.

### Realizad la insepección visual de normalidad de CosteFinal en escala logarítmica

Realizamos el mismo gráfico que antes pero para la variable LogCosteFinal:
```{r}
ggplot(mapping = aes(x=claimNet$LogCosteFinal)) + geom_density()
```

Como podemos apreciar, a priori cuando escalamos la variable logarítmica sí que sigue una distribución normal, pero para confirmarlo ejecutamos el mismo test que en el caso anterior.

### Realizad contraste de normalidad de Lilliefors de CosteFinal en escala logarítmica

Volvemos a aplicar el test de Lilliefors:
```{r}
lillie.test(claimNet$LogCosteFinal)
```

En este caso nos da un pvalue mayor que alfa, es decir, pvalue > 0.05, por lo tanto podemos asegurar que la variable CosteFinal en escala logarítmica sigue una distribución normal.

# Estadística inferencial

Utilizando el conjunto de datos claimNet.

## Intervalo de confianza de la media poblacional de la variable CosteFinal

Calculad manualmente el intervalo de confianza al 95% de la media poblacional de la variable ‘CosteFinal‘ en escala normal (No se pueden utilizar funciones como t.test o z.test para el cálculo). A partir del resultado obtenido, explicad cómo se interpreta el intervalor de confianza.

Al realizar este ejercicio no se nos proporciona la varianza poblacional por lo que tenemos que calcular la varianza o la desviación de la muestra, esto lógicamente supone más incertidumbre ya que calculamos la desviación a partir de la muestra no de la población, es por ello que se recomienda hacer el cálculo a partir de la distribución t-student.

Por lo tanto, lo primero que vamos a hacer es calcular la media y la desviación de la muestra:
```{r}
media_muestra = mean(claimNet$CosteFinal)
sd_muestra = sd(claimNet$CosteFinal)
```

Lo segundo es definir nuestra alfa (nivel de significancia) y el tamaño de la muestra:
```{r}
alfa = 1 - 0.95
tam_muestra = length(claimNet$CosteFinal)
```

Calculamos el estadístico correspondiente:
```{r}
estadistico = sd_muestra / sqrt(tam_muestra)
z = qt(alfa/2, tam_muestra - 1, lower.tail = FALSE)
```

Finalmente, calculamos los límites:
```{r}
lim_inferior = round(media_muestra - (z * estadistico), 2)
lim_superior = round(media_muestra + (z * estadistico), 2)

cat("El límite inferior del intervalo de confianza (L) es igual a: ", lim_inferior)
cat("El límite superior del intervalo de confianza (U) es igual a: ", lim_superior)
```

De la anterior ejecución observamos que el intervalo de confianza es [9450.34, 9958.82]. Cuando calculamos el intervalo de confianza no podemos afirmar que cualquier valor de la población esté dentro del intervalo, lo que conseguimos definir al calcular el intervalo de confianza es asegurarnos, en nuestro caso a un 95%, del procedimiento, es decir, que el nivel de confianza en el procedimiento de cualquier muestra esté garantizado dando lugar a un intervalo que contenga el valor.

Resumiendo, de lo que nos garantizamos es que el 95% de los intervalos de confianza calculados a partir de cada muestra contengan el valor del CosteFinal.

## Contraste de hipótesis para la diferencia de medias

¿Podemos aceptar que la indemnización a las mujeres supera en más de 1000 EUR la de los hombres? Responded a la pregunta utilizando un nivel de confianza del 95%.

Nota: se deben realizar los cálculos manualmente. No se pueden usar funciones de R que calculen directamente
el contraste como t.test o similar. Sí se pueden usar funciones como mean, sd, qnorm, pnorm, qt y pt.

### Escribid la hipótesis nula y la alternativa

En este caso, presentamos las siguientes hipótesis:

- H0 (hipótesis nula): media_CosteFinal_Mujeres = (media_CosteFinal_Hombres + 1000)

- H1 (hipótesis alternativa): media_CosteFinal_Mujeres > (media_CosteFinal_Hombres + 1000)

###  Justificación del test a aplicar

En este problema tenemos questión a responder y presenta las siguientes características:

- El contraste es de dos muestras, por un lado está la muestra de las mujeres y por otro el de hombres. Estas muestras son independientes no hay una relación entre sí.

- Sí que podemos asumir normalidad, es decir, podemos aplicar el Teorema del Límite Central, éste nos dice que si la media de una muestra es lo suficientemente grande (mayor que 30 observaciones) sigue una distribución normal. Al poder calcular la media de ambas muestras solo nos falta comprobar el tamaño de las mismas:
```{r}
claimNet_femenino = claimNet[claimNet$Sexo == "F",]
claimNet_masculino = claimNet[claimNet$Sexo == "M",]

cat("El número de observaciones del sexo femenino es: ", nrow(claimNet_femenino))
cat("El número de observaciones del sexo masculino es: ", nrow(claimNet_masculino))
```

- El test es paramétrico, éstos se basan en que los datos siguen una determinada distribución, por norma general que siguen una distribución normal, por lo que tal y como hemos visto en el punto anterior podemos asumir la normalidad de los datos.

- El problema es unilateral por la derecha, ésto nos lo determina la hipótesis alternativa, ya que al tener el símbolo ">" mayor que, solo aceptamos la hipótesis alternativa si el valor observado es mayor que el valor crítico.

- Respecto a si es homocedasticidad o heterocedasticidad hacemos uso de la función var.test y comprobamos si el p-valor es menor que alfa (nivel de signifancia), en nuestro caso si p-valor < alfa (0.05) rechazamos la hipótesis nula siendo ésta que las varianzas son iguales, tal y como podemos ver a continuación el p-valor es menor que alfa, por lo que rechazamos la hipótesis nula, lo cual significa que estamos en un caso de heterocedasticidad:
```{r}
var.test(claimNet_femenino$CosteFinal, claimNet_masculino$CosteFinal, conf.level = 0.95)
```

### Cálculos

Realizad los cálculos del estadístico de contraste, valor crítico y valor p con un nivel de confianza del 95%.

Lo primero de todo es definir la función que nos calcula el estadístico, el valor crítico y el pvalor:
```{r}
muestrasIndependientes_varianzasDesconocidas = function(valores_fem, valores_masc, alfa) {
  
  # Calculamos el estadístico de contraste
  sd_fem = sd(valores_fem)
  sd_masc = sd(valores_masc)
  n_fem = length(valores_fem) 
  n_masc = length(valores_masc)
  
  # -1000 porque se indica en la hipótesis media_CosteFinal_Mujeres > (media_CosteFinal_Hombres + 1000)
  numerador = mean(valores_fem) - mean(valores_masc) - 1000
  denominador = sqrt(((sd_fem)^2/n_fem) + ((sd_masc)^2/n_masc))
  
  z_obs = numerador / denominador
  
  # Calculamos los grados de libertad
  numerador = (((sd_fem)^2/n_fem) + ((sd_masc)^2/n_masc))^2
  denominador = (((sd_fem)^2/n_fem)^2 / (n_fem - 1)) + (((sd_masc)^2/n_masc)^2 / (n_masc - 1))
  grados_libertad = as.integer(numerador / denominador)
  
  # Calculamos el punto crítico
  z_cri = qt(alfa, df = grados_libertad, lower.tail = FALSE)
  
  # Calculamos el p-valor
  p_value = pt(z_obs, df = grados_libertad, lower.tail = FALSE)
  
  # Devolvemos los valores
  return(data.frame(L = "-INF", U = z_cri, Estadistico_Contraste = z_obs, P_Valor = p_value, Grados_Libertad = grados_libertad))
}
```

Una vez definida la función la llamamos para obtener los resultados:
```{r}
muestrasIndependientes_varianzasDesconocidas(claimNet_femenino$CosteFinal, claimNet_masculino$CosteFinal, 0.05)
```

# Interpretación del test

Podemos interpretar el test de dos formas distintas:

- La primera, comprobamos si el valor observado está fuera de la región de aceptación, si está fuera podemos rechazar la hipótesis nula en favor de la alternativa. En nuestro caso, la región de aceptación es el intervalo [-INF, 1.64495] y el valor observado 3.591609, como el valor observado está fuera de la región de aceptación rechazamos la hipótesis nula a favor de la alternativa, es decir, que es verdad que la indemnización de las mujeres supera en más de 1000 euros a la de hombres con un nivel de confianza del 95%.

- La segunda opción es comprobar el pvalor, si éste es menor que alfa rechazamos la hipótesis nula, en nuestro caso pvalor = 0.00016 < alfa = 0.05, al ser menor que alfa rechazamos la hipótesis nula en favor de la alternativa, es decir, confirmamos que la indemnización de las mujeres supera en 1000 euros a la de hombres con un nivel de confianza del 95%. 

# Modelo de regresión lineal

Estimad un modelo de regresión lineal múltiple que tenga como variables explicativas: Edad, Sexo, Estado, Dependientes, OtrosDepend, Salario, Jornada, HorasSemana, DiasSemana, Clasificacion, RiesgoSM, CosteInicio y como variable dependiente el CosteFinal en escala logarítmica (Nota: se recomienda transformar también a escala logarítmica la variable explicativa CosteInicio)

## Interpretación del modelo

Interpretad el modelo lineal ajustado:

– ¿Cuál es la calidad del ajuste?

– Explicad la contribución de las variables explicativas en el modelo.

Lo primero de todo es transformar a escala logarítmica la variable CosteInicio, para ello creamos una nueva variable en el dataframe:
```{r}
claimNet$LogCosteInicio = log(claimNet$CosteInicio)

str(claimNet)
```

Ahora sí, vamos a crear el modelo de regresión lineal:
```{r}
modelo_rlm = lm(formula = LogCosteFinal ~ Edad + Sexo + Estado + Dependientes + OtrosDepend + Salario + Jornada + HorasSemana + DiasSemana + Clasificacion + RiesgoSM + LogCosteInicio, data = claimNet)
```

Una vez creado el modelo, vamos a analizar la calidad del mismo junto con la contribución de cada variable:
```{r}
summary(modelo_rlm)
```

La calidad del modelo viene determindada por el coeficiente de determinación o R2. El coeficiente de determinación o R2, nos indica el grado de ajuste de la recta de regresión a los valores de la muestra, es decir, el cómo es la proporción de variación en nuestro modelo. R2 es un coeficiente que puede ir entre 0 y 1, cuanto más próximo a uno mejor es el modelo, ya que significa que los errores/residuos que se cometen son muy pequeños y los valores se ajustan a la línea de regresión.

En nuestro caso vemos que tenemoun un R2 de 0.733, es decir, estamos cerca del 1 pero no mucho por lo que aunque el modelo es bueno, no es el mejor de todos (se podría hacer un análisis de correlaciones y ver qué variables están correlacionadas para eliminarlas del modelo y ver si mejora éste...).

Respecto a la contribución de las variables al modelo, podemos observar que las que más contribuyen son las siguientes:

- Edad: es altamente significativa, es decir, influye en si el CosteFinal (indemnización) es mayor o menor, como su coeficitente es positivo cuando aumenta la edad aumenta la probabilidad de lo que queremos predecir.

- Sexo: aunque esta variable es dicotómica y solo aparece el sexo cuando es masculino, vemos que dicha variable Sexo también es altamente significativa, es decir, como su coeficiente es negativo si eres de sexo masculino el CosteFinal se va a reducir.

- Estado: lo mismo que sucede con el sexo pasa con el estado, es decir, si eres soltero es menos probable de que aumente el CosteFinal.

- Dependientes: es significativa y al aumentar el valor de dicha variable aumenta el CosteFinal.

- Salario: sigue siendo altamente significativa, y cuando aumenta el salario mayor CosteFinal vamos a obtener a la hora de predecir.

- Jornada: vemos que el tipo de jornada laboral también afecta, cuando la jornada es a tiempo partido mayor CosteFinal tenemos.

- DiasSemana: también influye en el CosteFinal, a mayor días semana trabajados menos CosteFinal tenemos y viceversa.

- Clasificación: esta variable es significativa, no tanto como las anteriores, pero si Clasificación es Rápido aumenta el CosteFinal.

- RiesgoSM: es altamente significativa en el modelo, si se padece de una enfermedad mental mayor CosteFinal.

- LogCosteInicio: vemos que también es altamente significativa, a medida que aumenta el valor de dicha variable aumenta el CosteFinal y viceversa.

## Análisis de residuos

Por último, para profundizar en la calidad del ajuste se deben analizar los residuos que nos indicarán realmente como se ajusta nuestro modelo a los datos muestrales.

- La salida de ‘summary()‘ presenta los principales estadísticos de la distribución de los residuos. Analizad los valores estimados de los estadísticos.

- Realizad un análisis visual de los residuos.

Lo primero de todo decir que el residuo es el error, es decir, el error que genera el modelo entre el valor que predice el propio modelo y el real.

```{r}
summary(modelo_rlm)
```

Analizando con summary vemos que tenemos un error estándar residual de 0.7901, cuanto más próximo esté ese valor a 0 significa que mejor ajusta el modelo, si el error fuera 0 significa que a priori el modelo es perfecto porque no comete ningún error, pero esto no sería del todo cierto porque el modelo estaría sobreajustando.

Pero para entrar en más análisis vamos a hacer un boxplot de los residuos:
```{r}
residuos = rstandard(modelo_rlm)
ajustados = fitted(modelo_rlm)

boxplot(residuos)
```

Del anterior gráfico vemos que el rango intercuartílico es muy pequeño, es decir, vemos que el 50% de los datos están entre -0.44 y 0.36, sin embargo el otro 50% o es superior o inferior a  estos valores. Además, apreciamos que la mediana no se sitúa en el centro de la caja (está casi pero no), presentamos una asimetría positiva, esto significa que los datos se concencetrán más en la parte inferior de la distribución, con esto no quiero decir que haya más datos, sino que el rango es más amplio (están más dispersos). Finalmente, al tener un rango intercuartílico tan pequeño tenemos muchos outliers.

Por otro lado, para analizar los residuos vamos a crear dos gráficos. El primero nos permite ver si la varianza es constante, para ello representamos los residuos vs los valores ajustados:
```{r}
qplot(x = ajustados, y = residuos, main = "Residuales Estandarizados vs Valores Ajustados", geom = c("point"), ylab = "Residuales Estandarizados", xlab = "Valores Ajustados") + geom_hline(yintercept = 0 , color = 2)
```

Del anterior gráfico observamos que el modelo se comporta por norma general bien, ya que no hay un patrón definido en los datos. Sin embargo, vemos que cuando el valor ajustado diminuye el error suele aumentar, y cuando el valor ajustado aumenta el error también aumenta.

En resumen, el modelo ajusta bien pero hay un cierto error para determinados valores, a veces el valor ajustado respecto del real es mayor y viceversa, pero estos son casos muy puntuales.

El segundo gráfico que vamos a analizar es el cuantil-cuatil, éste nos permite analizar los residuos del modelo con los valores de una variables que se distribuye normalmente:
```{r}
ggplot(as.data.frame(residuos),aes(sample = residuos)) +  geom_qq() 
```

Si los residuos siguieran una distribución normal, tendríamos todos los puntos alineados formando una recta, pero tal y como podemos ver los residuos hacen una pequeña "S", esto nos da la idea de que cuando los valores ajustados son pequeños el modelo no ajusta del todo bien, y cuando los valores ajustados son muy grandes tampoco hace un buen ajuste.

Resumiendo, el modelo se comporta medianamente bien, lo único que hay casos especiales en los que los valores ajustados son o demasiado pequeños, o demasiado grandes.

## Prediccion

Predecid el coste esperado para las siguientes características: Edad=24, Sexo= “F”, Estado=“S”, Dependientes=1, OtrosDepend=0, Salario=500, Jornada=“F”,HorasSemana=40,DiasSemana=5, Clasificacion=“Lento”, RiesgoSM=“TRUE” y “CosteInicio”=10000.

(Nota: Debes tener en cuenta que el valor esperado de una variable aleatoria que su logaritmo se distribuye según una normal, i.e. distribución lognormal, es exp(mu+var/2) donde mu y var son la media y la varianza de la transformación logarítmica).

A continuación vamos a predecir el valor del CosteFinal, pero antes creamos el dataframe con la prediccion, cabe destacar que en el modelo tenemos LogCosteInicio no CosteInicio, por lo tanto convertimos a escala logarítmica dicho valor:
```{r}
df_predict = data.frame(24, "F", "S", 1, 0, 500, "F", 40, 5, "Lento", "1", log(10000))
colnames(df_predict) = c("Edad", "Sexo", "Estado", "Dependientes", "OtrosDepend", "Salario", "Jornada", "HorasSemana", "DiasSemana", "Clasificacion", "RiesgoSM", "LogCosteInicio")
```

Una vez creado el dataframe, realizamos la predicción:
```{r}
prediccion_valor = predict(modelo_rlm, df_predict)
cat("El CosteFinal que se produce a partir de los datos es:", prediccion_valor, "€")
```

Cabe destacar que el valor que obtenemos está en escala logarítmica, por lo que reconvertimos dicho valor a exponecial:
```{r}
prediccion_valor = exp(prediccion_valor)
cat("El CosteFinal que se produce a partir de los datos es:", prediccion_valor, "€")
```

En este caso vemos que tendríamos un CosteFinal de 13601.47€.

# Regresión logística

## Modelo predictivo

Utilizando las mismas características como variables explicativas, ajustad un modelo predictivo basado en la regresión logística para predecir la probabilidad de que la compañía cuantifique inicialmente el coste del siniestro de forma insuficiente.

Para ello, cread una variable Deficit que indique si la valoración inicial del coste del siniestro (CosteInicio) es inferior a la indemnización finalmente pagada por la compañía (CosteFinal). La variable Deficit debe codificarse como una variable dicotómica, que toma el valor 0 cuando la valoración inicial ha sido suficiente y 1 cuando la valoración inicial ha sido insuficiente.

La variable Deficit será la variable dependiente del modelo. Analizad la calidad del modelo y las variables que son relevantes.

Lo primero que debemos de hacer para crear el modelo es la definición de la variable Deficit, es por ello que lo hacemos ahora:
```{r}
claimNet$Deficit[claimNet$CosteInicio >= claimNet$CosteFinal] = 0 
claimNet$Deficit[claimNet$CosteInicio < claimNet$CosteFinal] = 1
claimNet$Deficit = as.factor(claimNet$Deficit)

head(claimNet)
```

Una vez que tenemos la variable dependiente del modelo, creamos el mismo:
```{r}
modelo_rlg = glm(formula = Deficit ~ Edad + Sexo + Estado + Dependientes + OtrosDepend + Salario + Jornada + HorasSemana + DiasSemana + Clasificacion + RiesgoSM + LogCosteInicio, family = binomial(link=logit), data = claimNet)
```

Finalemente, para terminar con este apartado analizamos la calidad del modelo y las variables que son relevantes:
```{r}
summary(modelo_rlg)
```

Para poder medir cómo de bueno es el modelo, nos tenemos que fijar en la devianza, si la devianza residual es menor que la devianza nula, entonces hemos conseguido mejorar el modelo. Podemos obtener un valor númerico si hacemos (Null deviance - Residual deviance) / Null deviance:
```{r}
cat("Calidad del modelo de regresión logística es de:", round((modelo_rlg$null.deviance - modelo_rlg$deviance) / modelo_rlg$null.deviance * 100, 2), "%")
```

Como podemos observar la calidad del modelo no es que sea muy buena, si fuera un bueno modelo su valor estaría cercano al 100%, sin embargo, está más cerca del 0 que del 100 ya que la devianza residual es muy grande, por lo que no es un buen modelo.

El otro aspecto a analizar son las variables relevantes, pero como esto es lo que se pide en el siguiente apartado, es allí dónde lo vamos a hacer.

## Interpretación

Interpretad el modelo ajustado. Concretamente, explicad la contribución de las variables explicativas con coeficiente estadísticamente significativo para predecir si la valoración inicial es insuficiente para cubrir el coste del siniestro.

Las variables estadísticamente significativas son aquellas cuyo Pr(>|z|) < 0.05:

- Edad: su coeficiente es positivo, por lo que a medida que aumenta la edad, la probabilidad de lo que estamos prediciendo (Deficit) aumenta.

- Sexo: su coeficiente es negativo en este caso para ver el impacto tenemos que analizar su OR, es decir, si es un factor de riesgo o de protección. Tal y como podemos observar, si el sexo es masculino es un factor de protección respecto al sexo femenino, es decir, es menos probable que haya un deficit se eres hombre que si eres mujer:
```{r}
exp(coefficients(modelo_rlg))
```


- Estado: sucede parecido al Sexo, tenemos un coeficiente negativo y a la vista de los OR, EstadoS su OR es menor que 1, por lo tanto, tenemos un coeficiente de protección, es decir, si el Estado es soltero es menos probable que haya déficit que si estás casado.

- Dependientes: su coeficiente es positivo, por lo que a medida que aumenta dicha variable también aumenta la probabilidad que suceda el déficit.

- Salario: tiene un coeficiente positivo, por lo tanto a medida que aumenta el salario también aumenta la probabilidad de que haya un déficit.

- Jornada: tiene un coeficiente positivo, por lo que aumenta el riesgo de que haya un déficit.

- DiasSemana: tiene un coeficiente negativo, por lo que es un factor de protección, a medida que aumenta dicha variable menos riesgo hay de que haya un déficit.

- Clasificacion: tanto si la clasifación es rápida como lenta es un factor de riesgo, ya que supone un aumento en la probabilidad de que haya un déficit.

- LogCosteInicio: tiene un coeficiente negativo, analizando su OR vemos que es un factor de protección, ya que al aumentar dicha variable reduce la probabilidad de déficit.

## Matriz de confusión

A continuación analizad la precisión del modelo, comparando la predicción del modelo sobre los mismos datos del conjunto de datos. Asumiremos que la predicción del modelo es 1 (valoración inicial del coste insuficiente) si la probabilidad del modelo de regresión logística es superior o igual a 0.5 y 0 en caso contrario. Analizad la matriz de confusión y las medidas de ‘sensitivity’ y ‘specificity’.

Nota: Tomad como categoría de interés que haya déficit en la valoración inicial del coste. Por tanto, déficit igual a 1 será el caso positivo en la matriz de confusión y 0 el caso negativo.

Lo primero de todo es calcular las predicciones del modelo:
```{r}
predicciones = predict(object =  modelo_rlg, newdata = claimNet[, c(
  "Edad",
  "Sexo",
  "Estado",
  "Dependientes",
  "OtrosDepend",
  "Salario",
  "Jornada",
  "HorasSemana",
  "DiasSemana",
  "Clasificacion",
  "RiesgoSM",
  "LogCosteInicio"
)])
```

Aunque tengamos las predicciones en sí, tienes que estar dicotomizadas, es por ello que si la predicción es >= 0.5 toma valor 1 o 0 en caso contrario:
```{r}
predicciones[predicciones >= 0.5] = 1
predicciones[predicciones < 0.5] = 0
```

Ahora ya podemos construir la matriz de confusión, pero antes establecemos la variable objetivo como numérica, al hacer as.integer(factor) suma uno, por lo que restamos dicho valor:
```{r}
valores_originales = as.integer(claimNet$Deficit) - 1
```

Creamos la matriz de confusión:
```{r}
matriz_confusion = table(valores_originales, predicciones)
matriz_confusion
```

Cabe mencionar que "1" es caso positivo (hay déficit) y "0" es caso negativo.

Una vez calculada la matriz de confusión podemos calcular las medias que nos indican:

- Sensibilidad, es la tasa de verdaderos positivos, la proporción de los clasificados correctamente entre todos los que han dado positivo:
```{r}
sensibilidad = matriz_confusion[2,2] / (matriz_confusion[2,2] + matriz_confusion[2,1])
cat("La sensibilidad es igual a", sensibilidad)
```

Como vemos la sensibilidad no es del todo buena, ya que si fuera 1 indicaría que todo lo ha clasificado correctamente, al ser 0.54 por norma general lo clasifica bien, pero este modelo no es muy bueno al predecir positivos.

- Especificidad, es la tasa de verdaderos negativos, la proporción de casos correctamente clasificados entre todos los que han dado negativo:
```{r}
especificidad = matriz_confusion[1,1] / (matriz_confusion[1,1] + matriz_confusion[1,2])
cat("La especificidad es igual a", especificidad)
```

En este caso, vemos que la especificidad es algo mejor, está más próxima a uno por lo que el modelo es capaz de predecir mejor cuando un caso es negativo que cuando es positivo.

## Predicción

¿Con que probabilidad la valoración inicial del siniestro será insuficiente para un hombre de 20 años de edad, soltero, sin hijos ni otros dependientes, con un salario semanal de 300 EUR, jornada partida, con 30 horas semanales y cinco días a la semana, una clasificación del tiempo hasta la apertura del siniestro de “Muy lento”, una baja que no es por depresión y una valoración inicial de 10000EUR?

A continuación vamos a predecir el valor de Deficit, pero antes creamos el dataframe con la prediccion, cabe destacar que en el modelo tenemos LogCosteInicio no CosteInicio, por lo tanto convertimos a escala logarítmica dicho valor:
```{r}
df_predict = data.frame(20, "M", "S", 0, 0, 300, "P", 30, 5, "Muy lento", "0", log(10000))
colnames(df_predict) = c("Edad", "Sexo", "Estado", "Dependientes", "OtrosDepend", "Salario", "Jornada", "HorasSemana", "DiasSemana", "Clasificacion", "RiesgoSM", "LogCosteInicio")
```

Una vez creado el dataframe, realizamos la predicción:
```{r}
prediccion_valor = predict(modelo_rlg, df_predict)

if (prediccion_valor >= 0.5) cat("El Deficit que obtenemos a partir de los datos es: 1")
if (prediccion_valor < 0.5) cat("El Deficit que obtenemos a partir de los datos es: 0")
```

De la ejecución anterior, observamos que para ese tipo de cliente no hay riesgo de déficit.

# Análisis de la varianza (ANOVA) de un factor

Vamos a realizar un ANOVA para contrastar si existen diferencias en la variable CosteFinal en escala logarítmica en función de la clasificación del siniestro en relación al tiempo transcurrido hasta la apertura. Seguid los pasos que se indican.

## Hipótesis nula y alternativa

Escribid la hipótesis nula y la alternativa.

En un análisis de ANOVA partimos de la hipótesis nula que no hay diferencia de la variabilidad entre los diferentes grupos, y la hipóstesis alternativa es que al menos hay una diferencia. Esto lo podemos representar de la siguiente forma:

- H0 (hipótesis nula): variabilidad_MuyRapido = variabilidad_Rapido = variabilidad_Lento = variabilidad_MuyLento

- H1 (hipótesis alternativa): variablidad_Grupo_I != variabilidad_Grupo_J (donde i y j son los diferentes grupos, muy rápido, rápido, lento, muy lento. Al menos tiene que haber una diferencia entre grupos)

Otra forma de representarlo de forma general es:

- H0 (hipótesis nula): alfa_i = alfa_j

- H1 (hipótesis alternativa): alfa_i != alfa_j para algún i != j

## Modelo

Calculad el análisis de varianza, usando la función aov o lm. Interpretad el resultado del análisis, teniendo en cuenta los valores: Sum Sq, Mean SQ, F y Pr (> F).

Lo primero de todo es crear el modelo, nos indican que tenemos que analizar el CosteFinal en escala logarítmica en función de la clasificación del siniestro:
```{r}
modelo_anova = aov(formula = LogCosteFinal ~ Clasificacion, data = claimNet)

summary(modelo_anova)
```

De la anterior ejecución, vemos que Pr(>F) es mucho menor que el nivel de significancia, es decir, Pr(>F) < 0.05, por lo tanto podemos rechazar la hipótesis nula en favor de la alternativa, esto significa, que al menos hay una diferencia entre la variabilidad de los grupos según la Clasificación de los siniestros.

Al rechazar la hipótesis nula, estamos diciendo que la clasificación de los siniestros en relación al tiempo afecta al coste final de la indemnización.

Otra cosa a destacar, es que ANOVA nos hace una estimación de la varianza del error, en nuestro caso dicha estimación es de 2.34.

## Efectos de los niveles del factor

Calculad la variabilidad explicada por la variable Clasificacion sobre la variable CosteFinal mediante la métrica eta squared. Interpretad los resultados.

```{r}
etaSquared(modelo_anova)
```

El eta squared para la variable clasificación sobre la variable CosteFinal es de 0.0017, como podemos apreciar sale una variabilidad baja, por lo que el CosteFinal no depende de clasificación.

Esta medida lo que nos indica es la variabilidad explicada por el atributo Clasificación, es decir, en nuestro caso el 0.17% de la variabilidad de CosteFinal es explicado por este atributo, como vemos es muy poco, prácticamente insignificante. 

Cabe destacar que este resultado es el mismo si hacemos el Sum Sq de Clasificacion / (Sum Sq de Clasificacion + Sum Sq de Residuals).

## Contraste dos-a-dos
Como los factores han resultado significativos hay que hacer los contrastes de las comparaciones múltiples. Se puede utilizar la prueba de Tukey-Kramer que compara dos-a-dos las diferentes categorías de la variable.

(Nota: por ejemplo, con la función HSD.test() del paquete agricolae).

Hacemos uso del test de Tukey-Kramer para comparar dos a dos los diferentes grupos:
```{r}
HSD.test(modelo_anova, "Clasificacion", console = TRUE)
```

De la anterior ejecución observamos que aunque hay cuatro niveles de clasificación, el test los ha agrupado en 3 grupos:

- Un grupo cuando la clasificación es "Rápido".

- Otro grupo cuando la clasificación es "Lento" y "Muy lento".

- El último grupo cuando la clasificación es "Muy rápido".

Al generarnos 3 grupos, vemos que realmente sí que había diferencias significativas entre varios niveles:

- El nivel "Rápido" presenta una diferencia de medias respecto a los niveles ("Lento", "Muy lento") y "Muy rápido". Además, este grupo es que provoca un mayor Logcostefinal.

- Los niveles "Lento" y "Muy lento" presentan una diferencia de medias similar y por eso están agrupados, sin embargo, ésta es diferente del nivel "Rápido" y del nivel "Muy rápido". El LogCosteFinal que obtenemos respecto a este grupo es algo menor que en el anterior caso.

- El nivel "Muy rápido" tiene una diferencia de medias diferente al nivel "Rápido" y a los niveles ("Lento", "Muy lento"). Y el LogCosteFinal obtenido con este grupo es el menor de todos, por lo que si la clasificación es "Muy rápido" el LogCosteFinal y en consecuencia CosteFinal va a ser menor.

## Adecuación del modelo

Mostrad la adecuación del modelo ANOVA. Se pide lo siguiente:

- Análisis visual de normalidad de los residuos. Podéis usar la función plot sobre el modelo ANOVA calculado.

- Análisis visual de homocedasticidad de los residuos. Podéis usar plot sobre el modelo ANOVA calculado.

- Contraste de normalidad y homocedasticidad.

### Normalidad de los residuos

El análisis visual de la normalidad de los residuos se puede hacer a partir del gráfico Normal Q-Q. Mostrad e interpretad este gráfico.

```{r}
residuos = residuals(modelo_anova)
ajustados = fitted(modelo_anova)

ggplot(as.data.frame(residuos),aes(sample = residuos)) +  geom_qq()
```

En este caso vemos que los residuos sí que siguen más o menos una distribución normal casi perfecta, solo para valores muy pequeños o muy altos hay una ligera desviación, pero en su gran mayoría se ajustan muy bien a la distribución normal.

### Homocedasticidad de los residuos

El gráfico “Residuals vs Fitted” proporciona información sobre la homcedasticidad de los residuos. Mostrad e
interpretad este gráfico.

```{r}
qplot(x = ajustados, y = residuos, main = "Residuales Estandarizados vs Valores Ajustados", geom = c("point"), ylab = "Residuales Estandarizados", xlab = "Valores Ajustados") + geom_hline(yintercept = 0 , color = 2)
```

Tenemos cuatro tiras una por cada nivel que hay de la variable Clasificación, es decir, una tira para "Muy rápido", otra para "Rápido", otra para "Lento" y otra para "Muy lento". Además, los puntos dentro de cada grupo no están distribuidos de forma homógena no se puede garantizar que estemos en un caso de homogeneidad  de varianza, pero para comprobarlo aplicaremos el test de Bartlett en el siguiente punto.

### Contraste de normalidad

Comprobar el supuesto de normalidad de los residuos con las purebas estadísticas de  Shapiro-Wilk o Lilliefors, entre otros.

Realizamos la prueba de Lilliefors:
```{r}
lillie.test(residuos)
```

Como el pvalor es mayor que el nivel de significancia (0.05), mantenemos la hipótesis nula, siendo ésta que los datos siguen una distribución normal.

### Contraste de homocedasticidad

Comprobar el supuesto de homocedasticidad se puede comprobar a partir de la prueba de Bartleet.

Aplicamos la prueba de Bartlett:
```{r}
bartlett.test(formula = LogCosteFinal ~ Clasificacion, data = claimNet)
```

Vemos que el pvalor es menor que alfa (0.05), en este caso rechazamos la hipótesis nula a favor de la alternativa, es decir, estamos en una caso de heterogeneidad de varianzas.

# ANOVA multifactorial

A continuación, se desea evaluar el efecto sobre CosteFinal en escala logarítmica según Sexo combinado con el factor RiesgoSM. Seguid los pasos que se indican a continuación.



















