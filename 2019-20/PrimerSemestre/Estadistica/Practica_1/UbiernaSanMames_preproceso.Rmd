---
title: "A1 - Preprocesado de datos"
author: "Mario Ubierna San Mamés"
date: '`r format(Sys.Date(),"%e de %B, %Y")`'
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

******
# Carga del archivo
******  
<p> Realizamos la lectura del fichero </p>
```{r}
df = read.csv(file="./data/fifa_raw.csv", sep=",", encoding="UTF-8") #Hay que hacer el encoding ya que de lo contrario leía mal los datos, por ejemplo Luis Suárez no identifica bien el caracter con tilde

df_copy = df # Realizamos una copia del dataframe original, para así modificar la copia de éste y mantener los datos originales
head(df_copy)
```

<p> Usamos la función str() para analizar el tipo de datos que R ha interpretado y observar los valores resumen de cada variable </p>
```{r}
str(df_copy)
```
<p> Como podemos observar en la anterior ejecución, no todos los datos están bien "categorizados", es decir, hay atributos como por ejmplo Nationality en el que ha categorizado la variable como character cuando lo mejor sería como factor, las fechas también están mal categorizadas, se las considera character cuando son Date, el dorsal de cada jugador se considera de tipo integer cuando debería ser un factor... 

Se irán resolviendo estos problemas a medida que vamos haciendo la práctica, ya que en este punto solo se nos pide cargar el fichero y examinar el tipo de datos.
</p>


******
# Verficar duplicación de registros
******
<p> Verificamos si hay registros duplicados con el valor ID. En caso de duplicación, seleccionamos el registro con menor número de NA en las variables 

Puede haber registros duplicados si tienen el mismo ID: </p>
```{r}
nRows = nrow(df_copy) # Número de registros (filas) que hay en el df
nUniques = length(unique(df_copy[["ID"]])) # Número de valores únicos de ID en el df

cat("El número de registros duplicados con el valor ID son: ", nRows - nUniques)
```
<p> Como podemos observar no hay duplicados por el campo ID, esto no significa que no pueda haber valores duplicados en otras columnas, es por ello que hacemos uso de la función duplicated() para saber si hay duplicados: </p>
```{r}
nUniques = length(duplicated(df_copy)) # Número de valores duplicados en el df

cat("El número de registros duplicados son: ", nRows - nUniques)
```



******
# Normalización de los datos cuantitativos
******

## Rating
<p> Verificar que la variable Rating esté entre 0 y 100. </p>
```{r}
nMinRating = min(df_copy$Rating) # Valor de rating mínimo
nMaxRating = max(df_copy$Rating) # Valor de rating máximo

cat("El valor mínimo de rating es: ", nMinRating)
cat("El valor máximo de rating es: ", nMaxRating)
```
<p> Como podemos observar el valor mínimo y máximo están dentro del intervalo [0,100]. </p>

## Height
<p> La variable Height se ha de expresar en cm con 3 dígitos sin decimales. Para facilitar la lectura y tratamiento del fichero deben ser numéricas, por lo tanto se debe quitar el símbolo en cm </p>
```{r}
# Método que nos permite hacer la conversión tanto para la variable Height como Weight
heiWeiFromCharToNumeric = function(value) {
  
  if (!is.na(value) & is.character(value)) {
    lstPairValues = strsplit(value, "[[:space:]]+")
    
    if (toupper(lstPairValues[[1]][2]) == "M") return(trunc(as.numeric(gsub(",", ".", lstPairValues[[1]][1])) * 100))
    if (toupper(lstPairValues[[1]][2]) == "CM") return(trunc(as.numeric(lstPairValues[[1]][1])))
    if (toupper(lstPairValues[[1]][2]) == "KG") return(trunc(as.numeric(gsub(",", ".", lstPairValues[[1]][1]))))
    if (toupper(lstPairValues[[1]][2]) == "GR") return(trunc(as.numeric(lstPairValues[[1]][1]) / 1000))
    
  } else {
    return(NA)
  }
   
}
                                
 df_copy$Height = sapply(df_copy$Height, heiWeiFromCharToNumeric)
 str(df_copy$Height)
```

## Weight
<p> La variable Weight se ha de expresar en kg con 3 dígitos sin decimales. Si hay decimales, se ha de truncar el valor. Para facilitar la lectura y tratamiento del fichero debn ser numéricas, por lo tanto se debe quitar el símbolo de kg. </p>
```{r}
# Hacemos uso de la función anterior para realizar la conversión
df_copy$Weight = sapply(df_copy$Weight, heiWeiFromCharToNumeric)
str(df_copy$Weight)
```

## Resumen
<p> Podemos observar de forma general los cambios realizados en este apartado </p>
```{r}
head(df_copy)
```



******
# Normalización de los datos cualitativos
******

## Name y Nationality
<p> Las variable Name y Nationality no han de tener espacios en blanco antes o después de su valor. El valor para estas variables han de ser mayúsculas en la primera letra de cada palabra, tal como "Lionel Messi" </p>
```{r}
# Método que elimina los espacios en blanco del principio y del final
trim = function (x) gsub("^\\s+|\\s+$", "", x)

# Método convierte el valor de entrada en un nuevo valor en el que la primera letra de cada palabra empieza por mayúscula.
firstLettertoUpperCase = function(value) {
  
  cValue = trim(value)
  lstValues = strsplit(cValue, "[^[a-zA-Z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u024F0-9]+]*")[[1]] #Código Unicode de los caracteres latín
  paste(toupper(substring(lstValues, 1,1)), tolower(substring(lstValues, 2)), sep="", collapse=" ")
}
```

```{r}
# Hacemos uso de la función anterior para que transforme el dato Name
df_copy$Name = sapply(df_copy$Name, firstLettertoUpperCase)
str(df_copy$Name)
```

```{r}
# Hacemos uso de la función anterior para que transforme el dato de Nationality
df_copy$Nationality = sapply(df_copy$Nationality, firstLettertoUpperCase)
str(df_copy$Nationality)
```

## Preffered_Foot
<p> La variable Preffered_Foot ha de tener los valores Left y Right que corresponde a los valores actuales de 1 y 2, respectivamente </p>
```{r}
df_copy$Preffered_Foot[df_copy$Preffered_Foot == 1] = "Left"
df_copy$Preffered_Foot[df_copy$Preffered_Foot == 2] = "Right"
str(df_copy$Preffered_Foot)
```

## Work_Rate
<p> La variable Work_Rate se basa en la combinación de dos de estas tres categorías: Low, Medium y High. Verificar que se cumple y en caso contrario, hay que corregirlo. Puedes encontrar los nombres de las categorías cortado con tres letras. </p>
```{r}
unique(df_copy$Work_Rate)
```
<p> Como podemos apreciar en la anterior ejecución vemos que se cumple, es decir, la variable Work_Rate se basa en una combinación de dos de esas tres categorías. Para que haya una uniformidad en los datos voy a cambiar el nombre acortado por el nombre no abreviado. </p>
```{r}
df_copy$Work_Rate = df$Work_Rate
# Método que completa las abreviaturas de la variable Work_Rate por su nombre completo
transformWorkRate = function(value) {
  lstValues = strsplit(value, "\\s+")
  cReturn = ""
  
  for (element in lstValues[[1]]) {
    if (toupper(element) == "LOW") cReturn = paste(cReturn, "Low", sep = "", collapse = NULL)
    if (toupper(element) == "MED" | toupper(element) == "MEDIUM") cReturn = paste(cReturn, "Medium", sep = "", collapse = NULL)
    if (toupper(element) == "HIG" | toupper(element) == "HIGH") cReturn = paste(cReturn, "High", sep = "", collapse = NULL)
    if (element == "/") cReturn = paste(cReturn, "/ ")
  }
  
  cReturn
}

df_copy$Work_Rate = sapply(df_copy$Work_Rate, transformWorkRate)
unique(df_copy$Work_Rate)
```
<p> Si comparamos los dos anteriores resultado vemos que hemos pasado de 11 categorías a 9, esto se debe a que había información abreviada que significa lo mismo que la información no abreviada. </p>

## Resumen
<p> Podemos observar de forma general los cambios realizados en este apartado </p>
```{r}
head(df_copy)
```



******
# Posibles inconsistencias y variables tipo fecha
******

## Club_Joining
<p> Verificar que la variable Club_Joining está en el rango de los años 1990 a 2017. En caso de haber algún registro que no cumpla la condición, indicar el número de registro, Name y Club_Joining. </p>
<p> Lo primero que debemos hacer es convertir las columnas Club_Joining y Contract_Expiry a tipo date, cabe destacar que de Contract_Expiry conocemos solo el año no el mes y el día en el que acaba el contrato, por lo tanto, según la FIFA los contratos se expiran el 30 de Junio del año X, es por ello que he considerado esa fecha como nueva fecha del Contract_Expiry: </p>
```{r}
df_copy$Club_Joining = as.Date(df_copy$Club_Joining, format = "%m/%d/%Y")
df_copy$Contract_Expiry = as.Date(paste("0630", as.character(df_copy$Contract_Expiry)), format = "%m%d%Y")

str(df_copy$Club_Joining)
str(df_copy$Contract_Expiry)
```

<p> Una vez que hemos convertido los atributos correspondietes a Date, puedemos realizar el ejercicio: </p>
```{r}
dMinClubJoining = min(df_copy$Club_Joining, na.rm = TRUE) # Fecha de Club_Joining mínima
dMaxClubJoining = max(df_copy$Club_Joining, na.rm = TRUE) # Fecha de Club_Joining máxima

cat("La fecha mínima de Club_Joining es: ", as.character(dMinClubJoining))
cat("La fecha máxima de Club_Joining es: ", as.character(dMaxClubJoining))

```
<p> Como podemos observar, la fecha mínima y la fecha máxima están dentro de los valores que se nos piden entre [1990,2017], es decir, desde 1990 hasta el 2017 incluido. Sin embargo, he tenido que usar na.rm = True, ya que hay un registro cuya fecha está vacía. Dicho registro es el siguiente: </p>
```{r}
summary(df_copy$Club_Joining)

df_copy[is.na(df_copy$Club_Joining), c("ID", "Name", "Club_Joining")]
```

<p> Por si acaso, si se condira que el 2017 no está incluido, tendríamos el siguiente número de jugadores que no lo cumplen: </p>
```{r}
cat("El número de jugadores que se unieron a un equipo a partir del 2017 son: ", nrow(df_copy[df_copy$Club_Joining >= as.Date("01/01/2017", format = "%m/%d/%Y"), c("ID", "Name", "Club_Joining")]))

head(df_copy[df_copy$Club_Joining >= as.Date("01/01/2017", format = "%m/%d/%Y"), c("ID", "Name", "Club_Joining")])
```

## Contract_Expiry >= Club_Joining?
<p> Verificar que el año de expiración del contrato (Contract_Expiry) no es inferior al año de inicio del contrato (Club_Joining). En caso de haber algún registro que no cumple la condición, indicar el número de registro, Name, Club_Joining y Contract_Expiry </p>
```{r}
df_copy[df_copy$Contract_Expiry < df_copy$Club_Joining, c("ID", "Name", "Club_Joining", "Contract_Expiry")]
```
<p> Como bien he mencionado antes, al calcular la fecha del Contract_Expiry consideramos hasta el 30 de Junio según dictamina la FIFA, entonces al hacer la consulta para saber si algún registro no cumple que el Contract_Expiry >= Club_Joining, observamos que no hay ningún registro que no la cumpla. </p>

## Revisar si la edad corresponde a la fecha de nacimiento
<p> Verificar que la edad (Age) en la fecha 1/1/2017 corresponde a la calculada con la fecha de nacimiento (Birth_Date). En caso de haber algún registro que no cumpla la condición, modificar la edad en función del valor obtenido con la fecha de nacimiento. </p>
<p> Lo primero de todo es transformar de character a Date la variable Birth_Date: </p>
```{r}
df_copy$Birth_Date = as.Date(df_copy$Birth_Date, format = "%m/%d/%Y")
str(df_copy$Birth_Date)
```

<p> Una vez cambiado el tipo de dato, podemos continuar con la resolución del ejercicio. Pero antes, a la hora de calcular los años puedes redondear hacia arriba o truncar el valor, he considerado que lo mejor es truncar el valor, ya que si redondeas hacia arriba estás atribuyendo una edad que todavía no ha cumplido por muy cerca que esté </p>
```{r}
# Método que calcula la edad a 1/1/2017
calculateAge = function(value) {
  fecha = as.Date("01/01/2017", "%m/%d/%Y")
  nAge = as.integer((fecha - value) / 365)
  nAge
}
```

<p> Calculamos las nuevas edades: </p>
```{r}
vAges = sapply(df_copy$Birth_Date, calculateAge) # Calculamos todas las edades
head(vAges) # Nuevas edades
```

<p> Comprobamos las edades pasadas: </p>
```{r}
head(df_copy$Age) # Edades pasadas
```

<p> Realizamos el cambio de las nuevas edades por las pasadas: </p>
```{r}
vIndex = df_copy$Age != vAges # Obtenemos dónde hay que realizar el cambio de la edad
df_copy$Age[vIndex] = vAges[vIndex] # Realizamos el cambio
head(df_copy$Age) # Resultado
```

## Resumen
<p> Podemos observar de forma general los cambios realizados en este apartado: </p>
```{r}
head(df_copy)
```



******
# Valores atípicos
******

## Height
<p> Comprobamos que no haya valores atípicos en el Height: </p>
```{r}
library(ggplot2) # Importamos la librería ggplot2
ggplot(mapping = aes(x=df_copy$Height)) + geom_density()
```

<p> Como podemos observar a priori no hay valores atípicos, ya que la altura se mueve en un intervalor adecuado. También podemos usar la función summary() y así que se vea de una forma numérica: </p>
```{r}
summary(df_copy$Height)
```

<p> Algo que destacar de la anterior ejecución, es que tenemos tres registros con valores NA, eso no significa que es un outlier, sino que más bien es un valor perdido, lo cual solucionaremos más adelante. Otra forma de ver valore atípicos es mediante la representación de un gráfico box plot: </p>
```{r}
boxplot(df_copy$Height, main="Height", color="gray")
boxplot.stats(df_copy$Height)$out
```

<p> Tal y como podemos ver sí que tenemos outliers, sin embargo no se tratan de valores anormales ya que es totalmente válido que una persona mida 155 cm que mida 207 cm. Por lo tanto, aunque hay outliers en el Height no podemos consideran que sean valores atípicos. </p> 

## Weight
<p> Comprobamos que no haya valores atípicos en el Wight: </p>
```{r}
ggplot(mapping = aes(x=df_copy$Weight)) + geom_density()
```
<p> Tal y como podemos observar, se produce uan distribución coherente, no tenemos picos con un peso muy pequeño o muy grande, por lo que a priori no vamos a tener valores centinelas. </p>
```{r}
summary(df_copy$Weight)
```

<p> Ejecutando summary() tenemos un resumen para esta variable, lo más significativo es que tenemos tres registros cuyo peso es NA, pero esto no son outliers, sino que son valores perdidos. </p>
```{r}
boxplot(df_copy$Weight, main="Weight", color="gray")
boxplot.stats(df_copy$Weight)$out
```
<p> Al igual que sucedía con la variable Height, a priori según el box plot tenemos outliers, pero analizando con más detenimiento, esos valores son totalmente coherentes, ya que una persona puede pesar de 48 Kg a 110 Kg sin ningún problema. </p>



******
# Imputación de valores
******
<p> Buscar en las variables Weight y Height donde haya valores perdidos (NA) y realice una imputación de valores.

Para realizar una imputación de valores necesita hacer una regresión lineal que tenga como variable a predecir la variable con la NAs. Esto significa que hay que realizar dos modelos de regresión lineal, uno para predecir los valores NAs en Weight a partir de la variable Height. El otro es precisamente al revés, predecir los valores NAs en Height a partir de Weight.

La función para hacer regresión lineal es lm().

Muestre el resultado de la imputación y de la variable explicativa en aquellos casos donde había un NA. </p>

<p> Lo primero de todo es ver en qué registros hay NA para la variable Weight: </p>
```{r}
df_copy[is.na(df_copy$Weight), c("ID", "Name", "Weight", "Height")]
indexWeights = df_copy$ID[is.na(df_copy$Weight)] # Ïndices de los registros que debemos cambiar el peso
```

<p> Registros en los que hay NA para la variable Height: </p>
```{r}
df_copy[is.na(df_copy$Height), c("ID", "Name", "Weight", "Height")]
indexHeights = df_copy$ID[is.na(df_copy$Height)] # Ïndices de los registros que debemos cambiar la altura
```

<p> Una vez identificados los registros, podemos realizar la imputación de valores, pero primero lo vamos a hacer para la variable Weight, es decir, la variable independiente (x) = Height y la variable dependiente (y) = Weight </p>
```{r}
rlXHeightYWeight = lm(df_copy$Weight ~ df_copy$Height) # Creamos el modelo de regresión lineal
predictionsWeight = trunc(predict(rlXHeightYWeight, as.list(df_copy$Height))) # Hacemos las predicciones
df_copy$Weight[indexWeights] = predictionsWeight[indexWeights] # Imputamos los valores de las predicciones

df_copy[indexWeights, c("ID", "Name", "Weight", "Height")] # Visualizamos los cambios
```

<p> En segundo lugar lo vamos a hacer para la variable Height, es decir, la variable independiente (x) = Weight y la variable dependiente (y) = Height </p>
```{r}
rlXWeightYHeight = lm(df_copy$Height ~ df_copy$Weight) # Creamos el modelo de regresión lineal
predictionsHeight = trunc(predict(rlXWeightYHeight, as.list(df_copy$Weight))) # Hacemos las predicciones
df_copy$Height[indexHeights] = predictionsHeight[indexHeights] # Imputamos los valores de las predicciones

df_copy[indexHeights, c("ID", "Name", "Weight", "Height")] # Visualizamos los cambios
```

<p> Ahora, vamos a mostrar un resumen de cómo va el dataset, junto con el número de registros con Weight y Height con NA: </p>
```{r}
head(df_copy)
cat("El número de registros que tienen NA como peso son: ", nrow(df_copy[is.na(df_copy$Weight), ]))
cat("El número de registros que tienen NA como altura son: ", nrow(df_copy[is.na(df_copy$Height), ]))
```



******
# Estudio descriptivo de las variables cuantitativas
******
<p> Realice un breve estudio descriptivo de las variables cuantitativas una vez depuradas. Hay que crear una tabla con medidas de tendencia central y de dispersión, tanto robustas como no robustas. Haga un breve comentario sobre los resultados obtenidos entre estos tipos de medidas en todas las variables. </p>

<p> Para realizar este apartado, se considera exclusivamente las variables objeto de estudio que son cuantitativas (las variables que aparecen al comienzo del enunciado de la práctica), estas variables son:   </p>
```{r}
library(psych)

calculateStadisticTable = function(values, nTrim = 0.05) {
  nReturn = c()
  
  if (is.numeric(values)) {
    # Calculamos la media
    media = mean(values)
    # Mediana
    mediana = median(values)
    # Media recortada
    medRecortada = mean(values, trim = nTrim)
    # Media winsorizada
    medWindor = winsor.mean(values, trim = nTrim)
    # Desviación estándar
    desviacion = sd(values)
    # RIC
    ric = IQR(values)
    # DAM
    dam = mad(values)
    
    nReturn = c(media, mediana, medRecortada, medWindor, desviacion, ric, dam)
  }
  
  return(nReturn)
}
# Rating Height Weight Age
df_copy_numericCols = df_copy[, c("Rating", "Height", "Weight", "Age")]
df_stadistic_table = sapply(df_copy_numericCols, calculateStadisticTable)
rownames(df_stadistic_table) = c("Media", "Mediana", "Media Recortada", "Media Winsor", "Desviación", "RIC", "DAM")
df_stadistic_table
```

<p> Respecto a las medidas de tendencia central no robustas (media) y robustas (mediana, media recortada, media winsor) cabe destacar  que no hay apenas diferencia, por lo que podemos concluir que el rango de los datos está muy bien definido, es decir, aunque sí que hay outliers, éstos son coherentes como bien hemos mencionado en el apartado 6 sobre valores atípicos. Los valores que nos proporciona tampoco es algo que sea nuevo, es decir, sabemos que la mayoría de los jugadores tienen una calidad media de ahí un Rating de 66 (salvando el típico Messi o Cristiano), en cuanto a la altura y el peso son bastantes normales, los futbolistas no suelen pesar mucho (habría más lesiones) ni tampoco son demasiado altos como en el baloncesto (tienen ventaja los que tienen un punto de gravedad más bajo), y respecto a la edad, una carrear deportiva suele durar de los 20 a los 30 años (por norma general), por lo que una edad de 25 es lo más normal.

Por otro lado, sobre las medidas de dispersión no robustas (desviación) y robustas (RIC, DAM) podemos concluir al igual que en el caso anterior, que los datos no están muy dispersos, es decir, las medidas de dispersión nos permiten hacernos una idea de lo ancha que es la campana de la distribución de una variable (cómo de alejados están los puntos respecto al promedio). Con execpción del Rating, que es verdad que su desviación es un poco más elevada ya que no nos olvidemos que su valor mínimo es 45 y su vamor máximo es 94, por lo que tener una desviación/DAM de 7 es un poco más significativa, ya que el rango de estos valores es más pequeño. Respecto al resto de atributos la desviación está dentro de un baremo más normal. </p>



******
# Análisis de Componentes Principales (ACP)
******
<p> 
Realizar el Análisis de Componentes Principales sobre las variables “Rating”, “Height”, “Weight” y “Age”. Representar el gráfico biplot de dos dimensiones. Hacer un breve comentario indicando el porcentaje de variabilidad explicada en cada componente principal, la variabilidad explicada en las dos primeras dimensiones y qué variable original está más asociada a cada una de las dos primeras componentes principales. Interpretar.

Por ultimo, ¿hay algún punto más fuera de la nube de puntos?, si es así puedes mostrar los valores de las variables originales e indicar que tiene de especial este punto.

Para responder a este apartado, puede usar la función prcomp y ggbiplot.
</p>
```{r}
# Creamos un data frame exclusivo para este apartado.
df_copy_ACP = df_copy[, c("Rating", "Height", "Weight", "Age")]
```

<p> Una vez creado el data frame, tenemos que elegiar si es la matriz de covarianzas o de correlaciones la que vamos a usar. En este caso, al ser variabels con diferentes magnitudes lo mejor es usar la matriz de correlaciones. A continuación podemos ver dicha matriz:  </p>
```{r}
cor(df_copy_ACP)
```

<p> Las conclusiones que podemos obtener son: 
1 - El Rating está correlacionado con el Age
2 - El Height y Weight están altamente correlacioandos positivamente, es decir, que ha medida que aumenta un atributo también aumenta el otro.
3 - El Age aparte de estar corelacionado el Rating, también está un poco correlacionado con el Weight.

Una vez analizada la matriz de correlaciones podemos realizar un análisis de los componente principales:
</p>
```{r}
ACP.cor = prcomp(x=df_copy_ACP, center = TRUE, scale. = TRUE)
summary(ACP.cor)
```

<p> 
Con los resultados obtenidos vemos que la proporción de varianza es diferente para cada uno de los componentes, esto era de esperar, ya que viendo la matriz de correlaciones hay dos componentes claros, uno que en el que se relacionan el Rating y el Age y otro en el que se relacionan el Height y el Weight.

Por lo tanto, seleccionado los dos primeros componentes podemos proyectar más o menos la misma información que en el dataset original, pero reduciendo la dimensionalidad de 4 a 2 (se nos indica en el enundado seleccionar solo los dos primeros compenentes). Como podemos observar, las dos primeras dimensiones representan la mayor parte de la varianza total con un 80,72%.

Aunque se nos dictamina en este apartado seleccionar los dos primeros componentes, podríamos ver si es lo mejor haciendo uso de la regla de Kaiser-Guttman o un scree plot.
</p>
```{r}
valores_propios = ACP.cor$sdev^2
media_valores_propios = sum(valores_propios) / length(valores_propios)
cat("Los valores propios son: ", valores_propios)
cat("La media de los valores propios es: ", media_valores_propios)
```

<p> Como podemos apreciar, según Kaiser-Guttman hay que elegir como componentes princiales aquellos que cuyos valores propios superen la media de los mismos, en este caso solamente el componente 1 y componente 2 lo hacen, por lo que sí que es adecuado elegir estos dos componentes. 

Lo siguiente que debemos realizar es un Biplot, el cual nos permite mostrar tanto los nuevos puntos muestrales como las variables originales, permitiéndonos así ver que variables son más relevantes.
</p>

```{r}
library(ggbiplot)
biplot_ACP = ggbiplot(ACP.cor, scale = 1, obs.scale = 1, varname.size=5, varname.adjust=0) +
  ggtitle("ACP basado en la correlación")

biplot_ACP
```

<p> Como poedmos apreciar en el anterior gráfico, la variable Weight y Height están más asociadas con el componente 1, aunque también es verdad qeu algo relacionadas con el componente 2 pero algo menos. Por otro lado, las variables Rating y Age están más asocidaas con el componente 2 y poco con el componente 1.

Como podemos apreciar las nuevas proyecciones están prácticamente centralizadas, no podemos destinguir clusters claros y además las variables originales también están muy equilibradas, esto se debe a que como podemos apreciar solo hay una diferencia de un 14,1% de varianza entre ambos componentes, por lo que todo está muy igualado/equilibrado.

Finalmente, podemos ver cuales son los outliers que tenemos:
</p>
```{r}
ggbiplot(ACP.cor, scale = 1, obs.scale = 1, varname.size=5, varname.adjust=0) +
  ggtitle("Análisis de los outliers")
```

<p> Tal y como apreciamos en el anterior gráfico, vemos que hay un punto muy alejado de la nube de puntos, este se encuentra cerca de la esquina inferior izquierda. Si ese punto se encuentra ahí es porque tiene una altura y peso característicos, al mostrar las labels tanto para el peso como para la altura obtenemos los siguientes valores: </p>
```{r}
ggbiplot(ACP.cor, scale = 1, obs.scale = 1, varname.size=5, varname.adjust=0, labels = df_copy_ACP$Weight) +
  ggtitle("Análisis del outlier con su Weight")
ggbiplot(ACP.cor, scale = 1, obs.scale = 1, varname.size=5, varname.adjust=0, labels = df_copy_ACP$Height) +
  ggtitle("Análisis del outlier con su Height")
ggbiplot(ACP.cor, scale = 1, obs.scale = 1, varname.size=5, varname.adjust=0, labels = df_copy_ACP$Rating) +
  ggtitle("Análisis del outlier con su Rating")
ggbiplot(ACP.cor, scale = 1, obs.scale = 1, varname.size=5, varname.adjust=0, labels = df_copy_ACP$Age) +
  ggtitle("Análisis del outlier con su Age")
```

<p> Como vemos tiene un peso de 110kg y una altura de 207cm, como podemos ver tanto su peso como su altura son demasiado altos respecto a los demás. Por otro lado, tiene un rating de 67 y una edad de 29, las cuales son bastante comunes dentro del dataset.

Por lo tanto, este punto es especial simplemente por las condiciones físicas que presenta, tanto en altura como en peso y se corresponde con el siguiente jugador:
</p>
```{r}
df_copy[df_copy$Weight == 110 & df_copy$Height == 207,]
```



******
# Archivo final
******
<p> Una vez realizado el preprocesamiento sobre el archivo, copie el resultado de los datos en un archivo llamado “fifa_clean.csv”. </p>
```{r}
write.csv(df_copy, file = "./data/fifa_clean.csv", row.names = FALSE)
```

